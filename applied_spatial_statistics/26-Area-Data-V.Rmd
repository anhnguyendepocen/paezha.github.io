---
title: "14 Area Data V"
output: html_notebook
---

# Area Data V

*NOTE*: You can download the source files for this book from [here](https://github.com/paezha/Spatial-Statistics-Course). The source files are in the format of R Notebooks. Notebooks are pretty neat, because the allow you execute code within the notebook, so that you can work interactively with the notes. 

In the previous chapter, you learned about how to use local spatial statistics for exploratory spatial data analysis. 

If you wish to work interactively with this chapter you will need the following:

* An R markdown notebook version of this document (the source file).

* A package called `geog4ga3`.

## Learning Objectives

In this practice, you will:

1. Practice how to estimate regression models in R.
2. Learn about autocorrelation as a model diagnostic.
3. Learn about variable transformations.
4. Use autocorrelation analysis to improve regression models.

## Suggested Readings

- Bailey TC and Gatrell AC (1995) Interactive Spatial Data Analysis, Chapter 7. Longman: Essex.
- Bivand RS, Pebesma E, and Gomez-Rubio V (2008) Applied Spatial Data Analysis with R, Chapter 9. Springer: New York.
- Brunsdon C and Comber L (2015) An Introduction to R for Spatial Analysis and Mapping, Chapter 7. Sage: Los Angeles.
- O'Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley & Sons: New Jersey.

## Preliminaries

As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is `rm` (for "remove"), followed by a list of items to be removed. To clear the workspace from _all_ objects, do the following:
```{r}
rm(list = ls())
```

Note that `ls()` lists all objects currently on the worspace.

Load the libraries you will use in this activity:
```{r}
library(tidyverse)
#library(rgdal)
#library(geosphere)
library(ggmap)
library(geosphere)
#library(gridExtra)
library(sf)
library(plotly)
library(spdep)
#library(crosstalk)
library(geog4ga3)
```

Next, read an object of class `sf` (simple feature) with the census tracts of Hamilton CMA and some selected population variables from the 2011 Census of Canada:
```{r}
data(Hamilton_CT)
```

The `sf` object can be converted into a `SpatialPolygonsDataFrame` object for use with the `spdedp` package:
```{r}
Hamilton_CT.sp <- as(Hamilton_CT, "Spatial")
```

## Regression Analysis in R

This section provides a refresher on linear regression, before reviewing the estimation of regression models in `R`.

A linear regression model posits relationships between an outcome, called a dependent variable, and one or more covariates, called independent variables. Regression models capture statistical relationships, not causal relationships, but often the causality is implied by the choice of independent variables.

This is the form of a linear regression model:
$$
y_i = \beta_0 + \sum_{j=1}^k{\beta_jx_{ij}} + \epsilon_i
$$
where $y_i$ is the dependent variable and $x_ij$ ($j=1,...,k$) are the independent variables. The coefficients $\beta$ are not known, but can be estimated from the data. And $\epsilon_i$ is a term called a _residual_ (or _error_), because it is the difference between the systematic term of the model and the value of $y_i$:
$$
\epsilon_i = y_i - \bigg(\beta_0 + \sum_{j=1}^k{\beta_jx_{ij}}\bigg)
$$

Estimation of a linear regression model is the procedure used to obtain values for the coefficients. This typically involves defining a _loss function_ that needs to be minimized. In the case of linear regression, a widely used estimation procedure is _least squares_. This procedure allows a modeler to find the coefficients that minimize the sum of squared residuals. In very simple terms, the protocol is as follows:
$$
\text{Find the values of }\beta\text{ that minimize }\sum_{i=1}^n{\epsilon_i^2}
$$

For this procedure to be valid, there are a few assumptions that need to be satisfied, including:

1) The functional form of the model is correct.

2) The independent variables are not collinear; this is often diagnosed by calculating the correlations among the independent variables, with values greater than 0.8 often being problematic.

3) The residuals have a mean of zero:
$$
E[\epsilon_i|X]=0
$$

4) The residuals have constant variance:
$$
Var[\epsilon_i|X] = \sigma^2 \text{ }\forall i
$$

5) The residuals are independent, that is, they are not correlated among them:
$$
E[\epsilon_i\epsilon_j|X] = 0 \text{ }\forall i\neq j
$$

The last three assumptions ensure that the residuals are _random_. Violation of these assumptions is often a consequence of a failure in the first two (i.e., the model was not properly specified, and/or the residuals are not exogenous).

When all these assumptions are met, the coefficiens are said to be _BLUE_: Best Linear Unbiased Estimates - a desirable property because we wish to be able to quantify the relationships between covariates without bias.

The basic command for multivariate linear regression in R is `lm()`, for "linear model". This is the help file of this function:
```{r}
?lm
```

Lets see how to estimate a model using this function. 

To illustrate this, we will use the example of population density gradients. These gradients are representations of the variation of population density in cities, and are of interest because they are related to land rent, urban form, and commuting patterns, among other things (see accompanying reading for more information).

Urban economic theory suggests that population density declines with distance from the central business district of a city, or its CBD. This leads to the following model, where the population density at $i$ is a function of the distance of $i$ to the CBD. Since this is likely a stochastic process, we allow for some randomness by means of the residuals:
$$
P_i = f(D_i) + \epsilon_i
$$

Lets add distance to the CBD as a covariate to your dataframe. We will use Jackson Square as the CBD of Hamilton:
```{r}
xy_cbd <- c(-79.8708, 43.2584)
```

Convert these coordinates to `sf`, with EPSG code 4326 for longitude and latitude:
```{r}
#xy_cbd <- st_as_sf(xy_cbd, coords = c("lon", "lat"), crs = 4326)
```

```{r}
xy_ct <- coordinates(spTransform(Hamilton_CT.sp, CRSobj = "+proj=longlat +datum=WGS84 +no_defs"))
```

Given these coordinates, the function `sf::st_distance` can be used to calculate the great circle distance between the centroids of the census tracts and Hamilton's CBD. Call this `dist2cbd.sl`, i.e., straight line distance to CBD in a straight:
```{r}
dist2cbd.sl <- distGeo(xy_ct, xy_cbd)
```

Add the distance to `Hamilton_CT` for analysis:
```{r}
Hamilton_CT$dist.sl <- dist2cbd.sl
```

Regression analysis is implemented in `R` by means of the `lm` function. The arguments of the model include an object of type "formula" and a dataframe. Other arguments include conditions for subsetting the data, using sampling weights, and so on.

A formula is written in the form `y ~ x1 + x2`, and more complex expressions are possible too, as we will see below. For the time being, the formula is simply `POP_DENSIT ~ dist.sl`:
```{r}
model1 <- lm(formula = POP_DENSITY ~ dist.sl, data = Hamilton_CT)
summary(model1)
```

The value of the function is an object of class `lm` that contains the results of the estimation, including the coefficients with their significance diagnostics, and the coefficient of multiple determination, among other items.

Notice how the coefficient for distance is negative (and significant). This indicates that population density declines with increasing distance:
$$
P_i = f(D_i) + \epsilon_i = 4405.15414 - 0.17989D_i + \epsilon_i
$$

## Autocorrelation as a Model Diagnostic

Let quickly explore the fit of the model. The scatterplot is of the actual population density and the distance to CBD, whereas the blue line is the regression model:
```{r}
ggplot(data = Hamilton_CT, aes(x = dist.sl, y = POP_DENSITY)) + 
  geom_point() +
  geom_abline(slope = model1$coefficients[2], 
              intercept = model1$coefficients[1], 
              color = "blue", size = 1) +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0)
```

Clearly, there is a fair amount of noise (the scatter of the dots around the regression line). In this case, the regression line captures the general trend of the data, but underestimates most of the high population density areas, and overestimates most of the low population areas.

If the pattern of under- and over-estimation is random (i.e., the residuals are random), that would indicate that the model successfully retrieved all the systematic pattern. If the pattern is not random, there is a violation of assumption of independence.

Lets add the residuals of the model to your dataframes:
```{r}
Hamilton_CT$model1.e <- model1$residuals
```

And now we can create a map of the residuals, with red indicating negative residuals and blue positive:
```{r message = FALSE}
  plot_ly(Hamilton_CT) %>%
    add_sf(color = ~(model1.e > 0), colors = c("red", "dodgerblue4"))
```

Does the distribution of residuals look random?

Fortunately, we already have the tools to help us with this question. Lets create a set of spatial weights:
```{r}
Hamilton_CT.w <- nb2listw(poly2nb(Hamilton_CT.sp))
```

With this, we can calculate Moran's I:
```{r}
moran.test(Hamilton_CT$model1.e, Hamilton_CT.w)
```

This supports the visual inspection of the map, since the test of hypothesis rejects the null hypothesis of independence at a very high level of confidence (note the extremely small value of p).

Spatial autocorrelation, as mentioned above, is a violation of a key assumption of linear regression, and likely the consequence of a model that was not correctly specified, either because the functional form was incorrect (e.g., the relationship was not linear), or there are missing covariates.

Lets explore the first of these possibilities by means of variable transformations.

## Variable Transformations

The term linear regression refers to the linearity in the coefficients. Variable transformations allow you to consider non-linear relationships between covariates, while still preserving the linearity of the coefficients.

For instance, a possible transformation of the variable distance could be as inverse distance:
$$
f(D_i) = \beta_0 + \beta_1\frac{1}{D_i}
$$
 
Lets create a new covariate that is the inverse distance:
```{r}
Hamilton_CT <- mutate(Hamilton_CT, invdist.sl = 1/dist.sl)
```

Use inverse distance as the covariate in the model:
```{r}
model2 <- lm(formula = POP_DENSITY ~ invdist.sl, data = Hamilton_CT)
summary(model2)
```

As the scatterplot below shows (the model is the blue line), we can capture a non-linear relationship, which does a better job of describing the high density of tracts close to the CBD, but unfortunately is a poor description of density almost everywhere else:
```{r}
ggplot(data = Hamilton_CT, aes(x = dist.sl, y = POP_DENSITY)) + 
  geom_point() +
  stat_function(fun=function(x)2299.6 + 2260521.5/x, geom="line", color = "blue", size = 1) +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0)
```

Add the residuals of this model to the dataframe for further examination:
```{r}
Hamilton_CT$model2.e <- model2$residuals
```

If we calculate Moran's I, you will notice that the coefficient is lower than for the previous model but still significant, wich means that the residuals are not random:
```{r}
moran.test(Hamilton_CT$model2.e, Hamilton_CT.w)
```

The literature on population density gradients suggests other non-linear transformations, including:
$$
f(D_i) = exp(\beta_0)exp(\beta_1x_i)
$$

This function is no longer linear in the coefficients (since all the coefficientsare transformed by the exponential). Fortunately, there is a simple way of changing this to a linear expression, by taking the logarithm on both sides of the equation:
$$
ln(P_i) = \beta_0 + \beta_1x_i + \epsilon_i
$$

Lets create these covariates:
```{r}
Hamilton_CT <- mutate(Hamilton_CT, lnPOP_DEN = log(POP_DENSITY))
```

And estimate the model:
```{r}
model3 <- lm(formula = lnPOP_DEN ~ dist.sl, data = Hamilton_CT)
summary(model3)
```

```{r}
ggplot(data = Hamilton_CT, aes(x = dist.sl, y = POP_DENSITY)) + 
  geom_point() +
  stat_function(fun=function(x)exp(8.465 - 0.0001161 * x), 
                geom="line", color = "blue", size = 1) +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0)
```

As before, add the residuals of the model to the dataframe for further examination:
```{r}
Hamilton_CT$model3.e <- model3$residuals
```

While this latest model provides a somewhat better fit, there is still systematic under- and over-prediction, as seen in the map below (red are negative residuals and blue are positive):
```{r message = FALSE}
  plot_ly(Hamilton_CT) %>%
    add_sf(color = ~(model3.e > 0), colors = c("red", "dodgerblue4"))
```

Moran's I as well strongly suggests that the residuals are not independent:
```{r}
moran.test(Hamilton_CT$model3.e, Hamilton_CT.w)
```

## A Note about Spatial Autocorrelation in Regression Analysis

Spatial autocorrelation was originally seen as a problem in regression analysis. It is not difficult to see why, after testing three models in this chapter.

I prefer to view spatial autocorrelation as an opportunity for discovery. For instance, the models above all seem to struggle capturing the large variations in population density between the central parts of the city and the suburbs of Hamilton. Perhaps this could be due to a _regime change_, or in other words, the presence of an underlying process that operates somewhat differently in different parts parts of the city. The last model, for instance, suggests that Burlington might have an effect.

The analysis that follows is somewhat more advanced, but serves to illustrate the idea of spatial autocorrelation as a tool for discovery.

Lets begin by creating local Moran maps to identify potential spatial regimes: 
```{r message = FALSE, warning = FALSE}
localmoran.map(Hamilton_CT, Hamilton_CT.w, "POP_DENSITY", by = "TRACT")
```

Examination of the map above, suggests that there are possibly three regimes: a CBD ("HH" and significant tracts), Suburbs ("LL" and significant tracts), and Other (not significant tracts). Based on this, we will create two indicator variables, one for census tracts in the CBD and another for census tracts in the Suburbs. An indicator variable takes values of 1 or zero, depending on whether a condition is true. For instance, all census tracts in the CBD will take the value of 1 in the CBD indicator variable, and all others will be zero.

Begin by computing the local statistics:
```{r}
POP_DEN.lm <- localmoran(Hamilton_CT$POP_DENSITY, listw = Hamilton_CT.w)
```

Next, identify the type of tract based on the spatial relationships (i.e., "HH", "LL", or "HL/LH"). After that, identify as CBD all tracts for which Type is "HH" and the p-value is less than or equal to 0.05. Likewise, identify as Suburb all tracts for which Type is "LL" and the p-value is also less than or equal to 0.05:
```{r}
df_msc <- transmute(Hamilton_CT,
                    TRACT = TRACT,
                    Z = (POP_DENSITY - mean(POP_DENSITY)) / var(POP_DENSITY),
                    SMA = lag.listw(Hamilton_CT.w, Z),
                    Type = factor(ifelse(Z < 0 & SMA < 0, "LL",
                                         ifelse(Z > 0 & SMA > 0, "HH", "HL/LH"))))
df_msc <- cbind(df_msc, POP_DEN.lm)

CBD <- ifelse(df_msc$Type == "HH" & df_msc$`Pr.z...0` < 0.05, 1, 0)
Suburb <- ifelse(df_msc$Type == "LL" & df_msc$`Pr.z...0` < 0.05, 1, 0)
```

Add the indicator variables to the dataframe:
```{r}
Hamilton_CT$CBD <- CBD
Hamilton_CT$Suburb <- Suburb
```

The model that I propose to estimate is a variation of the last non-linear specification, but with _regime breaks_:
$$
ln(P_i) = \beta_0 + \beta_1x_i + \beta_2CBD_i + \beta_3Suburb_i + \beta_4CBD_ix_i + \beta_5Suburb_ix_i + \epsilon_i
$$

Since the indicator variables for CBD and Suburb take values of zero and one, effectively we have the following:
$$
ln(P_i)=\Bigg\{\begin{array}{l l}
(\beta_0 + \beta_2) + (\beta_1 + \beta_2)x_i + \epsilon_i \text{ if census tract } i \text{ is in the CBD}\\
(\beta_0 + \beta_3) + (\beta_1 + \beta_5)x_i + \epsilon_i \text{ if census tract } i \text{ is in the Suburbs}\\
\beta_0  + \beta_1x_i + \epsilon_i \text{ otherwise}\\
\end{array}
$$


Notice how the model now allows for different slopes and intercepts for observations in different parts of the city. Estimate the model:
```{r}
model4 <- lm(formula = lnPOP_DEN ~ dist.sl + CBD * dist.sl + Suburb * dist.sl,
             data = Hamilton_CT)
summary(model4)
```

The model is a much better fit (see the the coefficient of multiple determination).

Lets examine the residuals:
```{r message = FALSE, warning = FALSE}
Hamilton_CT$model4.e <- model4$residuals
plot_ly(Hamilton_CT) %>%
  add_sf(color = ~(model4.e > 0), colors = c("red", "dodgerblue4"))
```

It is not clear from the visual inspection that the residuals are independent, but this can be tested as usual by means of Moran's I coefficient:
```{r}
moran.test(Hamilton_CT$model4.e, Hamilton_CT.w)
```

Based on the results, we fail to reject the null hypothesis, and can be confident that the residuals are likely random.

The following figure illustrates the final model:
```{r}
fun.1 <- function(x)exp(7.943 + 0.9536 - (0.00003248  + 0.0000273) * x) #CBD
fun.2 <- function(x)exp(7.943 - 0.9535 - (0.00003248 + 0.0001006) * x) #Suburb
fun.3 <- function(x)exp(7.943 - 0.00003248 * x) #Other

ggplot(data = Hamilton_CT, aes(x = dist.sl, y = POP_DENSITY)) +
  geom_point() +
  geom_point(data = filter(Hamilton_CT, CBD == 1), color = "Red") +
  geom_point(data = filter(Hamilton_CT, Suburb == 1), color = "Blue") +
  stat_function(fun= fun.1, 
                geom="line", size = 1, aes(color = "CBD")) +
  stat_function(fun=fun.2, 
                geom="line", size = 1, aes(color = "Suburb")) +
  stat_function(fun=fun.3, 
                geom="line", size = 1, aes(color = "Other")) +
  scale_color_manual(values = c("red", "black", "blue"), 
                     labels = c("CBD", "Other", "Suburb")) +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0)
```