[
["index.html", "Applied Spatial Statistics with R Preface Spatial Analysis and Spatial Statistics Why this Text? Plan Audience Requisites Words of Appreciation Versioning", " Applied Spatial Statistics with R Antonio Paez 2020-01-09 Preface “Patterns cannot be weighed or measured. Patterns must be mapped.” — Fritjof Capra, The Web of Life: A New Scientific Understanding of Living Systems Spatial Analysis and Spatial Statistics The field of spatial statistics has experienced phenomenal growth in the past 20 years. From being a niche subdiscipline in quantitative geography, statistics, regional science, and ecology at the beginning of the 1990s, it is now a mainstay in applications in a multitude of fields, including medical imaging, remote sensing, civil engineering, geology, statistics and probability, spatial epidemiology, end ecology, to name just a few disciplines. The growth in research and applications in spatial statistics has been in good measure fueled by the explosive growth in geotechnologies: technologies for sensing and describing the natural, social, and built environments on Earth. An outcome of this is that spatial data are, to an unprecedented level, within the reach of multitudes. Hardware and software have become cheaper and increasingly powerful, and we have transitioned from a data poor environment (in all respects, but particularly in terms of spatial data) to a data rich environment. Twenty years ago, for instance, technical skills in spatial analysis included tasks such as digitizing. As a Masters student, I spent many boring hours digitizing paper maps before I could do any analysis on the single-seat (and relatively expensive) Geographic Information System (GIS) available in my laboratory. In that place at that time I was more or less a freak: altough there was an institutional push to adopt GIS, relatively few in my academic environment saw the value of spending hours digitizing maps, something that nowadays we would consider relatively low-level technical work. Surely, the time of a Masters student, let alone a professional researcher or business analyst, is more valuable than that. Indeed, very little time is spent anymore in such tasks low-level tasks, as data increasingly are collected and disseminated in native digital formats. Instead, there is a huge appetite for what could be called the brainware of spatial analysis, the intelligence counterpart of the hardware, software, and data provided by geotechnologies. The contribution of brainware to spatial analysis is to make sense of vast amounts of data, in effect transforming them into information. This information in turn can be useful to understand basic scientific questions (e.g., changes in land cover), to support public policy (e.g., what is the value capture of public infrastructure), and to inform business decisions (e.g., what levels of demand can be expected given the distribution of retail outlets). There are numerous forms of spatial analysis, including normative techniques (such as spatial optimization; see Tong and Murray 2012) and geometric and cartographic analysis (for instance, map algebra; Tomlin 1990). Among these, spatial statistics is one of the key elements in the family of toolboxes for spatial analysis. So what is spatial statistics? Very quickly, I will define spatial statistics as the application of statistical techniques to data that have geographical references - in other words, to the statistical analysis of maps. Like statistics more generally, spatial statistics is interested in hypothesis testing and inference. What distiguishes it as a branch of the broader field of statistics is its explicit interest in situations where data are not independent from each other (like throws of a dice) but rather display systemic associations. These associations, when seen through the lens of cartography, can manifest themselves as patterns of similarities (birds of a feather flock together) or disimilarities (repulsion due spatial competition among firms) - as two common examples of spatial patterns. Spatial statistics covers a broad array of techniques for the analysis of spatial patterns, including tools for testing whether patterns are random or not, and a wide variety of modelling approaches as well. These tools enhance the brainware of analysts by allowing them to identify and possibly model patterns for inferring processes and/or for making spatial predictions. Why this Text? The objective of this book is to introduce selected topics in applied spatial statistics. The foundation for the book are the notes that I have developed over several years of teaching applied spatial statistics at McMaster University. This course is a specialist course for senior-level undergraduate geographers and students in other disciplines who are often working towards specializations in GIS. Over the course of the years, my colleagues at McMaster and I have used at least three different textbooks for teaching spatial statistics. I have personally used McGrew and Monroe (2009) to introduce fundamental statistical concepts to geographers. McGrew and Monroe (currently on a third edition with Lembo) do a fine job of introducing statistics as a tool for decision making, and therefore offer a very valuable resource to learn matters of inference, for instance. Many of the examples in the book are geographical; however, the book is relatively limited in its coverage of spatial statistics (particularly models for spatial processes), which is a limitation for teaching a specialist course on this topic. My text of choice early on (approximately between 2003 and 2010) was the excellent book Interactive Spatial Data Analysis by Bailey and Gatrell (1995). A notable aspect of Bailey and Gatrell was that the book was accompanied by a software application to implement the techniques they discussed. I started using their book as a graduate student around 1998, but even then the limitations of the software that accompanied the book were apparent - in particular the absence of updates or a central repository for code (the book had a sleeve to store a \\(3\\frac{1}{2}\\) floppy disk to install the software). Despite the regretable obsolesence of the software, the book provided then, and still does, a very accessible yet rigorous treatment of many topics of interest in spatial statistics. Bailey and Gatrell’s book was, I believe, the first attempt to bridge, on the one hand, the need to teach mid- and upper-level university courses in spatial statistics, and on the other, the challenges of doing so with very specialized texts on this topic, including the excellent but demanding Spatial Econometrics (Anselin 1988), Advanced Spatial Statistics (Griffith 1988), Spatial Data Analysis in the Social and Environmental Sciences (Haining 1990), not to mention Statistics for Spatial Data (Cressie 1993). More recently, as Bailey and Gatrell aged, my book of choice for teaching spatial statistics became O’Sullivan and Unwin’s Geographical Information Analysis (O’Sullivan and Unwin 2010). This book updated a number of topics that were not covered by Bailey and Gatrell. To give one example, much work happened in the mid- to late-nineties with the development of local forms of spatial analysis, including pioneering research by Getis and Ord on concentration statistics (Getis and Ord 1992), Anselin’s Local Indicators of Spatial Association (Anselin 1995), and Brunsdon, Fotheringham, and Charlton’s research on geographically weighted regression (Brunsdon, Fotheringham, and Charlton 1996). These and related local forms of spatial analysis have become hugely influential in the intervening years, and are duly covered by O’Sullivan and Unwin in a way that merges well with a course focusing on spatial statistics - although other specialist texts also exist that delve in much more depth into this topic (e.g., Fotheringham and Brunsdon 1999; and Lloyd 2010). These resources, and many more, have proved invaluable for my teaching for the past few years, and I am sure that their influence will be evident in the present book. Other excellent sources are also available, including Applied Spatial Data Analysis in R (Bivand, Pebesma, and Gómez-Rubio 2008), Spatial Data Analysis in Ecology and Agriculture Using R (Plant 2012), An Introduction to R for Spatial Analysis &amp; Mapping (Brunsdon and Comber 2015), Spatial Point Patterns: Methodology and Applications with R (Baddeley, Rubak, and Turner 2016), and Geocomputation with R (Lovelace, Nowosad, and Muenchow 2019). This is in addition to other resources available online, such as M. Gismond’s Intro to GIS and Spatial Analysis and R. Hijmans’s Spatial Data Analysis and Modeling with R. So, if there are some excellent resources for teaching and learning spatial statistics, why am I moved to unleash on the world yet another text on this topic? I am convinced that there is richness in variety. As demand for training in spatial statistics grows, there is potential for different sources to satisfy different needs. Some books are geared towards specialized topics (e.g., point pattern analysis; Baddeley, Rubak, and Turner 2016) and cover their subject matter in much more depth than I could in an undegraduate course. For this reason, they are more useful as a reference or a tool for learning for researchers and graduate students. Other books focus more heavily on mapping in R than a course on spatial statistics can comfortably accommodate (e.g., Brunsdon and Comber 2015; Lovelace, Nowosad, and Muenchow 2019). And yet other books are geared towards specific disciplines (e.g., ecology and agriculture; Plant 2012). Bivand et al. (2008) is an excellent reference. At the time of their writing, much work was devoted to issues of spatial data representation. As a consequence, a good portion of their book is concerned with the critical issue of handling spatial data, including data classes and import/export operations which, while essential, happen for most practitioners at a baser level. My approach can be seen as complementary to some of the texts above. I have tried to write a text that introduces key concepts of data handling and mapping in R as they are needed to learn and practice spatial statistical analysis. This I have tried to do in as intuitive way as I could. Readers will see that the computational part of the book - everything that usually lives “under the hood” so to speak - is all bare in the open. The code is extensively documented as it is introduced (with some repetition for pedagogical purposes). Once that a reader has seen and used some commands, we proceed to introduce more sophisticated computational approaches, which are in turn documented extensively when they first appear. I like to think of this approach as introducing coding by stealth, with a gentle ramp for those students who may not have extensive experience in computer-speak. The computational aspects of the book constitute the “how to”. How to calculate a summary statistic. How to create a plot. How to map a variable. The how is an essential foundation for then exercising the brainware. By introducing the how to in a relatively gentle way, I have been able to concentrate in introducing (again, in what I hope is an intuitive way!) key concepts in spatial statistics. The text is not meant to be used as a reference, although some lectors may find that it works in that way in particular with respect to the implementation of techniques. Rather, the text is more suitable to be read linearly - indeed as a course on the topic of spatial statistics. Readers who have familiarized themselves with the text can possibly find it useful as a reference, but I do not recommend using it as a reference in the first place. Lastly, the focus of the text is on applied spatial statistics. There is, inevitably, a component of math, but I have tried, to the extent of my ability, to make the underlying math as intuitive and accessible as possible. As noted above, there is also an important computational component - in particular, as per the title, using the R statistical language. As McElreath (2016) notes, in addition to the pedagogical value of teaching statistics using a coding approach, much of statistics has in fact become so computational that coding skills are increasingly indispensable. I tend to agree with this, and there are reasons to believe that one of the strenghts of this approach as well is to make statistical work as open, clear, and reproducible as possible (see Rey 2009). Plan My aim with this book is to introduce key concepts and techniques in the statistical analysis of spatial data in an intuitive way. While there are more advanced treatments of every single one of these topics, this book should be appealing to undergraduate students or others who are approaching the topic for the first time. The book is organized thematically following the cannonical approach seen, for instance, in Bailey and Gatrell (1995), Bivand et al. (2008), and O’Sullivan and Unwin (2010). This approach is to conceptualize data by their unit of support. Accordingly, data are seen as being represented by: Discrete processes in space (e.g., points and events). Aggregations into zones for statistical purposes (e.g. demographic variables into census areas). As discrete measurements in space of an underlying continuous process (e.g. weather stations monitoring temperature) The book is organized in such a way that each chapter covers a topic that builds on previous material. All chapters, starting with Chapter 3, are followed by an activity. I have used the materials presented in this texts (in a variety of incarnations) for teaching spatial data analysis in different settings. Primarily, these notes have been used in the course GEOG 4GA3 Applied Spatial Statistics at McMaster University. This course is a full (Canadian) academic term, which typically means 13 weeks of classes. The course is organized as a 2-hour-per-week class, with a GIS-lab component which uses a complementary set of notes. For this reason, each chapter is designed to cover very approximately the material that I am used to cover in a 50 minutes lecture in a traditional classroom-lecturing setting. In this case, the activities that follows each chapter could be assigned as homework, optional materials, or as lab materials. For instructors who do not have a lab component, the activities could easily be adapted as lab exercises. More recently, I have experimented with delivery of contents in a flipped classroom format (see here for a discussion of flipped classrooms). Briefly, a flipped classroom changes the role of the instructor and the delivery of contents. In a flipped classroom, the instructor minimizes lecturing time, opting instead for offering study materials in advance (often the materials are online and may have an interactive component). This frees the instructor from the tyranny of lecturing, so that in-class time can be dedicated instead to hands-on activities. The instructor is no longer a magical source of wisdom, but rather a partner in the learning process. Under this scenario, students are responsible for reading the chapter or chapters required in advance to a class. The class then is dedicated to the activity that follows the chapter, with students working individually or in small groups in the activity. I have broken a 50-minutes session of this type as follows: 10 minutes for a short mini-lecture and to discuss any questions about the preceding reading/study materials, followed by 30 minutes to complete the activity; during this time I engage individually or in small groups with the students as they work; and before the end of the 50-minutes session a 10 minute recap, where I summarize the key aspects of the lesson, clearly identify the threshold concepts covered, and indicate how this relates to the next lesson. Increasingly I see this format as a form of apprenticeship, where the students learn by doing, and see links (which I have yet to explore) to experiential learning. In addition to the two formats above (traditional classroom-lecture and flipped classroom), I have also used portions of these notes to teach short courses in different places (e.g., the University of Western Australia and the Gran Sasso Scientific Institute in Italy). The materials can, with some relatively minor modifications, be used in this way. As I continue to work on these notes, I hope to be able to add optional (or bonus) chapters, that could be used 1) to extend a course on spatial statistics beyond the 13 week horizon of the Canadian term, and/or 2) to offer more advanced material to interested readers see here for an example on spatial filtering. Audience The notes were designed for a course in geography, but in fact, could be easily adjusted for an audience of earth scientists, environmental scientists, econometricians, planners, or students in other disciplines who have an interest in and work with georeferenced datasets. The prerequisites are an introductory college/university level course on multivariate statistics, ideally covering the fundamentals of probability, hypothesis testing, and multivariate linear regression analysis. Requisites To fully benefit from this text, up-to-date copies of R and RStudio are highly recommended. Many examples in the text use datasets that have been packaged for convenience as an R package. To install the package (geog4ga3) use the following command(which requires devtools): library(devtools) devtools::install_github(&quot;paezha/Spatial-Statistics-Course&quot;, subdir = &quot;geog4ga3&quot;) The source files for the chapters and activites can be obtained from the following GitHub repository: https://github.com/paezha/paezha.github.io/tree/master/applied_spatial_statistics It is also possible to suggest edits to the text, and it only requires a GitHub account (sign-up at github.com). After logging into GitHub, you can click on the ‘edit me’ icon (the fourth icon on the left top corner of the toolbar at the top). Words of Appreciation I would like to express my gratitude to the Paul R. MacPherson Institute for Leadership, Innovation and Excellence in Teaching. The Institute supported, through its Student Partners program, my work with some amazing student partners. As part of this program, I worked with Mr. Rajveer Ubhi in the Fall of 2018 and Winter of 2019 organizing all the materials for the text, documenting the code, and ensuring that it satisfied student needs. I also had the opportunity to work with Ms. Megan Coad and Ms. Alexis Polidoro in the Fall of 2019 and Winter of 2020. As former students of the course, Ms. Coad and Polidoro helped to develop a set of mini-lectures to accompany the materials, continued to document the code, and tested the activities. In the Winter 2020 they will also accompany me in the classroom to work directly with new students when we offer the course again. Dr. Anastasios Dardas helped develop illustrative applications that helped us understand the value of interactivity in delivering many of the contents. Working with these wonderful individuals has been a pleasure, and I am grateful for their contributions to this effort. Versioning These notes were developed using the following version of R: ## _ ## platform x86_64-w64-mingw32 ## arch x86_64 ## os mingw32 ## system x86_64, mingw32 ## status ## major 3 ## minor 6.2 ## year 2019 ## month 12 ## day 12 ## svn rev 77560 ## language R ## version.string R version 3.6.2 (2019-12-12) ## nickname Dark and Stormy Night References "],
["preliminaries-installing-r-and-rstudio.html", "Chapter 1 Preliminaries: Installing R and RStudio 1.1 Introduction 1.2 Learning Objectives 1.3 R: The Open Statistical Computing Project 1.4 Packages in R", " Chapter 1 Preliminaries: Installing R and RStudio 1.1 Introduction Statistical analysis is the study of the properties of a dataset. There are different aspects of statistical analysis, and they often require that we work with data that are messy. According to Wickham and Grolemund (2016), computer-assisted data analysis includes the steps outlined in Figure 1.1. First, the data are imported to a suitable software application. This can include data from primary sources (suppose that you collected coordinates using a GPS) or from secondary sources (the Census of Canada). Data will likely be text tables, or an Excel file, among other possible formats. Before data can be analyzed, they need to be tidied. This means that the data need to be arranged in such a way that they match the process that you are interested in. For instance, a travel survey can be organized so that each row is a traveler, or as an alternative so that each row is a trip. Once that data are tidy, Exploratory Data Analysis (EDA) and/or its geographical extension Exploratory Spatial Data Analysis (ESDA) can be conducted. This involves transforming the raw data into information. Examples of transformations include calculating the mean and the standard deviation. Visualization is also part of this exploratory exercise. In EDA this could be creating a histogram or a scatterplot. Mapping is a key visualization technique in spatial statistics. Modeling is a process that further extracts information from the data, typically by looking at relationships between multiple variables. All of the tasks mentioned above, and many more, can be handled easily in a variety of software applications. For this course, you will use the statistical computing language R. Figure 1.1: The process of doing data analysis (from Wickham and Grolemund, 2016) 1.2 Learning Objectives In this reading, you will learn: How to install R. About the RStudio Interactive Development Environment. About packages in R. 1.3 R: The Open Statistical Computing Project 1.3.1 What is R? R is an open-source language for statistical computing. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland, in New Zealand, as a way to offer their students an accessible, no-cost tool for their courses. R is now maintained by the R Development Core Team, and is developed by hundreds of contributors around the globe. R is an attractive alternative to other software applications for data analysis (e.g., Microsoft Excel, STATA) due to its open-source character (i.e., it is free), its flexibility, and large and dedicated user community. The presence of a very active community of developers and users, especially in an open context, means if there is something you want to do (for instance, linear regression), it is very likely that someone has already developed functionality for it in R. A good way to think about R is as a core package, to which a library, consisting of additional packages, can be attached to increase its functionality. R can be downloaded for free at: https://cran.rstudio.com/ R comes with a built-in console (a user graphical interface), but better alternatives to the basic interface exist, including RStudio, an Integrated Development Environment, or IDE for short. RStudio can also be downloaded for free, by visiting the website: https://www.rstudio.com/products/rstudio/download/ R requires you to work using the command line, which is going to be unfamiliar to many of you accustomed to user-friendly graphical interfaces. Do not fear. People worked for a long time using the command line, or even more cumbersomely, using punched cards in early computers. Graphical user interfaces are convenient, but they have a major drawback, namely their inflexibility. A program that functions based on graphical user interfaces allows you to do only what is hard-coded in the user interface. Command line, as we will see, is somewhat more involved, but provides much more flexibility in operation, and it frees you from the constraints inherent in a point-and-click system. Go ahead. Install R and RStudio in your computer. (If you are at McMaster working in the GIS lab, you will find that these have already been installed there). Before introducing some basic functionality in R, lets quickly take a tour of the RStudio IDE. 1.3.2 The RStudio IDE The RStudio IDE provides a very complete interface to interact with the language R, and do much more in addition. It consists of a window with several panes. Some panes include, in addition, several tabs. There are the usual drop-down menus for common operations, such as creating new files, saving, common commands for editing, etc. See Figure 1.2 below. Figure 1.2: The RStudio IDE The editor pane allows you to open and work with text and other files, where you can write instructions that can be passed on to the program. Writing something in the editor does not execute any instructions, it merely records them for possible future use. In fact, much of what is written in the editor will not be instructions, but rather comments, discussion, and other text that is useful to understand code. The console pane is where instructions are passed on to the program. When an instruction is typed (or copied and pasted) there, R will understand that it needs to do something. The instructions must be written in a way that R understands, otherwise errors will occur. If you have typed instructions in the editor, you can use “ctrl-Enter” (in Windows) or “cmd-Enter” (in Mac) to send to the console and execute. The environment is where all data that is currently in memory is reported. The History tab acts like a log: it keeps track of the instructions that have been executed in the console. The last pane includes a number of useful tabs. The File tab allows you to navigate your computer, change the working directory, see what files are where, and so on. The Plot tab is where plots are rendered, when instructions require R to do so. The Packages tab allows you to manage packages, which as mentioned above, are pieces of code that can augment the functionality of R. The Help tab is where you can consult the documentation for functions/packages/see examples, and so on. The Viewer tab is for displaying local web content, for instance, to preview a Notebook (more on Notebooks soon). This brief introduction should have allowed you to install both R and RStudio. The next thing that you will need is a library of packages. 1.4 Packages in R According to Wickham (2015) packages are the basic units of reproducible code in the R multiverse. Packages allow a developer to create a self-contained unit of code that often is meant to achieve some task. For instance, there are packages in R that specialize in statistical techniques, such as cluster analysis, visualization, or data manipulation. Some packages can be miscellaneous tools, or contain mostly datasets. Packages are a very convenient way of maintaining code, data, and documentation, and also of sharing all these resources. Packages can be obtained from different sources (including making them!). One of the reasons why R has become so successful is the relative facility with which packages can be distributed. A package that I use frequently is called tidyverse (Wickham 2017). The tidyverse is a collection of functions for data manipulation, analysis, and visualization. This package can be downloaded and installed in your personal library of R packages by using the function install.packages, as follows: install.packages(&quot;tidyverse&quot;) The function install.packages retrieves packages from the Comprehensive R Archive Network, or CRAN for short. CRAN is a collection of sites (accessible via the internet) that carry identical materials for distribution for R. There are other ways of distributing packages. For instance, throughout this book you will make use of a package called geog4ga3 that contains a collection of datasets and functions used in the readings or activities. This package is not on CRAN, but instead can be obtained from GitHub, a repository and versioning system. To retrieve packages from GitHub you need a function called install_github, which in turn is part of the package devtools. To download and install the package geog4ga3, you need first to download and install devtools as follows: install.packages(&quot;devtools&quot;) Once that a package has been downloaded and installed, it needs to be loaded into a session to be available to use. I find it useful to think of packages that I download as “books” that I place in my personal “bookshelf”. Some “books” I obtain from the central library (i.e., CRAN), while others are shared by friends, and some I have even written myself. Once that the “books” are in my “bookshelf” they are part of my own personal library. This means that they are available for use. Next time I want to use a “book” from my library, I need to retrieve it from the bookshelf. This is similar to taking the book and opening it on my desk: now all the magic contained in the package is available for use! Similarly, once that the book is in my library, I do not need to retrieve it again from the bookstore - a package, once installed, does not need to be installed again (it might need updates, but that is a different matter). This analogy suggests that I can have many packages in my library, only some of which I may need at any specific time for a task. To retrieve a package (i.e., a book) from the library, so that we can use it, the function library is invoked as in this example: library(devtools) This allows you to use all the functions in the package devtools. In particular, at this point you want to use a function that allows you to retrieve other packages! With the functionality of devtools::install_github you can download and install the companion package for the book by running the following instruction: install_github(&quot;paezha/Spatial-Statistics-Course&quot;, subdir=&quot;geog4ga3&quot;) This will install the package (i.e., put it in your library) so that you can also benefit from its functionality. References "],
["basic-operations-and-data-structures-in-r.html", "Chapter 2 Basic Operations and Data Structures in R 2.1 Learning Objectives 2.2 RStudio IDE 2.3 Some Basic Operations 2.4 Data Classes in R 2.5 Data Types in R 2.6 Indexing and Data Transformations 2.7 Visualization 2.8 Creating a Simple Map", " Chapter 2 Basic Operations and Data Structures in R NOTE: You can download the source files for this book from here. The source files are in the format of R Notebooks. Notebooks are pretty neat, because the allow you execute code within the notebook, so that you can work interactively with the notes. The preceding chapter showed you how to install R and RStudio, and explained some key concepts, such as packages as fundamental units of reproducible code, and the concept of your library (where the packages that you install are stored). Now that you have installed R and RStudio we can begin with an overview of basic operations and data structures in this computing language. Please note that this document you are reading, called an R Notebook, is an example of what is called literate programming, a style of document that uses code to illustrate a discussion, as opposed to the traditional programming style that uses natural language to discuss/document the code. By focusing on natural language as opposed to computer-speak, literate programming flips around the usual manner of technical writing to make documents more intuitive and accessible. Whenever you see a chunk of code in an R Notebook, you can run it (by clicking the ‘play’ icon on the top right corner) to see the results. Try it! print(&quot;Hello, Geography 4GA3&quot;) ## [1] &quot;Hello, Geography 4GA3&quot; If, instead of the R Notebook, you are reading this chapter as a WebBook or in print, you can type or copy and paste the instructions on your console. Try it! The chunk of code above instructed R (and through R the computer) to print (or display on the screen) some text. 2.1 Learning Objectives In this practice, you will learn: Basic operations in R. Data classes, data types, and data transformations. More about the use of packages in R. Basic visualization. 2.2 RStudio IDE If you are reading this, you probably already read the introductory chapter that instructed you to install R and RStudio. We can now proceed to discuss some basic concepts of operations and data types. 2.3 Some Basic Operations R can perform many types of operations. Some simple operations are arithmetic. Other are logical. And so on. For instance, R can be instructed to conduct sums, as follows: # `R` understands numbers and arithmetic operators such as `+` for addition 2 + 2 ## [1] 4 R can be instructed to do multiplications: # The sign to instruct `R` to multiply is `*` 2 * 3 ## [1] 6 And sequences of operations, possibly using brackets to indicate their order. Compare the following two expressions: 2 * 3 + 5 ## [1] 11 2 * (3 + 5) ## [1] 16 Other operations produce logical results (values of true and false): # Is the statement true? 3 &gt; 2 ## [1] TRUE # Is this true? 3 &lt; 2 ## [1] FALSE And of course, you can combine operations in an expression: 2 * 3 + 5 &lt; 2 * (3 + 5) ## [1] TRUE As you can see, R can be used as a calculator, but it is much more powerful than that. We can also create variables. You can think of a variable as a box with a name, whose contents can change. Variables are used to keep track of important stuff in your calculations, and to automate operations. To create a variable, a value is assigned to a name, using this notation &lt;-. You can read this x &lt;- 2 as “assign the value of 2 to a variable called x”. For instance: # `&lt;-` means &quot;put the value of 2 in the object called `x`&quot; x &lt;- 2 # `&lt;-` means &quot;put the value of 3 in the object called `y`&quot; y &lt;- 3 # `&lt;-` means &quot;put the value of 5 in the object called `z`&quot; z &lt;- 5 Check your “Global Environment”, the tab where the contents of your “Workspace” are displayed for you. You can also simply type the name of the variable in the Console to see its contents. Now that we have some variables with values, we can express operations as follows (same as above) x * y + z ## [1] 11 x * (y + z) ## [1] 16 However, if we wanted, we could change the values of any of x, y, and/or z and repeat the operations. This allows to automate some instructions: x &lt;- 4 x * y + z ## [1] 17 The famous mathematician Henri Poincaré once wrote that “[m]athematics is the art of giving the same name to different things”. Working with a computer language is a lot like that: giving the same name to different values allows us to explore with ease “what would happen if…”. It is a very powerful tool to help us understand the world. 2.4 Data Classes in R As you saw above R can work with different data classes. Some data are numbers. Other data are logical (i.e., take values of TRUE or FALSE). These are some data classes: Numerical Character Logical Factor The existence of different data classes is very useful, since it allows you to store information in different forms. For instance, you may want to save some text: name &lt;- &quot;Hamilton&quot; Or numerical information: population &lt;- 551751 If you wish to check what class an object is, you can use the function class: class(name) ## [1] &quot;character&quot; class(population) ## [1] &quot;numeric&quot; 2.5 Data Types in R R can work with different data types, including scalars (essentially matrices with only one element), vectors (matrices with one dimension of size 1) and matrices (more generally). print(&#39;This is a scalar&#39;) ## [1] &quot;This is a scalar&quot; 1 ## [1] 1 print(&#39;This is a vector&#39;) ## [1] &quot;This is a vector&quot; # c() is a function to concatenate, that is, to put values in a vector c(1,2,3,4) ## [1] 1 2 3 4 print(&#39;This is a matrix&#39;) ## [1] &quot;This is a matrix&quot; # matrix() creates a two-dimensional array with `nrow` rows, and `ncol` columns matrix(c(1,2,3,4),nrow = 2, ncol=2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 The command c() is used to concatenate the arguments, that is, to join them in a single object. The objects must be of the same class: they must be all numeric, or all character, or all logical, and so on. We cannot combine different data classes in a vector. The command matrix() creates a matrix with the specified number of rows and columns. An important data type in R is a data frame. A data frame is a table consisting of rows and columns - commonly a set of vectors that have been collected for convenience. A data frame is used to store data in digital format. (If you have used Excel or another spreadsheet software before, data frames will be familiar to you: they look a lot like a sheet in a spreadsheet.) A data frame can accommodate large amounts of information (several billion individual items). The data can be numeric, character, logical, and so on. Each grid cell in a data frame has an address that can be identified based on the row and column it belongs to. R can use these addresses to perform mathematical operations. `R`` labels columns alphabetically and rows numerically (or less commonly alphabetically). To illustrate a data frame, let us first create the following vectors, that include names (character class), populations (numeric class), average salaries (numeric class), and coordinates (numeric class) of some cities: # c() is a function to concatenate, that is, to put values in a vector Name &lt;- c(&#39;Hamilton&#39;,&#39;Waterloo&#39;,&#39;Toronto&#39;) Population &lt;- c(551751, 219153, 2731571) AvgSalary &lt;- c(45692, 57625, 48920) Latitude &lt;- c(43.255203, 43.4668, 43.6532) Longitude &lt;- c(-79.843826, -80.51639, -79.3832) Again, note that &lt;- is an assignment. In other words, it assigns the item on the right to the name on the left. After you execute the chunk of code above, you will notice that new values appear in your Environment. These are five vectors of size 1:3. You can also see what is the class of the vector: one that is composed of alphanumeric information (or chr, for ‘character’) and four columns that are numeric (num). These vectors can be collected in a dataframe. This is done for convenience, so we know that all these data belong together in some way. Please note that to create a data frame, the vectors must have the same length. In other words, you cannot create a table with elements that have different numbers of rows (other data types allow you to do this, but not data frames). We will now create a data frame. We will call it “Cities”. There are rules for names (for example, they cannot begin with a number), but in most cases it helps if the names are intuitive and easy to remember. The function used to create a data frame is data.frame() and the arguments are the vectors that we wish to collect there. Cities &lt;- data.frame(Name, Population, AvgSalary, Latitude, Longitude) After running the chunk above, now you have a new object in your environment, namely a data frame called Cities. If you double clic on Cities in the Environment tab, you will see that this data frame has five columns (labeled Name, Population, AvgSalary, Latitude, and Longitude), and three rows. You can enter data into a data frame and then use the many built-in functions of R to perform various types of analysis. At this point, you may notice that Name, which was an alphanumeric vector, was converted to a factor in the data frame. A factor (data class) is a way to store nominal/categorical variables that may have two or more levels. Nominal variables are like labels. In the present case, the factor variable has three levels, corresponding to three cities. If we had information for multiple years, each city might appear more than once, for each year that information was available. 2.6 Indexing and Data Transformations Data frames store information that is related in a compact way. To perform operations effectively, it is useful to understand the way R locates information in a data frame. As noted before, each grid cell has an address, or in other words an index, that can be referenced in several convenient ways. For instance, assume that you wish to reference the first value of the data frame, that is, row 1 of column Name. To do this, you would use the following instruction: # To index elements in a data frame we use square brackets `[]` the first number in the square bracket is the row, and the second number (separated by a comma) is the column Cities[1,1] ## [1] Hamilton ## Levels: Hamilton Toronto Waterloo This will recall the element in the first row and first column of Cities. It also tells you what are the levels of this variable. As an alternative, you could type: Cities$Name[1] ## [1] Hamilton ## Levels: Hamilton Toronto Waterloo As you see, this has the same effect. The string sign $ is used to reference columns in a data frame. Therefore, R will call the first element of Name in data frame Cities. Cities[1,2] is identical to Cities$Name[2]. Try changing the code in the chunk and executing. If you type Cities$Name, R will recall the full column. Indexing is useful to conduct operations. Suppose for instance, that you wished to calculate the total population of two cities, say Hamilton and Waterloo. You can execute the following instructions: # The string sign `$` is used to make reference to a column in the data frame. The square brackets index the row in the column. Cities$Population[1] + Cities$Population[2] ## [1] 770904 (More involved indexing is also possible, for example, if we use logical operators. Do not worry too much about the details at this point, just verify that the results are identical) # The indexing now is a logical statement. The double equal sign `==` is used to make logical comparisons. `R` will find the rows for which `Cities$Name==&#39;Hamilton&#39;` in the first element of the sum, and the rows for which `Cities$Name==&#39;Waterloo&#39;` is true in the second element of the sum. Cities$Population[Cities$Name==&#39;Hamilton&#39;] + Cities$Population[Cities$Name==&#39;Waterloo&#39;] ## [1] 770904 Suppose that you wanted to calculate the total population of the cities in your data frame. To do this, you would use the function sum(): # `sum()` is a function to add all elements in a numerical vector - which could be a column in a data frame sum(Cities$Population) ## [1] 3502475 You have already seen how it allows you to store in memory the results of some instruction, by means of an assignment &lt;-. You can also perform many other useful operations. For instance, calculate the maximum value for a set of values: # `max()` finds the maximum value in a numerical vector max(Cities$Population) ## [1] 2731571 And, if you wanted to find which city is the one with the largest population, you would use a logical statement as an index: # `R` will find all rows for which the statement `Cities$Population==max(Cities$Population)`, that is, all the rows with a population identical to the maximum population! Cities$Name[Cities$Population==max(Cities$Population)] ## [1] Toronto ## Levels: Hamilton Toronto Waterloo As you see, Toronto is the largest city (by population) in this dataset. Using indexing in imaginative ways provides a way to do fairly sophisticated data analysis. Likewise, the function for finding the minimum value for a set of values is min(): # `min() finds the minimum value in a numerical vector min(Cities$Population) ## [1] 219153 Try calculating the average of the population of the cities, using the command mean(). Use the empty chunk below for this (the result should be 1167492), or do this in your console in RStudio: Finding the maximum and minimum, aggregating (calculating the sum of a series of values), and finding the average are examples of transformations applied to the data. They give insights into aspects of the dataset that are not necessarily evident from the raw data, especially if the number of observations (or cases) is large. Imagine trying to visually scan a spreadsheet with ten thousand observations to find the maximum value stored there! 2.7 Visualization The data frame, in essence a table, informative as it is, is no usually the best way to learn from the data. Transformations (or descriptive statistics as discussed above) are helpful to understand important properties of a dataset. In addition, visualization is often a valuable complement to data analysis. Say, we might be interested in finding which city has the largest population and which city has the smallest population in a dataset. We could achieve this by using similar instructions as before, for example: # `paste()` is similar to `print()`, except that it converts everything to characters before printing. We use this function because the contents of `Name` in the data frame `Cities` are not characters, but levels of a factor` paste(&#39;The city with the largest population is&#39;,Cities$Name[Cities$Population==max(Cities$Population)]) ## [1] &quot;The city with the largest population is Toronto&quot; paste(&#39;The city with the smallest population is&#39;, Cities$Name[Cities$Population==min(Cities$Population)]) ## [1] &quot;The city with the smallest population is Waterloo&quot; Another way, perhaps more convenient of understanding these data is by visualizing them, using for instance a bar chart. We will proceed to create a bar chart, using a package called ggplot2. This package implements a grammar of graphics, and is a very flexible way of creating plots in R. Since ggplot2 is a package, we first must ensure that it is installed. You can install it using the command install as follows: # Once you have installed a package, it does not need to be installed again! It already is in your library and you only need to load it with `library()` install.packages(&quot;ggplot2&quot;) As an alternative to the install.packages() function, you can use the Packages tab in RStudio. Simply navigate to the tab, click install, and select ggplot2 from there. Note that you need to install the package only once! Essentially install adds it to your library of packages, where it will remain available. Once the package is installed, it becomes available, but to use it you must load it in memory (similar to opening a “book” on your desktop as you work). For this, we use the command library(), which is used to load a package, that is, to activate it for use. Assuming that you already have installed ggplot2, we proceed to load it: library(ggplot2) Now all commands from the ggplot2 package are available to you. The package ggplot2 works by layering a series of objects, beginning with a blank plot, to which we can add things. The command to create a plot is ggplot(). This command accepts different arguments. For instance, we can pass data to it in the form of a data frame. We can also indicate different aesthetic values, that is, the things that we wish to plot. None of this is plotted, though, until we indicate which kind of geom or geometric object we wish to plot. For a bar chart, we would use the following instructions: # The function `ggplot()` creates an object for plotting, using a data frame as indicated by the input argument `data =`. Furthermore, we can specify how to map elements in the data frame to things in the plot. In this example, we wish to map the names of cities to the x-axis of the plot, and the population to the y-axis of the plot. Accordingly, we define as aesthetic values `aes()` `x = Name` and `y = Population`. The geometric object that we wish to plot is bars, so we use `geom_bar()` with the argument `stat = &quot;identity&quot;` so the data are not transformed before plotting: ggplot(data = Cities, aes(x = Name, y = Population)) + geom_bar(stat = &quot;identity&quot;) Since this is the first time that we use ggplot(), it is informative to break down these instructions. We are asking ggplot2 to create a plot that will use the data frame Cities. Furthermore, we tell it to use the values of Names in the x-axis, and the values of Population in the y-axis. Run the following chunk: ggplot(data = Cities, aes(x = Name, y = Population)) Notice how ggplot2 creates a blank plot, and it has yet to actually render any of the population information in there. We layer elements on a plot by using the + sign. It is only when we tell the package to add some geometric element that it renders something on the plot. In the previous case, we told ggplot2 to draw bars (by using the geom_bar() function). The argument of geom_bar was stat = 'identity', to indicate that the data for the y-axis was to be used ‘as-is’ without further statistical transformations. There are many different geoms that can be used in ggplot2. You can always consult the help/tutorial files by typing ??ggplot2 in the console. See: ??ggplot2 2.8 Creating a Simple Map We will see how maps are used in spatial statistical analysis. The simplest one that can be created is a so-called dot map. A dot map simply displays the locations of events of interest, as points. A dot map is, in fact, simply a scatterplot of the coordinates of events. We can use ggplot2 to create a simple dot map of the cities in our sample dataset. For this, we create a ggplot2 object, and for the x and y aesthetics we use the coordinates. The geometric element that we want to render is a point: # The longitude is mapped to the x-axis of the plot and the latitude is map to the y-axis of the plot. The function `geom_points()` is used to draw points: ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point() This is a simple dot map that simply shows the locations of the cities. We can add labels by means of the geometric element text: # `geom_text()` is used to write text on the plot, still using the longitude and latitude information: ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point() + geom_text(aes(label = Name)) The dot map above tells us the location of the cities in our dataframe and their name. We can include more information in the plot in different ways. For example, a proportional symbol map changes the size of the symbols (the points) to add information to the plot. To create a proportional symbol map, we add to the aesthetics the instruction to use some variable for the size of the symbols: # The `size` of the points will be proportional to the `Population` values in the data frame ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point(aes(size = Population)) + geom_text(aes(label = Name)) AFurthermore, we can fix the position of the labels by adding a vertical justification to the text (vjust), and to avoid the text from being cut we can also expand the limits of the plot (expand_limits()): ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point(aes(size = Population)) + geom_text(aes(label = Name), vjust = 2) + expand_limits(x = c(-80.7, -79.2), y = c(43.2, 43.7)) The example above has guided you in the creation of a relatively simple proportional symbols map! You can see that creating a plot is simply a matter of instructing R (through ggplot2) to complete a series of instructions: Create a ggplot2 object using a dataset, which will render stuff at locations given by variable1 and variable 2: ggplot(data = dataset, aes(x = variable1, y = variable2)) Add stuff to the plot. For instance, to add points use geom_point, to add lines use geom_line, and so on. Check the ggplot2 Cheat Sheet for more information on how to use this package. A last note. Many other visualization alternatives (for instance, Excel) provide point-and-click functions for creating plots. In contrast, ggplot2 in R requires that the plot be created by meticulously instructing the package what to do. While this is more laborious, it also means that you have complete control over the creation of plots, which in turn allows you to create more flexible and inventive visuals. Below are some of figures that I have created using R in recent years, including diagrams, thematic maps, and raster data. Figure 2.1: Example of visualization: diagram of catchment areas for accessibility analysis (from Paez, Higgins, and Vivona (2018) Figure 2.2: Example of visualization: accessibility to family doctors in Hamilton (from Paez, Higgins, and Vivona (2018) Figure 2.3: Example of visualization: water sources (triangles) and households (circles) in a region in central Kenya (from Paez et al. (2020) This concludes your basic overview of basic operations and data structures in R. You will have an opportunity to learn more about creating maps in R with your reading. "],
["introduction-to-mapping-in-r.html", "Chapter 3 Introduction to Mapping in R 3.1 Learning Objectives 3.2 Suggested Readings 3.3 Preliminaries 3.4 Packages 3.5 Exploring Dataframes and a Simple Proportional Symbols Map 3.6 Improving on the Proportional Symbols Map 3.7 Some Simple Spatial Analysis 3.8 Other Resources", " Chapter 3 Introduction to Mapping in R NOTE: You can download the source files for this book from here. The source files are in the format of R Notebooks. Notebooks are pretty neat, because the allow you execute code within the notebook, so that you can work interactively with the notes. Spatial statistics is a sub-field of spatial analysis that has grown in relevance in recent years as a result of 1) the availability of information that is geo-coded, in other words, that has geographical references; and 2) the availability of software to analyze such information. A key technology fuelling this trend is that of Geographical Information Systems (GIS). GIS are, in simplest terms, digital mapping for the 21st century. In most cases, however, GIS go beyond cartographic functions to also enable and enhance our ability to analyze data. There are many available packages for geographical information analysis. Some are very user friendly, and widely available in many institutional contexts, such as ESRI’s Arc software. Others are fairly specialized, such as Caliper’s TransCAD, which implements many operations of interest for transportation engineering and planning. Others packages have the advantage of being more flexible and/or free. Such is the case of the R statistial computing language. R has been adopted by many in the spatial analysis community, and a number of specialized libraries have been developed to support mapping and spatial data analysis functions. The objective of this note is to provide an introduction to mapping in R. Maps are one of the fundamental tools of spatial statistics and spatial analysis, and R allows for many GIS-like functions. If you wish to work interactively with this chapter you will need the following: An R markdown notebook version of this document (the source file). A package called geog4ga3. In the previous reading/practice you created a simple proportional symbols map. In this reading/practice you will learn how to create more sophisticated maps in R. 3.1 Learning Objectives In this reading, you will: Revisit how to install and load a package. Learn how to invoke a data and view the data structure. Learn how to easily create maps using R. Think about how statistical maps help us understand patterns. 3.2 Suggested Readings Bivand RS, Pebesma E, Gomez-Rubio V (2008) Applied Spatial Data Analysis with R, Chapters 2-3. Springer: New York Brunsdon C and Comber L (2015) An Introduction to R for Spatial Analysis and Mapping, Chapter 3. Sage: Los Angeles 3.3 Preliminaries It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: # The function `ls()` lists all objects in the Environment, that is, your current workspace; `rm()` removes all objects listed in the argument `list = ` rm(list = ls()) 3.4 Packages According to Wickham (2015) packages are the basic units of reproducible code in the R multiverse. Now that your workspace is clear, you can proceed to load a package. In this case, the package is the one used for this book/course, called geog4ga3: #The function &#39;library&#39; is used to load the data we want to work with. In this case, it is the geog4ga3 master package that we want to work with library(geog4ga3) ## Warning: replacing previous import &#39;plotly::filter&#39; by &#39;stats::filter&#39; when ## loading &#39;geog4ga3&#39; ## Warning: replacing previous import &#39;dplyr::lag&#39; by &#39;stats::lag&#39; when loading ## &#39;geog4ga3&#39; The package includes a few datasets that will be used throughout the book: #The function &#39;data&#39; is used to check if a dataset is present within any loaded packages. In this case, we are looking for &#39;snow_deaths&#39; data(&quot;snow_deaths&quot;) 3.5 Exploring Dataframes and a Simple Proportional Symbols Map If you correctly loaded the library, you can now access the dataframes in the package geog4ga3. For this section, you will need two dataframes, namely snow_pumps and snow_deaths: #The function &#39;head&#39; will display the first few rows of the dataframe, snow_deaths head(snow_deaths) ## long lat Id Count ## 0 -0.1379301 51.51342 1 3 ## 1 -0.1378831 51.51336 2 2 ## 2 -0.1378529 51.51332 3 1 ## 3 -0.1378120 51.51326 4 1 ## 4 -0.1377668 51.51320 5 4 ## 5 -0.1375369 51.51318 6 2 These data are from the famous London cholera example by John Snow (not the one from Game of Thrones, but the British physician). John Snow is considered the father of spatial epidemiology, and his study mapping the outbreak is credited with helping find its cause.This study investigates the cholera outbreak of Soho, London, in 1854. The dataframe snow_deaths includes the geocoded addresses of cholera deaths in long and lat, and the number of deaths (the Count) recorded at each address, as well as unique identifiers for the addresses (Id). A second dataframe snow_pumps includes the geocoded locations of water pumps in Soho: head(snow_pumps) ## long lat Id Count ## 01 -0.1366679 51.51334 251 1 ## 1100 -0.1395862 51.51388 252 1 ## 250 -0.1396710 51.51491 253 1 ## 310 -0.1316299 51.51235 254 1 ## 410 -0.1335944 51.51214 255 1 ## 510 -0.1359191 51.51154 256 1 As in your previous reading, it is possible to map the cases using ggplot2. Begin by loading the package tidyverse: #&#39;Tidyverse&#39; is a collection of R packages designed for data science used in everyday data analyses library(tidyverse) Now, you can create a blank ggplot2 object from which you can render the points for deaths and the pumps. #The function &#39;ggplot&#39; is used for data visualization - it creates a graph. The function &#39;geom_point&#39; tells R you want to create a plot of points. &#39;data = snow_deaths&#39; tells R you want to use the &#39;snow_deaths&#39; dataframe. &#39;aes&#39; stands for aesthetics of your graph where &#39;x = long&#39; sets the x axis to &#39;long&#39;, where &#39;y = lat&#39; sets the y axis to &#39;lat&#39;, where &#39;color = blue&#39; colours the points blue and &#39;shape = 16&#39; assigns the shape of the points - in this case, &#39;16&#39; are circles and &#39;17&#39; are triangles ggplot() + geom_point(data = snow_deaths, aes(x = long, y = lat), color = &quot;blue&quot;, shape = 16) + geom_point(data = snow_pumps, aes(x = long, y = lat), color = &quot;black&quot;, shape = 17) This map is a decent example of how to represent visually some contents in the dataframe. Here, information is displayed using different colours and symbols to represent pumps and deaths from the London Cholera Example. Though this map provides useful insights, it is not of the greatest quality. We will illustrate other ways of creating maps below, including interactive maps. 3.6 Improving on the Proportional Symbols Map A package that extends the functionality of mapping in R is leaflet. A key feature of the leaflet package is the ability to make maps interactive for the user. We will see next how to enhance our proportional symbol map using this package. First you need to load the package (you need to install it first if you have not already): # &#39;Leaflet&#39; is a package used for visualizing data on a map in R. &#39;Magrittr&#39; is a package used for creating pipe operators #install.packages(&#39;leaflet&#39;) # Run only if you have not yet installed `leaflet`! #install.packages(&#39;magrittr&#39;) # Run only if you have not yet installed `magrittr`! library(leaflet) library(magrittr) The first step is to create a leaflet object, which will be saved in m : # Here, we create a `leaflet` object and assign it to the variable, &#39;m&#39;. The &#39;setView&#39; function sets the view of the map where &#39;lng = -0.136&#39; sets the longitutde, &#39;lat = 51.513&#39; sets the latitude and the map zoom is set to 16. The &#39;%&gt;%&#39; is a pipe operator that passes the output from the left hand side of the operator to the first argument of the right hand side of the operator. In this case we are telling `R` that we want to center the map on the set longitude and latitude, with a zoom level of 16, which corresponds roughly to a neighborhood m &lt;- leaflet(data = snow_deaths) %&gt;% setView(lng = -0.136, lat = 51.513, zoom = 16) This map looks like this at this point: m The map is empty! This is because we have not yet added any geographical information to plot. We can begin by adding a basemap as follows: # We are adding a basemap or background map of the study location by means of the `addTiles` function to the &#39;m&#39; variable m &lt;- m %&gt;% addTiles() m The map now shows the neighborhood in Soho where the cholera outbreak happened. Now, at long last, we can add the cases of cholera deaths to the map. For this, we indicate the coordinates (preceded by ~), and set an option for clustering by means of the clusterOptions in the following fashion: # We are adding the cholera deaths to the map using &#39;group = Deaths&#39;. The &#39;~&#39; symbol tells R to use the same longitude and latitude values used in the previous block of code and the &#39;clusterOptions = markerClusterOptions()&#39; clusters a large number of markers on the map - in this case it is clusturing number of deaths into icons with numbers m &lt;- m %&gt;% addMarkers(~long, ~lat, clusterOptions = markerClusterOptions(), group = &quot;Deaths&quot;) m The map now displays the locations of cholera deaths on the map. If you zoom in, the clusters will rearrange accordingly. Try it! The other information that we have available is the location of the water pumps, which we can add to the map above (notice that the Broad Street Pump is already shown in the basemap!): m %&gt;% addMarkers(data = snow_pumps, ~long, ~lat, group = &quot;Pumps&quot;) An alternative and quicker way to run the same bit of code is by means of pipe operators (%&gt;%). These operators make writing code a lot faster, easier to read, and more intuitive! Recall that a pipe operator will take the output of the preceding function, and pass it on as the first argument of the next: m_test &lt;- leaflet () %&gt;% setView(lng = -0.136, lat = 51.513, zoom = 16) %&gt;% addTiles() %&gt;% addMarkers(data = snow_deaths, ~long, ~lat, clusterOptions = markerClusterOptions(), group = &quot;Deaths&quot;) %&gt;% addMarkers(data = snow_pumps, ~long, ~lat, group = &quot;Pumps&quot;) m_test The above results in a much nicer map. Is this map informative? What does it tell you about the incidence of cholera and the location of the pumps? 3.7 Some Simple Spatial Analysis Despite the simplicity of this map, we can begin to do some spatial analysis. For instance, we could create a heatmap. You have probably seen heatmaps in many different situations before, as they are a popular visualization tool. Heatmaps are created based on a spatial analytical technique called kernel analysis. We will cover this technique in more detail later on. For the time being, it can be illustrated by taking advantage of the leaflet.extras package, which contains a heatmap function. Load the package as follows: #install.packages(&quot;leaflet.extras&quot;) # Run only if you have not installed `leaflet.extras` yet! library(leaflet.extras) Next, create a second leaflet object for this example, and call it m2. Notice that we are using the same setView parameters: m2 &lt;- leaflet(data = snow_deaths) %&gt;% setView(lng = -0.136, lat = 51.513, zoom = 16) %&gt;% addTiles() Then, add the heatmap. The function used to do this is addHeatmap. We specify the coordinates and the variable for the intensity (i.e., each case in the dataframe is representative of Count deaths at the address). Two parameters are important here, the blur and the radius. If you are working with the R notebook version of the book, experiment changing these parameters: # The &#39;addHeatmap&#39; function is making a heat map. We specify the coordinates, same as the block of code above. The &#39;intensity&#39; function sets a numeric value, the &#39;blur&#39; specifies the amount of blur to apply and the &#39;radius&#39; function sets the radius of each point on the heatmap m2 %&gt;% addHeatmap(lng = ~long, lat = ~lat, intensity = ~Count, blur = 40, max = 1, radius = 25) Lastly, you can also add markers for the pumps as follows: m2 %&gt;% addHeatmap(lng = ~long, lat = ~lat, intensity = ~Count, blur = 40, max = 1, radius = 25) %&gt;% addMarkers(data = snow_pumps, ~long, ~lat, group = &quot;Pumps&quot;) And everything together: m2_test &lt;- leaflet(data = snow_deaths) %&gt;% setView(lng = -0.136, lat = 51.513, zoom = 16) %&gt;% addTiles() %&gt;% addHeatmap(lng = ~long, lat = ~lat, intensity = ~Count,blur = 40, max = 1, radius = 25) %&gt;% addMarkers(data = snow_deaths, ~long, ~lat, clusterOptions = markerClusterOptions(), group = &quot;Deaths&quot;) %&gt;% addMarkers(data = snow_pumps, ~long, ~lat, group = &quot;Pumps&quot;) m2_test A heatmap (essentially a kernel density of spatial points; more on this in a later chapter) makes it very clear that most cases of cholera happend in the neighborhood of one (possibly contaminated) water pump! At the time, Snow noted with respect to this geographical pattern that: “It will be observed that the deaths either very much diminished, or ceased altogether, at every point where it becomes decidedly nearer to send to another pump than to the one in Broad street. It may also be noticed that the deaths are most numerous near to the pump where the water could be more readily obtained.” Snow’s analysis helped to convince officials to close the pump, after which the cholera outbreak subsided. This illustrates how even some relatively simple spatial analysis can help to inform public policy and even save lives. You can read more about this case here. In this practice you have learned how to implement some simple mapping and spatial statistical analysis using R. In future readings we will further explore the potential of R for both. 3.8 Other Resources For more information on the functionality of leaflet, please check Leaflet for R References "],
["mapping-in-r-continued.html", "Chapter 4 Mapping in R: Continued 4.1 Learning Objectives 4.2 Suggested Readings 4.3 Preliminaries 4.4 Summarizing a Dataframe 4.5 Factors 4.6 Subsetting Data 4.7 Pipe Operator 4.8 More on Visualization", " Chapter 4 Mapping in R: Continued NOTE: You can download the source files for this book from here. The source files are in the format of R Notebooks. Notebooks are pretty neat, because the allow you execute code within the notebook, so that you can work interactively with the notes. In the preceding chapters, you were introduced to the following concepts: Basic operations in R. These include arithmetic and logical operations, among others. Data classes in R. Data can be numeric, characters, logical values, etc. Data types in R. Ways to store data, for instance as vector, matrix, dataframes, etc. Indexing. Ways to retrieve information from a data frame by referring to its location therein. Creating simple maps in R. Please review the previous practices if you need a refresher on these concepts. If you wish to work interactively with this chapter you will need the following: An R markdown notebook version of this document (the source file). A package called geog4ga3. 4.1 Learning Objectives In this reading, you will learn: How to quickly summarize the descriptive statistics of a dataframe. More about factors. Factors are a class of data that is used for categorical data. For instance, a parcel may be categorizes as developed or undeveloped; a plot of land may be zoned for commercial, residential, or industrial use; a sample may be mineral x or y. These are not quantities but rather reflect a quality of the entity that is being described. How to subset a dataset. Sometimes you want to work with only a subset of a dataset. This can be done using indexing with logical values, or using specialized functions. More on the use of pipe operators. A pipe operator allows you to pass the results of a function to another function. It makes writing instructions more intuitive and simple. You have already seen pipe operators earlier: they look like this %&gt;%. You will add layers to a ggplot object to improve a map. 4.2 Suggested Readings Bivand RS, Pebesma E, Gomez-Rubio V (2008) Applied Spatial Data Analysis with R, Chapters 2-3. Springer: New York. Brunsdon C and Comber L (2015) An Introduction to R for Spatial Analysis and Mapping, Chapter 3. Sage: Los Angeles. O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapters 1-3. John Wiley &amp; Sons: New Jersey. 4.3 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(geog4ga3) ## Warning: replacing previous import &#39;plotly::filter&#39; by &#39;stats::filter&#39; when ## loading &#39;geog4ga3&#39; ## Warning: replacing previous import &#39;dplyr::lag&#39; by &#39;stats::lag&#39; when loading ## &#39;geog4ga3&#39; Now that your workspace is clear, you can proceed to invoke the sample dataset. You can do this by means of the function data. data(&quot;missing_df&quot;) The dataframe missing_df includes \\(n = 65\\) observations (Note: text between $ characters is mathematical notation in LaTeX). These observations are geocoded using a false origin and coordinates normalized to the unit-square (the extent of their values is between zero and one). The coordinates are x and y. In addition, there are three variables associated with the locations (VAR1, VAR2, VAR3). The variables are generic. Feel free to think of them as housing prices, concentrations in ppb of some contaminant or any other variable that will help clarify your understading. Finally, a factor variable states whether the variables were measured for a location: if the status is “FALSE”, the values of the variables are missing. 4.4 Summarizing a Dataframe Obtaining a set of descriptive statistics for a dataframe is very simple thanks to the function summary. For instance, the summary of missing_df is: # `summary()` reports basic descriptive statistics of columns in a data frame summary(missing_df) ## x y VAR1 VAR2 ## Min. :0.01699 Min. :0.01004 Min. : 50.0 Min. : 50.0 ## 1st Qu.:0.22899 1st Qu.:0.19650 1st Qu.: 453.3 1st Qu.: 570.1 ## Median :0.41808 Median :0.50822 Median : 459.1 Median : 574.4 ## Mean :0.49295 Mean :0.46645 Mean : 458.8 Mean : 562.1 ## 3rd Qu.:0.78580 3rd Qu.:0.74981 3rd Qu.: 465.4 3rd Qu.: 594.2 ## Max. :0.95719 Max. :0.98715 Max. :1050.0 Max. :1050.0 ## NA&#39;s :5 NA&#39;s :5 ## VAR3 Observed ## Min. : 50.0 FALSE: 5 ## 1st Qu.: 630.3 TRUE :60 ## Median : 640.0 ## Mean : 638.1 ## 3rd Qu.: 646.0 ## Max. :1050.0 ## NA&#39;s :5 This function reports the minimum, maximum, mean, median, and quantile values of a numeric variable. When variables are characters or factors, their frequency is reported. For instance, in missing_df, there are five instances of FALSE and sixty instances of TRUE. 4.5 Factors A factor describes a category. You can examine the class of a variable by means of the function class. From the summary, it is clear that several variables are numeric. However, for Observed, it is not evident if the variable is a character or factor. Use of class reveals that it is indeed a factor: class(missing_df$Observed) ## [1] &quot;factor&quot; Factors are an important data type because they allow us to store information that is not measured as a quantity. For example, the quality of the cut of a diamond is categorized as Fair &lt; Good &lt; Very Good &lt; Premium &lt; Ideal. Sure, we could store this information as numbers from 1 to 5. However, the quality of the cut is not a quantity, and should not be treated like one. In the dataframe missing_df, the variable Observed could have been coded as 1’s (for missing) and 2’s (for observed), but this does not mean that “observed” is twice the amount of “missing”! In this case, the numbers would not be quantities but labels. Factors in R allow us to work directly with the labels. Now, you may be wondering what does it mean when the status of a datum’s Observed variable is coded as FALSE. If you check the summary again, there are five cases of NA in the variables VAR1 through VAR3. NA essentially means that the value is missing. Likely, the five NA values correspond to the five missing observations. We can check this by subsetting the data. 4.6 Subsetting Data We subset data when we wish to work only with parts of a dataset. We can do this by indexing. For example, we could retrieve the part of the dataframe that corresponds to the FALSE values in the Observed variable: missing_df[missing_df$Observed == FALSE,] ## x y VAR1 VAR2 VAR3 Observed ## 61 0.34 0.83 NA NA NA FALSE ## 62 0.29 0.52 NA NA NA FALSE ## 63 0.13 0.32 NA NA NA FALSE ## 64 0.62 0.10 NA NA NA FALSE ## 65 0.88 0.85 NA NA NA FALSE Data are indexed by means of the square brackets [ and ]. The indices correspond to the rows and columns. The logical statement missing_df$Observed == False selects the rows that meet the condition, whereas leaving a blank for the columns simply means “all columns”. As you can see, the five NA values correspond, as anticipated, to the locations where Observed is FALSE. Using indices is only one of many ways of subsetting data. Base R also has a subset command, that is implemented as follows: subset(missing_df, Observed == FALSE) ## x y VAR1 VAR2 VAR3 Observed ## 61 0.34 0.83 NA NA NA FALSE ## 62 0.29 0.52 NA NA NA FALSE ## 63 0.13 0.32 NA NA NA FALSE ## 64 0.62 0.10 NA NA NA FALSE ## 65 0.88 0.85 NA NA NA FALSE And the package dplyr (part of the tidyverse) has a function called filter: filter(missing_df, Observed == FALSE) ## x y VAR1 VAR2 VAR3 Observed ## 1 0.34 0.83 NA NA NA FALSE ## 2 0.29 0.52 NA NA NA FALSE ## 3 0.13 0.32 NA NA NA FALSE ## 4 0.62 0.10 NA NA NA FALSE ## 5 0.88 0.85 NA NA NA FALSE The three approaches give the same result, but subset and filter are somewhat easier to write. You could nest any of the above approaches as part of another function. For instance, if you wanted to do a summary of the selected subset of the data, you would: summary(filter(missing_df, Observed == FALSE)) ## x y VAR1 VAR2 VAR3 ## Min. :0.130 Min. :0.100 Min. : NA Min. : NA Min. : NA ## 1st Qu.:0.290 1st Qu.:0.320 1st Qu.: NA 1st Qu.: NA 1st Qu.: NA ## Median :0.340 Median :0.520 Median : NA Median : NA Median : NA ## Mean :0.452 Mean :0.524 Mean :NaN Mean :NaN Mean :NaN ## 3rd Qu.:0.620 3rd Qu.:0.830 3rd Qu.: NA 3rd Qu.: NA 3rd Qu.: NA ## Max. :0.880 Max. :0.850 Max. : NA Max. : NA Max. : NA ## NA&#39;s :5 NA&#39;s :5 NA&#39;s :5 ## Observed ## FALSE:5 ## TRUE :0 ## ## ## ## ## Or: summary(missing_df[missing_df$Observed == FALSE,]) ## x y VAR1 VAR2 VAR3 ## Min. :0.130 Min. :0.100 Min. : NA Min. : NA Min. : NA ## 1st Qu.:0.290 1st Qu.:0.320 1st Qu.: NA 1st Qu.: NA 1st Qu.: NA ## Median :0.340 Median :0.520 Median : NA Median : NA Median : NA ## Mean :0.452 Mean :0.524 Mean :NaN Mean :NaN Mean :NaN ## 3rd Qu.:0.620 3rd Qu.:0.830 3rd Qu.: NA 3rd Qu.: NA 3rd Qu.: NA ## Max. :0.880 Max. :0.850 Max. : NA Max. : NA Max. : NA ## NA&#39;s :5 NA&#39;s :5 NA&#39;s :5 ## Observed ## FALSE:5 ## TRUE :0 ## ## ## ## ## Nesting functions makes it difficult to read the code, since functions are evaluated from the innermost to the outermost function, whereas we are used to reading from left to right. Fortunately, R implements (as part of package magrittr which is required by tidyverse) a so-called pipe operator that simplifies things and allows for code that is more intuitive to read. 4.7 Pipe Operator A pipe operator is written this way: %&gt;%. Its objective is to pass forward the output of a function to a second function, so that they can be chained to create more complex instructions that are still relatively easy to read. For instance, instead of nesting the subsetting instructions in the summary function, you could do the subsetting first, and pass the results of that to the summary for further processing. This would look like this: # Remember, the pipe operator `%&gt;%` takes pases the value of the left-hand side to the function on the right-hand side subset(missing_df, Observed == FALSE) %&gt;% summary() ## x y VAR1 VAR2 VAR3 ## Min. :0.130 Min. :0.100 Min. : NA Min. : NA Min. : NA ## 1st Qu.:0.290 1st Qu.:0.320 1st Qu.: NA 1st Qu.: NA 1st Qu.: NA ## Median :0.340 Median :0.520 Median : NA Median : NA Median : NA ## Mean :0.452 Mean :0.524 Mean :NaN Mean :NaN Mean :NaN ## 3rd Qu.:0.620 3rd Qu.:0.830 3rd Qu.: NA 3rd Qu.: NA 3rd Qu.: NA ## Max. :0.880 Max. :0.850 Max. : NA Max. : NA Max. : NA ## NA&#39;s :5 NA&#39;s :5 NA&#39;s :5 ## Observed ## FALSE:5 ## TRUE :0 ## ## ## ## ## The code above is read as “subset missing_df and pass the results to summary”. Pipe operators make writting and reading code somewhat more natural. 4.8 More on Visualization Observations in the sample dataset are georeferenced, and so they can be plotted. Since they are based on false origins and are normalized, we cannot map them to the surface of the Earth. However, we can still visualize their spatial distribution. This can be done by using ggplot2. For instance, for missing_df: # `coord_fixed()` forces the plot to use a ratio of 1:1 for the units in the x- and y-axis; in this case, since the values we are mapping to those axes are coordinates, we wish to represent them using the same scale, i.e., one unit in x looks identical to one unit in y (as an experiment, repeat the plot without fixing the coordinates) ggplot() + geom_point(data = missing_df, aes(x = x, y = y), shape = 17, size = 3) + coord_fixed() The above simply plots the coordinates, so that we can see the spatial distribution of the observations. (Notice the use of coord_fixed to maintain the aspect ratio of the plot to 1, i.e. the relationship between width and height). You have control of the shape of the markers, as well as their size. You can consult the shapes available here. Experiment with different shapes and sizes if you wish. The dataframe missing_df includes more attributes that could be used in the plot. For instance, if you wished to create a thematic map showing VAR1 you would do the following: ggplot() + geom_point(data = missing_df, aes(x = x, y = y, color = VAR1), shape = 17, size = 3) + coord_fixed() The shape and size assignments happen outside of aes, and so are applied equally to all observations. In some cases, you might want to let other aesthetic attributes vary with the values of a variable in the dataframe. For instance, if we let the sizes change with the value of the variable: ggplot() + geom_point(data = missing_df, aes(x = x, y = y, color = VAR1, size = VAR1), shape = 17) + coord_fixed() ## Warning: Removed 5 rows containing missing values (geom_point). Note how there is a warning, saying that five observations were removed because data were missing! These are likely the five locations where Observed == FALSE! To make it more clear which observations are these, you could set the shape to vary according to the value of Observed, as follows: ggplot() + geom_point(data = missing_df, aes(x = x, y = y, color = VAR1, shape = Observed), size = 3) + coord_fixed() Now it is easy to see the locations of the five observations that were Observed == FALSE!, which are labeled with grey circles. You can change the coloring scheme by means of scale_color_distiller (you can can check the different color palettes available here): ggplot() + geom_point(data = missing_df, aes(x = x, y = y, color = VAR1, shape = Observed), size = 3) + scale_color_distiller(palette = &quot;RdBu&quot;) + coord_fixed() You will notice maybe that with this coloring scheme some observations become very light and difficult to distinguish from the background. This can be solved in many different ways (for instance, by changing the color of the background!). A simple fix is to add a layer with hollow symbols, as follows: ggplot() + geom_point(data = missing_df, aes(x = x, y = y, color = VAR1), shape = 17, size = 3) + geom_point(data = missing_df, aes(x = x, y = y), shape = 2, size = 3) + scale_color_distiller(palette = &quot;RdBu&quot;) + coord_fixed() Finally, you could try subsetting the data to have greater control of the appareance of your plot, for instance: ggplot() + geom_point(data = subset(missing_df, Observed == TRUE), aes(x = x, y= y, color = VAR1), shape = 17, size = 3) + geom_point(data = subset(missing_df, Observed == TRUE), aes(x = x, y= y), shape = 2, size = 3) + geom_point(data = subset(missing_df, Observed == FALSE), aes(x = x, y= y), shape = 18, size = 4) + scale_color_distiller(palette = &quot;RdBu&quot;) + coord_fixed() These are examples of creating and improving the aspect of simple symbol maps, which are often used to represent observations in space. References "],
["activity-2-statistical-maps-ii.html", "Chapter 5 Activity 2: Statistical Maps II 5.1 Housekeeping Questions 5.2 Learning objectives 5.3 Suggested reading 5.4 Preliminaries 5.5 Activity", " Chapter 5 Activity 2: Statistical Maps II Remember, you can download the source file for this activity from here. 5.1 Housekeeping Questions Answer the following questions: How many examinations are there in this course? What is the date of the first examination? Where is the office of your instructor? 5.2 Learning objectives In this activity you will: Learn about patterns and processes, including random patterns. Understand the general approach to retrieve a process from a pattern. Discuss the importance of discriminating random patterns. 5.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapters 1-3. John Wiley &amp; Sons: New Jersey. 5.4 Preliminaries For this activity you will need the following: An R markdown notebook version of this document (the source file). A package called geog4ga3. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(geog4ga3) ## Warning: replacing previous import &#39;plotly::filter&#39; by &#39;stats::filter&#39; when ## loading &#39;geog4ga3&#39; ## Warning: replacing previous import &#39;dplyr::lag&#39; by &#39;stats::lag&#39; when loading ## &#39;geog4ga3&#39; Now that your workspace is clear, you can proceed to invoke the datasets required for this activity: data(&quot;missing_df&quot;) data(&quot;PointPattern1&quot;) data(&quot;PointPattern2&quot;) data(&quot;PointPattern3&quot;) The datasets include the following dataframe which will be used in the first part of the activity: missing_df This dataframe includes \\(n = 65\\) observations (Note: text between $ characters is mathematical notation in LaTeX). These observations are geocoded using a false origin and coordinates normalized to the unit-square (the extent of their values is between zero and one). The coordinates are x and y. In addition, there are three variables associated with the locations (VAR1, VAR2, VAR3). The variables are generic. Feel free to think of them as if they were housing prices or concentrations in ppb of some contaminant. Finally, a factor variable states whether the variables were measured for a location: if the status is “FALSE”, the values of the variables are missing. The following dataframes will be used in the second part of the activity: PointPattern1 PointPattern2 PointPattern3 The dataframes PointPattern* are locations of some generic events. The coordinates x and y are also based on a false origin and are normalized to the unit-square. Feel free to think of these events as cases of flu, the location of trees of a certain species, or the location of fires. 5.5 Activity *1. Create thematic maps for variables VAR1 through VAR3 in the dataframe missing_df. *2. Plot all three point patterns. Suppose that you were tasked with estimating the value of a variable for the locations where those were not measured. For instance, you could be a realtor, and you need to assess the value of a property, and the only information available is the published values of other properties in the region. As an alternative, you could be an environmental scientist, and you need to estimate what the concentration of a contaminant at a site, based on previous measurements at other sites in the region. Propose one or more ways to guess those missing values, and explain your reasoning. The approach does not need to be the same for all variables! Imagine that you are a public health official and you need to plan services to the public. If you were asked to guess where the next event would emerge, where would be your guess in each map? Explain your answer. "],
["maps-as-processes-null-landscapes-spatial-processes-and-statistical-maps.html", "Chapter 6 Maps as Processes: Null Landscapes, Spatial Processes, and Statistical Maps 6.1 Learning Objectives 6.2 Suggested Readings 6.3 Preliminaries 6.4 Random Numbers 6.5 Null Landscapes 6.6 Stochastic Processes 6.7 Simulating Spatial Processes 6.8 Processes and Patterns", " Chapter 6 Maps as Processes: Null Landscapes, Spatial Processes, and Statistical Maps NOTE: You can download the source files for this book from here. The source files are in the format of R Notebooks. Notebooks are pretty neat, because the allow you execute code within the notebook, so that you can work interactively with the notes. In last practice your learning objectives were: How to obtain a descriptive summary of a dataframe. Factors and how to use them. How to subset a dataframe. Pipe operators and how to use them. How to improve your maps. Please review the previous practices if you need a refresher on these concepts. If you wish to work interactively with this chapter you will need the following: An R markdown notebook version of this document (the source file). 6.1 Learning Objectives In this chapter, you will learn: How to generate random numbers with different properties. About Null Landscapes. About stochastic processes. How to create new columns in a dataframe using a formula. How to simulate a spatial process. 6.2 Suggested Readings Bivand RS, Pebesma E, Gomez-Rubio V (2008) Applied Spatial Data Analysis with R, Analysing Spatial Data (pp. 169-171). Springer: New York. O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 4. John Wiley &amp; Sons: New Jersey. 6.3 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) 6.4 Random Numbers Colloquially, we understand random as something that happens in an unpredictable manner. The same word in statistics has a precise meaning, as the outcome of a process that cannot be predicted with any form of certainty. The question whether random processes exist is philosophically interesting. In the early stages of the invention of science, there was much optimism that humans could one day understand every aspect of the universe. This notion is well illustrated by Laplace’s Demon, a hypothetical entity that could predict the state of the universe in the future based on an all-encompassing knowledge of the state of the universe at any past point in time (see here). There are two important limitations to this perspective. First, there is the assumption that the mechanisms of operation of phenomena are well understood (in the case of Laplace’s Demon, it was somewhat naively assumed that classical Newtonian mechanics were sufficient). And secondly, the assumption that all relevant information is available to the observer. There are many processes in reality that are not fully understood, which make Laplace’s Demon an interesting, but unreliable source on predicting the state of the universe. Furthermore, there are often constraints in terms of how much and how acurately information can be collected with respect to any given phenomenon. ##Types of Processes A process can be deterministic. However, When limited knowledge/limited information prevent us from being able to make certain predictions, we assume that the process is random. It is important to note that “random” does not mean that just any outcome is possible. For instance, if you flip a coin, there are only two possible outcomes. If you roll a dice, there are only six possible outcomes. The concentration of a pollutant cannot be negative. The height of a human adult cannot be zero or 10 meters. And so on. It is the result of the possible outcomes that is random, as there is no process controlling the respective outcome. Over time, many formulas have been devised to describe different types of random processes. A random probability distribution function describes the probability of observing different outcomes. For instance, a formula for processes similar to coin flips was discovered by Bernoulli in 1713 (see here). The following function reports a random binomial variable. The number of observations n is how many random numbers we require. The size is the number of trials. For instance, if the experiment was flipping a coin, it would be how many times we get heads in size flips. The probability of success prob is the probability of getting heads in any given toss. Execute the chunk repeatedly to see what happens. #This function simulates the outcome of flipping a coin. Here, we are simulating the result for flipping heads, which has a probability of 0.5. The value of `n` is the number of experiments and `size` is the number of trials in each experiment rbinom(n = 1, size = 1, prob = 0.5) ## [1] 0 It can be noted that although there are only two outcomes, we do not have control over the result of the process, making the result random. If you tried this “experiment” repeatedly, you would find that “heads” (1s) and “tails” (0s) appear each about 50% of the time. A way to implement this is to increase n- think of this as recruiting more people to do coin flips at the same time: n &lt;- 1000 # Number of people tossing the coin one time. coin_flips &lt;- rbinom(n = n, size = 1, prob = 0.5) sum(coin_flips)/n ## [1] 0.53 What happens if you change the size to 0, and why? The binomial function is an example of a discrete probability distribution function, because it can take only one of a discrete (limited) number of values (i.e., 0 and 1). Other random probability distribution functions are for continuous variables, variables that can take any value within a predefined range. The most famous of this distributions is the normal distribution, which you may know also as the bell curve. This probability distribution is attributed to Gauss (see here). The normal distribution is defined by a centering paramater (the mean of the distribution) and a spread parameter (the standard deviation). In the normal distribution, 68% of values are within one standard deviation from the mean, 95% of values are within two standard deviations from the mean, and 99.7% of values are within three standard deviations from the mean. The following function reports a value taken at random from a normal distribution with mean zero and standard deviation sd of one. Execute this chunk repeatedly to see what happens: # This function generates random numbers based on the normal distribution conditional on the given arguments, i.e., the mean and the standard deviation `sd`. rnorm(1, mean = 0, sd = 1) ## [1] 1.242477 Let’s say that the average height of men in Canada is 170.7 cm and the standard deviation is 7 cm. The heigh of a random person in this population would be: rnorm(1, mean = 170.7, sd = 7) ## [1] 170.4182 And the distribution of heights of n men in this population would be: #Creating a data frame using the random numbers generated from n=1000 people. The results in the data frame are then plotted using ggplot. The end result is a distribution of heights of 1000 men. You are able to see which heights are most common out of the sample. n &lt;- 1000 height &lt;- rnorm(n, mean = 170.7, sd = 7) height &lt;- data.frame(height) # `geom_histogram()` is a geometric object in `ggplot2` that represents the frequency of values in a vector as a bar chart ggplot(data = height, aes(x = height)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Men shorter than 150 cm would be extremely rare, as well as men taller than 190 cm. 6.5 Null Landscapes So what do random variables have to do with maps? Random variables can be used to generate purely random maps. These are called null landscapes or neutral landscapes in spatial ecology (With and King 1997) (Paper is available to download). The concept of null landscapes is quite useful. They provide a benchmark to compare the results of statistical maps. Let’s see how to generate a null landscape of events. Suppose that there is a landscape with coordinates in the unit square, that is divided in very small discrete units of land. Each of these units of land can be the location of an event. For example, a tree might be present; or a case of a disease. Let’s first create a landscape. For this, we will use the expand.grid function to find all combinations of two sets of coordinates in the unit interval, using small partitions: # expand.grid created a set of coordinates by obtaining all the combinations of the input variables. Here, our landscape ranges in the x-axis from 0 to 1, increasing by 0.05, and the y-axis also from 0 to 1, increasing by 0.05 coords &lt;- expand.grid(x = seq(from = 0, to = 1, by = 0.05), y = seq(from = 0, to = 1, by = 0.05)) Now, let’s generate a binomial random variable to go with these coordinates. # `nrow()` returns the number of rows that are present in a data frame. Here, it returns the number of rows in the data frame `coords` events &lt;- rbinom(n = nrow(coords), size = 1, prob = 0.5) We will collect the coordinates and the random variable in a dataframe for plotting: # `data.frame()` collects the inputs in a data frame; they must have the same number of rows null_pattern &lt;- data.frame(coords, events) We can plot the null landscape we just generated as follows: ggplot() + geom_point(data = filter(null_pattern, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() By changing the probability prob in the function rbinom you can make the event more or less likely, i.e., frequent. If you are working with the notebook version of this document you can try changing the parameters to see what happens. A continuous random variable can also be used to generate a null landscape. For instance, imagine that a group of individuals are asked to stand in formation, and that they arrange themselves purely at random. What would a map of their heights look like? First, we will generate a random variable using the same parameters we mentioned above for the height of men in Canada: #heights will be random numbers genreated based on the average height of men, 7 standard deviations, and the null landscape &quot;coords&quot; created previously. heights &lt;- rnorm(n = nrow(coords), mean = 170.7, sd = 7) The random values that were generated can be collected in a dataframe with the coordinates for the purpose of plotting: null_trend &lt;- data.frame(coords, heights) One possible map of heights when the individuals stand in formation at random would look like this: # Our plot is created based on the dataframe of coords and heights. The value of `x` is plotted to the x-axis, the value of `y` is plotted to the y-axis, and the color of the points depends on the values of `heights`. We can change the _scale_ of colors by means of `scale_color_distiller()`. There, palette `spectral` associates higher values of `heights` as red (taller men), while lower values of `heights` (i.e., shorter men) are appear in blue. More generally, we can control the scale of aesthetic aspects of the plot by means of scale_*something* (scale_shape, scale_size, etc.) ggplot() + geom_point(data = null_trend, aes(x = x, y = y, color = heights), shape = 15) + scale_color_distiller(palette = &quot;Spectral&quot;) + coord_fixed() These two examples illustrate only two of many possible techniques to generate null landscapes. We will discuss other strategies to work with null landscapes later in the course. 6.6 Stochastic Processes Some processes are random, such as the ones used above to create null landscapes. These processes take values with some probability, but cannot be predicted with any certainty. Let’s illustrate using again a unit square: # Remember that `expand.grid()` will find all combinations of values in the inputs coords &lt;- expand.grid(x = seq(from = 0, to = 1, by = 0.05), y = seq(from = 0, to = 1, by = 0.05)) Here is an example of a random pattern of events: # Create a random variable and join to the coordinates to generate a null landscape events &lt;- rbinom(n = nrow(coords), size = 1, prob = 0.5) null_pattern &lt;- data.frame(coords, events) # Plot the null landscape you just created ggplot() + geom_point(data = subset(null_pattern, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() A systematic or deterministic process is one that contains no elements of randomness, and can therefore be predicted with complete certainty. For instance (note the use of xlim to set the extent of x axis in the plot): # Copy the coordinates to a new object deterministic_point_pattern &lt;- coords # `mutate()` adds new variables to a data frame while preserving esixting variables. Here, we create a new column in our data frame, called `events` that will take the value of `x` (the position of an observation along the x-axis) and will `round()` it, i.e., if it is less than 0.5 it will round it to zero, and if it is equal to or greater than 0.5 it will round to 1 deterministic_point_pattern &lt;- mutate(deterministic_point_pattern, events = round(x)) # Plot the new landscape: `filter()` keeps the rows in a dataframe that meet a condition (for example, that the value of `events` is 1), and discards the rest ggplot() + geom_point(data = filter(deterministic_point_pattern, events == 1), aes(x = x, y = y), shape = 15) + xlim(0, 1) + coord_fixed() In the process above, we used the function round() and the coordinate x. The function gives a value of one for all points with x &gt; 0.5, and a value of zero to all points with x &lt;= 0.5. The pattern is fully deterministic: if I know the value of the x coordinate I can predict whether an event will be present. A stochastic process, on the other hand, is a process that is neither fully random or deterministic, but rather a combination of the two. Let’s illustrate: # Copy the coordinates to a new object stochastic_point_pattern &lt;- coords # Here, we combine the function `round()`, which does an deterministic operation, and `rbinom()` to generate a random number stochastic_point_pattern &lt;- mutate(stochastic_point_pattern, events = round(x) - round(x) * rbinom(n = nrow(coords), size = 1, prob = 0.5)) # Plot the new landscape ggplot() + geom_point(data = subset(stochastic_point_pattern, events == 1), aes(x = x, y = y), shape = 15) + xlim(0, 1) + coord_fixed() The process above has a deterministic component (the probability of an event is zero if x &lt;= 0.5), and a random component (the probability of a coordinate being an event is 0.5 when x &gt; 0.5). The landscape is not fully random, but also it is not fully deterministic. Instead, it is the result of a stochastic process, a process that combines deterministic and random elements. 6.7 Simulating Spatial Processes Null landscapes are interesting as a benchmark. More interesting are landscapes that emerge as the outcome of a non-random process - either a systematic/deterministic or stochastic process. Here we will see more ways to introduce a systematic element into a null landscape to simulate spatial processes. Let’s begin with the point pattern, using the same landscape that we used above. We will first copy the coordinates of the landscape to a new dataframe, that we will call pattern1: # Copy the coordinates to a new object, called `pattern1` pattern1 &lt;- coords Next, we will use the function mutate from the dplyr package that is part of the tidyverse. This function adds a column to a data frame that could be calculated using a formula. For instance, we will now make the probability prob of the random binomial number generator a function of the coordinates: # Remember, mutate adds a new column to a data frame. In this example, mutate creates a new column, `events` using random binomial values; however, notice that the `prob` is not 0.5! Instead, it depends on `x` the position of the event on the x-axis pattern1 &lt;- mutate(pattern1, events = rbinom(n = nrow(pattern1), size = 1, prob = (x))) Plot this pattern: ggplot() + geom_point(data = subset(pattern1, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() Since the probability of a “success” in the binomial experiment is proportional to the value of x (the coordinate of the event), now the events are clustered to the right of the plot. The underlying process in this case can be described in simple terms as “the probability of an event increases in the east direction”. In a real process, this could be possibly as a result of wind conditions, soil fertility, or other environmental factors that follow a trend. Let’s see what happens when we make this probability a function of the y coordinate: # Overwrite the `events`, now the probability of success in the random binomial number generator is a function of `y`, the position of the event on the y-axis pattern1 &lt;- mutate(pattern1, events = rbinom(n = nrow(pattern1), size = 1, prob = (y))) # Plot the new events ggplot() + geom_point(data = subset(pattern1, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() Since the probability of a “success” in the binomial experiment is proportional to the value of y (the coordinate of the event), now the events are clustered to the top. The probability could be the interaction of the two coordinates: # Now the probability is the product of `x` and `y` pattern1 &lt;- mutate(pattern1, events = rbinom(n = nrow(pattern1), size = 1, prob = (x * y))) # Plot ggplot() + geom_point(data = subset(pattern1, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() Which of course means that the events cluster on the top-right corner. A somewhat more sophisticated example could make the probability a function of distance from the center of the region: # Copy the coordinates to the object `pattern1` pattern1 &lt;- coords # In this case, `mutate()` creates a new variable, `distance`, which is the straight line distance from the center of the region (at coordinates x = 0.5 and y = 0.5). Now the probability of success in the random binomial number generator depends on this `distance` pattern1 &lt;- mutate(pattern1, distance = sqrt((0.5 - x)^2 + (0.5 - y)^2), events = rbinom(n = nrow(pattern1), size = 1, prob = 1 - exp(-0.5 * distance))) Don’t worry too much about the formula that I selected to generate this process; we will see different tools to describe a spatial process. In this particular example, I selected a function that makes the probability increase with distance from the center of the region. Plot this pattern: ggplot() + geom_point(data = subset(pattern1, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() As you would expect, there are few events near the center, and the number of events tends to increase away from the center. To conclude this practice, let’s revisit the example of the people standing in formation. Now, taller people are asked to stand towards the back of the formation (assuming that the back is in the positive direction of the y-axis). As a result of this instruction, now the sorting is not random, since taller people tend to stand towards the back. However, people are not able to assess the height of each other exactly, so there will be some random variation in the distribution of heights. We can simulate this by making the height a function of position. First, we copy the coordinates to a new dataframe for our trend experiment: trend1 &lt;- coords Again we use mutate to add a column to a data frame that could be calculated using a formula. For instance, we will now make the probability prob of the random binomial number generator a function of the coordinates: trend1 &lt;- mutate(trend1, heights = 160 + 20 * y + rnorm(n = nrow(pattern1), mean = 0, sd = 7)) If people have a preference for standing next to people about their same height, and shorter people have a preference for standing near the front, this is a possible map of heights in the formation: ggplot() + geom_point(data = trend1, aes(x = x, y = y, color = heights), shape = 15) + scale_color_distiller(palette = &quot;Spectral&quot;) + coord_fixed() As expected, shorter people are towards the “front” (bottom of the plot) and taller people towards the back. It is not a uniform process, since there is still some randomness, but a trend can be clearly appreciated. 6.8 Processes and Patterns O’Sullivan and Unwin (2010) make an important distinction between processes and patterns. A process is like a recipe, a sequence of events or steps, that leads to an outcome, that is, a pattern. You can think of the simulation procedures above as having two components: the process is the formula, function, or algorithm used to simulate a pattern. For instance, a random process could be based on the binomial distribution, whereas a stochastic process would have in addition to a random component some deterministic elements. The pattern is the outcome of the process. In the case of spatial processes, the outcome is typically a statistical map. The procedures in the preceding sections illustrate just a few different ways to simulate spatial processes with the aim of generating statistical maps that display spatial patterns. There are in fact many more ways to simulate spatial processes, and articles (e.g., Geyer and Møller 1994) - and even books (e.g., Moller and Waagepetersen 2003) - have been written on this topic! Simulation is a very valuable tool in spatial statistics, as we shall see in later chapters. It is important to note, however, that in the vast majority of cases we do not actually know the process; that is precisely what we wish to infer. Understanding process generation in a statistical sense, as well as null landscapes, is a useful tool that can help us to infer processes in applications with empirical (as opposed to simulated) data. In this sense, spatial statistics is often a tool used to make decisions about spatial patterns: are they random? And, if they are not random, can we infer the underlying process? References "],
["activity-3-maps-as-processes.html", "Chapter 7 Activity 3: Maps as Processes 7.1 Practice Questions 7.2 Learning Objectives 7.3 Suggested Reading 7.4 Preliminaries 7.5 Activity", " Chapter 7 Activity 3: Maps as Processes Remember, you can download the source file for this activity from here. 7.1 Practice Questions Answer the following questions: What is a Geographic Information System? What distinguishes a statistical map from other types of mapping techniques? What is a null landscape? 7.2 Learning Objectives In this activity, you will: Simulate landscapes using various types of processes. Discuss the difference between random and non-random landscapes. Think about ways to decide whether a landscape is random. 7.3 Suggested Reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 4. John Wiley &amp; Sons: New Jersey. 7.4 Preliminaries For this activity you will need the following: An R markdown notebook version of this document (the source file). A package called geog4ga3. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) In the practice that preceded this activity, you learned how to simulate null landscapes and spatial processes. 7.5 Activity *1. Simulate and plot a landscape using a random, stochastic, or deterministic process. It is your choice whether to simulate a point pattern or a continuous variable. Identify the key parameters that make a landscape more or less random. Repeat several times changing those parameters. Recreate any one of the maps you created and share the map with a fellow student. Ask them to guess whether the map is random or non-random. Repeat step 2 several times (depending on time, between two and four times). Propose one or more ways to decide whether a landscape is random, and explain your reasoning. The approach does not need to be the same for point patterns and continuous variables! "],
["point-pattern-analysis-i.html", "Chapter 8 Point Pattern Analysis I 8.1 Learning Objectives 8.2 Suggested Readings 8.3 Preliminaries 8.4 Point Patterns 8.5 Processes and Point Patterns 8.6 Intensity and Density 8.7 Quadrats and Density Maps 8.8 Defining the Region for Analysis", " Chapter 8 Point Pattern Analysis I NOTE: You can download the source files for this book from here. The source files are in the format of R Notebooks. Notebooks are pretty neat, because the allow you execute code within the notebook, so that you can work interactively with the notes. In last practice your learning objectives were: How to generate random numbers with different properties. About Null Landscapes. How to create new columns in a dataframe using a formula. How to simulate a spatial process. Please review the previous practices if you need a refresher on these concepts. If you wish to work interactively with this chapter you will need the following: An R markdown notebook version of this document (the source file). A package called geog4ga3. 8.1 Learning Objectives In this practice, you will learn: A formal definition of point pattern. Processes and point patterns. The concepts of intensity and density. The concept of quadrats and how to create density maps. More ways to control the look of your plots, in particular faceting and adding lines. 8.2 Suggested Readings Bailey TC and Gatrell AC (1995) Interactive Spatial Data Analysis, Chapter 3. Longman: Essex. Baddeley A, Rubak E, Turner R (2016) Spatial Point Pattern: Methodology and Applications with R, Chapter 1, 1.1 - 1.2. CRC: Boca Raton. Bivand RS, Pebesma E, Gomez-Rubio V (2008) Applied Spatial Data Analysis with R, Chapter 7. Springer: New York. Brunsdon C and Comber L (2015) An Introduction to R for Spatial Analysis and Mapping, Chapter 6, 6.1 - 6.6. Sage: Los Angeles. O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 8.3 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(spatstat) library(geog4ga3) ## Warning: replacing previous import &#39;plotly::filter&#39; by &#39;stats::filter&#39; when ## loading &#39;geog4ga3&#39; ## Warning: replacing previous import &#39;dplyr::lag&#39; by &#39;stats::lag&#39; when loading ## &#39;geog4ga3&#39; Load the data that you will use for this practice: data(&quot;PointPatterns&quot;) Quickly check the contents of this dataframe: summary(PointPatterns) ## x y Pattern ## Min. :0.0169 Min. :0.005306 Pattern 1:60 ## 1st Qu.:0.2731 1st Qu.:0.289020 Pattern 2:60 ## Median :0.4854 Median :0.550000 Pattern 3:60 ## Mean :0.5074 Mean :0.538733 Pattern 4:60 ## 3rd Qu.:0.7616 3rd Qu.:0.797850 ## Max. :0.9990 Max. :0.999808 The dataframe contains the x and y coordinates of four different patterns of points, each with \\(n=60\\) events. 8.4 Point Patterns Previously you created different types of maps and learned about different kinds of processes (i.e., random, stochastic, deterministic). A map that you have seen in several occasions is one where the coordinates of an event of interest are available. The simplest kind of data of this type is called a point pattern. This occurs when only the coordinates are available. A point pattern is given by a set of events of interest that are observed in a region \\(R\\). A region has an infinite number of points, essentially coordinates \\((x_i, y_i)\\) on the plane. The number of points is infinite, because there is a point defined by, say, coordinates (1,1), and also a point for coordinates (1.1,1), and for coordinates for (1.01,1), and so on. Any location that can be described by a set of coordinates contained in the region is a point. Not all points are events, however. An event is defined as a point where something of interest happened. This could be the location where a tree exists, or a crime happened, the epicenter of an earthquake, a case of a disease was reported, and so on. There might be one such occurrence, or more. Each event is be denoted by: \\[ \\textbf{s}_i \\] with coordinates: \\[ (x_i,y_i). \\] Sometimes other attributes of the events have been measured as well. For example, the event could be an address where cholera was reported (as in John Snow’s famous map). In addition to the address (which can be converted into the coordinates of the event), the number of cases could be recorded. Other examples could be the height and diameter of trees, the magnitude of an earthquake, etc. It is important, for reasons that will be discussed later, that the point pattern is a complete enumeration. What this means is that every event that happened has been recorded! Interpretation of most analyses becomes dubious if the events are only sampled, that is, if only a few of them have been observed and recorded. 8.5 Processes and Point Patterns Point patterns are interesting in many applications. In these applications, a key question of interest is whether the pattern is random. Imagine a point pattern that records crimes in a region. The pattern might be random, in which case there is no way to anticipate where the next occurrence of criminal activity will be. Non-random patterns, on the other hand, are likely the outcome of some meaningful process. For instance, crimes might cluster as a consequence of some common environmental variable (e.g., concentration of wealth). On the contrary, they might repeal each other (e.g., the location of a crime draws attention of law enforcement, and therefore the next occurrence of a crime tends to happen away from it). Deciding whether the pattern is random or not is the initial step towards developing hypotheses about the underlying process. Consider for example the following patterns. To create the following figure, you can use faceting by means of ggplot2::facet_wrap(): # This uses function &quot;ggplot&quot; to plot data &quot;PointPatterns&quot; loaded into the data frame earlier, by means of X and Y coordinates # the function `facet_wrap()` is used to create multiple plots according to one (or more) variables in the dataset. Here, it is used to create individual plots for each of the four patterns in the dataframe, but put them all in a single figure ggplot() + geom_point(data = PointPatterns, aes(x = x, y = y)) + facet_wrap(~ Pattern) + coord_fixed() As you can see, faceting is a convenient way to simultaneously plot different parts of a dataframe (in the present case, the different Patterns). In the preceding activity, you were asked to generate ideas regarding possible ways of deciding whether a map of events (i.e., a point pattern) is random. In this chaper we will formalize a specific way to do so, by considering the intensity of the process. 8.6 Intensity and Density The intensity of a spatial point process is the expected number of events per unit area. This is conventionally denoted by the greek letter \\(\\lambda\\). In most cases the process is not know, so its intensity cannot be directly measured. In its place, the density of the point pattern is taken as the empirical estimate of the intensity of the underlying process. The density of the point pattern is calculated very simply as the number of events divided by the area of the region, that is: \\[ \\hat{\\lambda} = \\frac{(S \\in R)}{a} = \\frac{n}{a}. \\] Notice the use of the “hat” symbol on top of the Greek lambda. This symbol is called “caret”. The hat notation is used to indicate an estimated value of an unobserved parameter of a process as opposed to the true (but usually unknown) value. In the present case this is the intensity of the spatial point process. Consider one of the point patterns in your sample dataset, say “Pattern 1”. If we filter for “Pattern 1” we can then summarize it: filter(PointPatterns, Pattern == &quot;Pattern 1&quot;) %&gt;% summary() ## x y Pattern ## Min. :0.0285 Min. :0.005306 Pattern 1:60 ## 1st Qu.:0.3344 1st Qu.:0.236509 Pattern 2: 0 ## Median :0.5247 Median :0.500262 Pattern 3: 0 ## Mean :0.5531 Mean :0.500248 Pattern 4: 0 ## 3rd Qu.:0.8417 3rd Qu.:0.761218 ## Max. :0.9888 Max. :0.999808 We see that there are \\(n = 60\\) points in this dataset. Since the region is the unit square (check how the values of the coordinates range from approximately zero to approximately 1), the area of the region is 1. This means that for “Pattern 1”: \\[ \\hat{\\lambda} = \\frac{60}{1} = 60 \\] This is the overall density of the point pattern. 8.7 Quadrats and Density Maps The overall density of a point process (calculated above) can be mapped by means of the geom_bin2d function of the ggplot2 package. This function divides two dimensional space into bins and reports the number of events or the density of the events in the bins. Let’s give this a try: # `geom_bin2d()` creates a tessellation and counts the number of events in each of the &quot;tiles&quot; in the tesselation. It then assigns colors based on the count of events. The `binwidth` determines the size of the squaes in the tessellation, in this case squares of size 1 by 1...which corresponds to the size of the region! ggplot() + geom_bin2d(data = subset(PointPatterns, Pattern == &quot;Pattern 1&quot;), aes(x = x, y = y), binwidth = c(1, 1)) + coord_fixed() Let’s see step-by-step how this plot is made. ggplot() creates a plot object. geom_bin2d is called to plot a map of counts of events in the space defined by the bins. The dataframe used for plotting the bins is PointPatterns, subset so that only the points in “Pattern 1” are used. The coordinates x and y are used to plot (in aes(), we indicate that x in the dataframe corresponds to the x axis in the plot, and y in the dataframe corresponds to y axis in the plot) The size of the bin is defined as 1-by-1 (binwidth = c(1, 1)) coord_fixed is applied to ensure that the aspect ratio of the plot is one (one unit of x is the same length as one unit of y in the plot). The map of the overall density of the process above is not terribly interesting. It only reports what we already knew, that globally the density of the point pattern is 60. It would be more interesting to see how the density varies across the region. We do this by means of the concept of quadrats. Imagine that instead of calculating the overall (or global) intensity of the point pattern, we subdivided the region into a set of smaller subregions. For instance, we could draw horizontal and vertical lines to create smaller squares: # `geom_vline()` draws vertical lines that cross the x-axis at the points indicated; `geom_hline()` draws horizontal lines that cross the y-axis at the points indicated ggplot() + geom_vline(xintercept = seq(from = 0, to = 1, by = 0.25)) + geom_hline(yintercept = seq(from = 0, to = 1, by = 0.25)) + geom_point(data = filter(PointPatterns, Pattern == &quot;Pattern 1&quot;), aes(x = x, y = y)) + coord_fixed() Notice how we used to create the vertical lines (geom_vline) and horizontal lines (geom_hline), from 0 to 1 every 0.25 units of distance respectively. This creates a tessellation that divides the original region into 16 smaller squares, or subregions. Each of the smaller squares used to subdivide the region is called a quadrat. To make things more interesting, instead of calculating the overall density, we can calculate the density for each quadrat. Now the size of the quadrats will be \\(0.25\\times 0.25\\). Here we visualize the density of the quadrats: ggplot() + geom_bin2d(data = filter(PointPatterns, Pattern == &quot;Pattern 1&quot;), aes(x = x, y = y), binwidth = c(0.25, 0.25)) + geom_point(data = filter(PointPatterns, Pattern == &quot;Pattern 1&quot;), aes(x = x, y = y)) + scale_fill_distiller(palette = &quot;RdBu&quot;) + coord_fixed() You can, of course, change the size of the quadrats. Let’s take a look at the four point patterns (by means of faceting), after creating a variable to easily control the size of the quadrat. Let’s call this variable q_size: # `q_size` controls the size of the quadrats; experiment changing this parameter q_size &lt;- 0.5 ggplot() + geom_bin2d(data = PointPatterns, aes(x = x, y = y), binwidth = c(q_size, q_size)) + geom_point(data = PointPatterns, aes(x = x, y = y)) + facet_wrap(~ Pattern) + scale_fill_distiller(palette = &quot;RdBu&quot;) + coord_fixed() Notice the differences in the density maps? Try changing the size of the quadrat to 1. What happens, and why? Next, try a smaller quadrat size, say 0.25. What happens, and why? Try even smaller quadrat sizes, but greater than zero. What happens now? The package spatstat (Baddeley, Rubak, and Turner 2016) includes numerous functions for the analysis of point patterns. A relevant function for us at this stage, is quadratcount(), which returns the number of events per quadrat. To use this function, we need to convert the point patterns to a type of object used by spatstat denominated ppp (for plannar point pattern). This is simple, thanks to a utility function in spatstat called as.ppp. This function takes as arguments (inputs) a set of coordinates, and data to define a window. To benefit from the functionality of spatstat we will convert our data frame with spatial patterns into ppp objects. First, define the window by means of the owin function, and using the 0 to 1 interval for our region: # `owin()` creates a window for `ppp` objects, which becomes the _region_ under study. Here, we define a window that is the unit square and we will discuss the importance of an appropriate definition of the region later. The windows in `spatstat` need not be squares or rectangles, and can actually be irregular shapes Wnd &lt;- owin(c(0,1), c(0,1)) Now, a ppp object can be created: # `as.ppp()` will take an object and convert it to a `ppp` object. Here, it does a fairly good job of guessing the contents of the data frame! The second argument to create the `ppp` object is a window, that is, an `owin` object ppp1 &lt;- as.ppp(PointPatterns, Wnd) If you examine these new ppp objects, you will see that they pack the same basic information (i.e., the coordinates), but also the range of the region and so on: summary(ppp1) ## Marked planar point pattern: 240 points ## Average intensity 240 points per square unit ## ## Coordinates are given to 8 decimal places ## ## Multitype: ## frequency proportion intensity ## Pattern 1 60 0.25 60 ## Pattern 2 60 0.25 60 ## Pattern 3 60 0.25 60 ## Pattern 4 60 0.25 60 ## ## Window: rectangle = [0, 1] x [0, 1] units ## Window area = 1 square unit As you can see, the ppp object includes the four patterns, calculates the frequency of each (the number of events), and their respective overall intensities. Objects of the class ppp can be plotted using base R plotting functions: plot(ppp1) To plot each pattern separately we can split the different patterns using the function split.ppp(). Notice how $ works for indexing the patterns here, just as it does for indexing columns in a data frame: plot(split.ppp(ppp1)$`Pattern 1`) Once the patterns are in ppp form, quadratcount can be used to compute the counts of events. To calculate the count separately for each pattern, you need to use again split.ppp() (if you don’t index a pattern, it will apply the function to all of them). The other two arguments are the number of quadrats in the horizontal (nx) and the vertical (ny) directions: quadratcount(split(ppp1), nx = 4, ny = 4) ## List of spatial objects ## ## Pattern 1: ## x ## y [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1] ## [0.75,1] 3 5 1 6 ## [0.5,0.75) 2 3 4 6 ## [0.25,0.5) 5 4 2 3 ## [0,0.25) 2 4 4 6 ## ## Pattern 2: ## x ## y [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1] ## [0.75,1] 14 2 2 6 ## [0.5,0.75) 0 0 4 6 ## [0.25,0.5) 6 3 1 2 ## [0,0.25) 4 6 2 2 ## ## Pattern 3: ## x ## y [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1] ## [0.75,1] 2 11 5 7 ## [0.5,0.75) 1 1 6 4 ## [0.25,0.5) 1 10 3 2 ## [0,0.25) 2 1 2 2 ## ## Pattern 4: ## x ## y [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1] ## [0.75,1] 4 5 6 3 ## [0.5,0.75) 3 3 4 2 ## [0.25,0.5) 3 3 4 2 ## [0,0.25) 5 4 6 3 Compare the counts of the quadrats for each pattern. They should replicate what you observed in the density plots before. 8.8 Defining the Region for Analysis It is important when conducting the type of analysis described above (and more generally any analysis with point patterns), to define a region for analysis that is consistent with the pattern of interest. Consider for instance what would happen if the region was defined, instead of in the unit square, as a bigger region. Create a second window: # This new window measure 3 units in the x-axis, and also 3 units in the y-axis (from -1 to 2) Wnd2 &lt;- owin(c(-1,2), c(-1,2)) Create a second ppp object using this new window: # Here, we use the same events as before, but place them in the larger window we just created ppp2 &lt;- as.ppp(PointPatterns, Wnd2) Repeat the plot but using the new ppp object: plot(split.ppp(ppp2)$`Pattern 1`) Repeat but now using an even bigger region. Create a third window: Wnd3 &lt;- owin(c(-2, 3), c(-2, 3)) And also a third ppp object using the third window: ppp3 &lt;- as.ppp(PointPatterns, Wnd3) Now the plot looks like this: plot(split.ppp(ppp3)$`Pattern 1`) Which of the three regions that you saw above is more appropriate? What do you think is the effect of selecting an inappropriate region for the analysis? This concludes this chapter. The next activity will illustrate how quadrats are a useful tool to explore the question whether a map is random. References "],
["activity-4-point-pattern-analysis-i.html", "Chapter 9 Activity 4: Point Pattern Analysis I 9.1 Practice questions 9.2 Learning objectives 9.3 Suggested reading 9.4 Preliminaries 9.5 Activity", " Chapter 9 Activity 4: Point Pattern Analysis I Remember, you can download the source file for this activity from here. 9.1 Practice questions Answer the following questions: What is a random process? What is a deterministic process? What is a stochastic process? What is a pattern? What is the usefulness of a null landscape? 9.2 Learning objectives In this activity, you will: Use the concept of quadrats to analyze a real dataset. Learn about a quadrat-based test for randomness in point patterns. Learn how to use the p-value of a statistical test to make a decision. Think about the distribution of events in a null landscape. Think about ways to decide whether a landscape is random. 9.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 9.4 Preliminaries For this activity you will need the following: An R markdown notebook version of this document (the source file). A package called geog4ga3. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(spatstat) library(maptools) # Needed to convert a `Spatial Polygons` object into an `owin` object library(sf) library(geog4ga3) ## Warning: replacing previous import &#39;plotly::filter&#39; by &#39;stats::filter&#39; when ## loading &#39;geog4ga3&#39; ## Warning: replacing previous import &#39;dplyr::lag&#39; by &#39;stats::lag&#39; when loading ## &#39;geog4ga3&#39; In the practice that preceded this activity, you learned about the concepts of intensity and density, about quadrats, and also how to create density maps. Begin by loading the data that you will use in this activity: data(&quot;Fast_Food&quot;) data(&quot;Gas_Stands&quot;) data(&quot;Paez_Mart&quot;) Next the geospatial files need to be read. For this example, the city boundary of Toronto is provided in two different formats, as a dataframe (which can be used to plot using ggplot2) and as a SpatialPolygons object, a format widely used in R for spatial analysis. The : data(&quot;Toronto&quot;) If you inspect your workspace, you will see that the following dataframes are there: Fast_Food Gas_Stands Paez_Mart These are locations of a selection of fast food restaurants, and also of gas stands in Toronto (data are from 2008). Paez Mart on the other hand is a project to cover Toronto with convenience stores. The points are the planned locations of the stores. Also, there should be an object of class sf. This dataframe contains the city boundary of Toronto: class(Toronto) ## [1] &quot;sf&quot; &quot;data.frame&quot; Try plotting the following: ggplot() + geom_sf(data = Toronto, color = &quot;black&quot;, fill = NA, alpha = 1, size = .3) + geom_sf(data = Paez_Mart) + coord_sf() As discussed in the preceding chapter, the package spatstat offers a very rich collection of tools to do point pattern analysis. To convert the three sets of events (i.e., the fast food establishments, gas stands, and Paez Mart) into ppp objects we first must define a region or window. To do this we take the sf and convert to an owin (a window object) for use with the package spatstat (this is done via SpatialPolygons, hence as(x, \"Spatial\"): # `as.owin()` will take a &quot;foreign&quot; object (foreign to `spatstat`) and convert it into an `owin` object. Here, there are two steps involved: first, we take the `sf` object with the boundaries of Toronto and convert it into a &quot;Spatial&quot; object, and then the &quot;Spatial&quot; object is passed on to `as.owin()` Toronto.owin &lt;- as(Toronto, &quot;Spatial&quot;) %&gt;% as.owin() # Requires `maptools` package And, then convert the dataframes to ppp objects (this necessitates that we extract the coordinates of the events by means of st_coordinates): Fast_Food.ppp &lt;- as.ppp(st_coordinates(Fast_Food), W = Toronto.owin) Gas_Stands.ppp &lt;- as.ppp(st_coordinates(Gas_Stands), W = Toronto.owin) Paez_Mart.ppp &lt;- as.ppp(st_coordinates(Paez_Mart), W = Toronto.owin) These objects can now be used with the functions of the spatstat package. For instance, you can calculate the counts of events by quadrat by means of quadrat.count. The input must be a ppp object, and the number of quadrats on the horizontal (nx) and vertical (ny) direction (notice how I use the function table to present the frequency of quadrats with number of events): q_count &lt;- quadratcount(Fast_Food.ppp, nx = 3, ny = 3) table(q_count) ## q_count ## 0 6 44 48 60 64 85 144 163 ## 1 1 1 1 1 1 1 1 1 As you see from the table, there is one quadrat with zero events, one quadrat with six events, one quadrat with forty-four events, and so on. You can also plot the results of the quadratcount() function! plot(q_count) A useful function in the spatstat package is quadrat.test. This function implements a statistical test that compares the empirical distribution of events by quadrats to the distribution of events as expected under the hypothesis that the underlying process is random. This is implemented as follows: q_test &lt;- quadrat.test(Fast_Food.ppp, nx = 3, ny = 3) ## Warning: Some expected counts are small; chi^2 approximation may be inaccurate q_test ## ## Chi-squared test of CSR using quadrat counts ## ## data: Fast_Food.ppp ## X2 = 213.74, df = 8, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## ## Quadrats: 9 tiles (irregular windows) The quadrat test reports a \\(p\\)-value which can be used to make a decision. The \\(p\\)-value is the probability that you will be mistaken if you reject the null hypothesis. To make a decision, you need to know what is the null hypothesis, and your own tolerance for making a mistake. In the case above, the \\(p\\)-value is very, very small (2.2e-16 = 0.00000000000000022). Since the null hypothesis is spatial randomness, you can reject this hypothesis and the probability that this decision is mistaken is vanishingly small. Try plotting the results of quadrat.test: plot(q_test) Now that you have seen how to do some analysis using quadrats, you are ready for the next activity. 9.5 Activity *1. Use Fast_Food, Gas_Stands, Paez_Mart, and Toronto to create density maps for the three point patterns. Select a quadrat size that you think is appropriate. *2. Use Fast_Food.ppp, Gas_Stands, and Paez_Mart, and the function quadratcount to calculate the number of events per quadrat. Remember that you need to select the number of quadrats in the horizontal and vertical directions! *3 Use the function table() to examine the frequency of events per quadrat for each of the point patterns. Show your density maps to a fellow student. Did they select the same quadrat size? If not, what was their rationale for their size? Again, use the function table() to examine the frequency of events per quadrat for each of the point patterns. What are the differences among these point patterns? What would you expect the frequency of events per quadrat to be in a null landscape? Use Fast_Food.ppp, Gas_Stands, and Paez_Mart, and the function quadrat.test to calculate the test of spatial independence for these point patterns. What is your decision in each case? Explain. "],
["point-pattern-analysis-ii.html", "Chapter 10 Point Pattern Analysis II 10.1 Learning Objectives 10.2 Suggested Readings 10.3 Preliminaries 10.4 A Quadrat-based Test for Spatial Independence 10.5 Limitations of Quadrat Analysis: Size and Number of Quadrats 10.6 Limitations of Quadrat Analysis: Relative Position of Events 10.7 Kernel Density", " Chapter 10 Point Pattern Analysis II NOTE: You can download the source files for this book from here. The source files are in the format of R Notebooks. Notebooks are pretty neat, because the allow you execute code within the notebook, so that you can work interactively with the notes. In the last practice/session your learning objectives included: A formal definition of point pattern. Processes and point patterns. The concepts of intensity and density. The concept of quadrats and how to create density maps. More ways to control the look of your plots, in particular faceting and adding lines. Please review the previous practices if you need a refresher on these concepts. If you wish to work interactively with this chapter you will need the following: An R markdown notebook version of this document (the source file). A package called geog4ga3. 10.1 Learning Objectives In this practice, you will learn: The intuition behind the quadrat-based test of independence. About the limitations of quadrat-based analysis. The concept of kernel density. More ways to manipulate objects to do point pattern analysis using spatstat. 10.2 Suggested Readings Bailey TC and Gatrell AC (1995) Interactive Spatial Data Analysis, Chapter 3. Longman: Essex. Baddeley A, Rubak E, Turner R (2016) Spatial Point Pattern: Methodology and Applications with R, Chapter 6. CRC: Boca Raton. Bivand RS, Pebesma E, Gomez-Rubio V (2008) Applied Spatial Data Analysis with R, Chapter 7. Springer: New York. Brunsdon C and Comber L (2015) An Introduction to R for Spatial Analysis and Mapping, Chapter 6, 6.1 - 6.6. Sage: Los Angeles. O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 10.3 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(spatstat) library(geog4ga3) ## Warning: replacing previous import &#39;plotly::filter&#39; by &#39;stats::filter&#39; when ## loading &#39;geog4ga3&#39; ## Warning: replacing previous import &#39;dplyr::lag&#39; by &#39;stats::lag&#39; when loading ## &#39;geog4ga3&#39; Load the datasets that you will use for this practice: data(&quot;PointPatterns&quot;) data(&quot;pp0_df&quot;) PointPatterns is a data frame with four sets of spatial events, labeled as “Pattern 1”, “Pattern 2”, “Pattern 3”, and “Pattern 4”. Each set has \\(n=60\\) events. You can check the class of this object by means of the function class class(). class(PointPatterns) ## [1] &quot;data.frame&quot; The second data frame (i.e., pp0_df) includes the coordinates x and y of two sets of spatial events, labeled as “Pattern 1” and “Pattern 2”. The summary for PointPatterns shows that these point patterns are located in a square-unit window (check the max and min values of x and y): summary(PointPatterns) ## x y Pattern ## Min. :0.0169 Min. :0.005306 Pattern 1:60 ## 1st Qu.:0.2731 1st Qu.:0.289020 Pattern 2:60 ## Median :0.4854 Median :0.550000 Pattern 3:60 ## Mean :0.5074 Mean :0.538733 Pattern 4:60 ## 3rd Qu.:0.7616 3rd Qu.:0.797850 ## Max. :0.9990 Max. :0.999808 The same is true for pp0_df: summary(pp0_df) ## x y marks ## Min. :0.0456 Min. :0.03409 Pattern 1:36 ## 1st Qu.:0.2251 1st Qu.:0.22963 Pattern 2:36 ## Median :0.4282 Median :0.43363 ## Mean :0.4916 Mean :0.47952 ## 3rd Qu.:0.7812 3rd Qu.:0.77562 ## Max. :0.9564 Max. :0.94492 As seen in the previous practice and activity, the package spatstat employs a type of object called ppp (for planar point pattern). Fortunately, it is relatively simple to convert a data frame into a ppp object by means of as.ppp(). This function requires that you define a window for the point pattern, something we can do by means of the owin function: # &quot;W&quot; will appear in your environment as a defined window with boundaries of (1,1) W &lt;- owin(xrange = c(0, 1), yrange = c(0, 1)) Then the data frames are converted using the as.ppp function: # Converts the data frame to planar point pattern using the defined window &quot;W&quot; pp0.ppp &lt;- as.ppp(pp0_df, W = W) PointPatterns.ppp &lt;- as.ppp(PointPatterns, W = W) You can verify that the new objects are indeed of ppp-class: #&quot;class&quot; is an excellent tool to use when verifying the execution of a previous line of code class(pp0.ppp) ## [1] &quot;ppp&quot; class(PointPatterns.ppp) ## [1] &quot;ppp&quot; 10.4 A Quadrat-based Test for Spatial Independence In the preceding activity, you used a quadrat-based spatial independence test to help you decide whether a pattern was random (the function was quadrat.test). We will now review the intuition of the test. Let’s begin by plotting the patterns. You can use split to do plots for each pattern separately, instead of putting all of them in a single plot (this approach is not as refined as ggplot2, where we have greater control of the aspect of the plots; on the other hand, it is quick): #The split functions separates without defining a window. This is a quicker option to get relative results plot(split(PointPatterns.ppp)) Recall that you can also plot individual patterns by using $ followed by the factor that identifies the desired pattern (this is a way of indexing different patterns in ppp-class objects): # Using &quot;$&quot; acts as a call sign to retrive information from a data frame. In this case, you are calling &quot;Pattern 4&quot; from &quot;PointPatterns.ppp&quot; plot(split(PointPatterns.ppp)$&quot;Pattern 4&quot;) Now calculate the quadrat-based test of independence: # `quadrat.test()` generates a quadrat-based test of independence, in this case, for &quot;Pattern 2&quot; called from &quot;PointPatterns.ppp&quot;, using 3 quadrats in the direction of the x-axis and 3 quadrats in the direction of the y-axis q_test &lt;- quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 3, ny = 3) q_test ## ## Chi-squared test of CSR using quadrat counts ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 48, df = 8, p-value = 1.976e-07 ## alternative hypothesis: two.sided ## ## Quadrats: 3 by 3 grid of tiles Plot the results of the quadrat test: plot(q_test) As seen in the preceding chapter, the expected distribution of events on quadrats under the null landscape tends to be quite even. This is because each quadrat has equal probability of having the same number of events (depending on size, when the quadrats are not all the same size the number will be proportional to the size of the quadrat). If you check the plot of the quadrat test above, you will notice that the first number (top left corner) is the number of events in the quadrat. The second number (top right corner) is the expected number of events for a null landscape. The third number is a residual, based on the difference between the observed and expected number of events. More specifically, the residual is a Pearson residual, defined as follows: \\[ r_i=\\frac{O_i - E_i}{\\sqrt{E_i}}, \\] where \\(O_i\\) is the number of observed events in quadrat \\(i\\) and \\(E_i\\) is the number of expected events in quadrat \\(i\\). When the number of observed events is similar to the number of expected events, \\(r_i\\) will tend to be a small value. As their difference grows, the residual will also grow. The independence test is calculated from the residuals as: \\[ X^2=\\sum_{i=1}^{Q}r_i^2, \\] where \\(Q\\) is the number of quadrats. In other words, the test is based on the squared sum of the Pearson residuals. The smaller this number is, the more likely that the observed pattern of events is not different from a null landscape (i.e., a random process), and the larger it is, the more likely that it is different from a null landscape. This is reflected by the \\(p\\)-value of the test (technically, the \\(p\\)-value is obtained by comparing the test to the \\(\\chi^2\\) distribution, pronounced “kay-square”). Consider for instance the first pattern in the examples: plot(quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 1&quot;, nx = 3, ny = 3)) You can see that the Pearson residual of the top left quadrat is indeed -0.6567673, the next to its right is -0.2704336, and so on. The value of the test statistic should be then: # The &quot;Paste&quot; function joins together several arguments as characters. Here, this is a string of values for &quot;X2&quot;, where X2&quot; is the squared sum of the residuals paste(&quot;X2 = &quot;, (-0.65)^2 + (-0.26)^2 + (0.52)^2 + (-0.26)^2 + (0.9)^2 + (0.52)^2 + (-1)^2 + (0.13)^2 + (0.13)^2) ## [1] &quot;X2 = 2.9423&quot; Which you can confirm by examining the results of the test (the small difference is due to rounding errors): quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 1&quot;, nx = 3, ny = 3) ## ## Chi-squared test of CSR using quadrat counts ## ## data: split(PointPatterns.ppp)$&quot;Pattern 1&quot; ## X2 = 3, df = 8, p-value = 0.1313 ## alternative hypothesis: two.sided ## ## Quadrats: 3 by 3 grid of tiles Explore the remaining patterns. You will notice that the residuals and test statistic tend to grow as more events are concentrated in space. In this way, the test is a test of density of the quadrats: is their density similar to what would be expected from a null landscape? 10.5 Limitations of Quadrat Analysis: Size and Number of Quadrats As hinted by the previous activity, one issue with quadrat analysis is the selection of the size for the quadrats. Changing the size of the quadrats has an impact on the counts, and in turn on the aspect of density plots and even the results of the test of independence. For example, the results of the test for “Pattern 2” in the dataset change when the number of quadrats is modified. For instance, with a small number of quadrats: quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 2, ny = 1) ## ## Chi-squared test of CSR using quadrat counts ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 1.6667, df = 1, p-value = 0.3934 ## alternative hypothesis: two.sided ## ## Quadrats: 2 by 1 grid of tiles Compare to four quadrats: quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 2, ny = 2) ## ## Chi-squared test of CSR using quadrat counts ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 6, df = 3, p-value = 0.2232 ## alternative hypothesis: two.sided ## ## Quadrats: 2 by 2 grid of tiles And: quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 3, ny = 2) ## ## Chi-squared test of CSR using quadrat counts ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 23.2, df = 5, p-value = 0.0006182 ## alternative hypothesis: two.sided ## ## Quadrats: 3 by 2 grid of tiles Why is the statistic generally smaller when there are fewer quadrats? A different issue emerges when the number of quadrats is large: quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 4, ny = 4) ## Warning: Some expected counts are small; chi^2 approximation may be inaccurate ## ## Chi-squared test of CSR using quadrat counts ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 47.2, df = 15, p-value = 6.84e-05 ## alternative hypothesis: two.sided ## ## Quadrats: 4 by 4 grid of tiles A warning now tells you that some expected counts are small: space has been divided so minutely, that the expected number of events per quadrat has become too thin; as a consequence, the approximation to the probability distribution may be inaccurate. While there are no hard rules to select the size/number of quadrats, the following rules of thumb are sometimes suggested: Each quadrat should have a minimum of two events. The number of quadrats is selected based on the area (A) of the region, and the number of events (n): \\[ Q=\\frac{2A}{N} \\] Caution should be exercised when interpreting the results of the analysis based on quadrats, due to the issue of size/number of quadrats. 10.6 Limitations of Quadrat Analysis: Relative Position of Events Another issue with quadrat analysis is that it is not sensitive to the relative position of the events within the quadrats. Consider for instance the following two patterns in pp0: plot(split(pp0.ppp)) These two patterns look quite different. And yet, when we count the events by quadrats: plot(quadratcount(split(pp0.ppp), nx = 3, ny = 3)) This example highlights how quadrats are relatively coarse measures of density, and fail to distinguish between fairly different event distributions, in particular because quadrat analysis does not take into account the relative position of the events with respect to each other. 10.7 Kernel Density In order to better take into account the relative position of the events with respect to each other, a different technique can be devised. Imagine that a quadrat is a kind of “window”. We use it to observe the landscape. When we count the number of events in a quadrat, we simply peek through that particular window: all events inside the “window” are simply counted, and all events outside the “window” are ignored. Then we visit another quadrat and do the same, until we have visited all quadrats. Imagine now that we define a window that, unlike the quadrats which are fixed, can move and visit different points in space. This window also has the property that, instead of counting the events that are in the window, it gives greater weight to events that are close to the center of the window, and less weight to events that are more distant from the center of the window. We can define such a window by selecting a function that declines with increasing distance. We will call this function a kernel. An example of a function that can work as a moving window is the following. # Here we create a data.frame to use for plotting; it includes a single column with a variable called `dist` for distance, that varies between -3 and 3; the function `stat_function()` is used in `ggplot2` to transform an input by means of a function, which in this case is `dnorm` the normal distribution! `ylim()` sets the limits of the plot in the y-axis ggplot(data = data.frame(dist = c(-3, 3)), aes(dist)) + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylim(c(0, 0.45)) As you can see, the value of the function declines with increasing distance from the center of the window (when dist == 0; note that the value never becomes zero!). Since we used the normal distribution, this is a Gaussian kernel. The shape of the Gaussian kernel depends on the standard deviation, which controls how “big” the window is, or alternatively, how quickly the function decays. We will call the standard deviation the kernel bandwidth of the function. Since the bandwidth controls how rapidly the weight assigned to distant events decays, if the argument changes, so will the shape of the kernel function. As an experiment, change the value of the argument sd in the chunk above. You will see that as it becomes smaller, the slope of the kernel becomes steeper (and distant observations are downweighted more rapidly). On the contrary, as it becomes larger, the slope becomes less steep (and distant events are weighted almost as highly as close events). Kernel density estimates are usually obtained by creating a fine grid that is superimposed on the region. The kernel function then visits each point on the grid and obtains an estimate of the density by summing the weights of all events as per the kernel function. Kernel density is implemented in spatstat and can be used as follows. The input is a ppp object, and optionally a sigma argument that corresponds to the bandwidth of the kernel: # The &quot;density&quot; function computes estimates of kernel density. Here we are creating a Kernel Density estimate using &quot;pp0.ppp&quot; from our data frame by means of a bandwidth defined by &quot;sigma&quot; kernel_density &lt;- density(split(pp0.ppp), sigma = 0.1) plot(kernel_density) Compare to the distribution of events: plot(split(pp0.ppp)) It is important to note that the gradation of colors is different in the two kernel density plots. Whereas the smallest value in the plot on the left is less than 20 and the largest is greater than 100, on the other plot the range is only between 45 to approximately 50. Thus, the intensity of the process is much higher at places in Pattern 1 that in Pattern 2. The plots above illustrate how the map of the kernel density is better able to capture the variations in density across the region. In fact, kernel density is a smooth estimate of the underlying intensity of the process, and the degree of smoothing is controlled by the bandwidth. References "],
["activity-5-point-pattern-analysis-ii.html", "Chapter 11 Activity 5: Point Pattern Analysis II 11.1 Practice questions 11.2 Learning objectives 11.3 Suggested reading 11.4 Preliminaries 11.5 Activity", " Chapter 11 Activity 5: Point Pattern Analysis II Remember, you can download the source file for this activity from here. 11.1 Practice questions Answer the following questions: How does the quadrat-based test of independence respond to a small number of quadrats? How does the quadrat-based test of independence respond to a large number of quadrats? What are the limitations of quadrat analysis? What is a kernel function? How does the bandwidth affect a kernel function? 11.2 Learning objectives In this activity, you will: Explore a dataset using quadrats and kernel density. Experiment with different parameters (number/size of kernels and bandwidths). Discuss the impacts of selecting different parameters. Hypothesize about the underlying spatial process based on your analysis. 11.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 11.4 Preliminaries For this activity you will need the following: An R markdown notebook version of this document (the source file). A package called geog4ga3. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(spatstat) library(geog4ga3) ## Warning: replacing previous import &#39;plotly::filter&#39; by &#39;stats::filter&#39; when ## loading &#39;geog4ga3&#39; ## Warning: replacing previous import &#39;dplyr::lag&#39; by &#39;stats::lag&#39; when loading ## &#39;geog4ga3&#39; In the practice that preceded this activity, you learned about the concepts of intensity and density, about quadrats, and also how to create density maps. Begin by loading the data that you will use in this activity: data(&quot;bear_df&quot;) This dataset was sourced from the Scandinavia Bear Project, a Swedish-Noruegian collaboration that aims to study the ecology of brown bears, to provide decision makers with evidence to support bear management, and to provide information regarding bears to the public. You can learn more about this project here. The project involves tagging bears with GPS units, so that their movements can be tracked. The dataset includes coordinates of one bear’s movement over a period of several weeksin 2004. The dataset was originally taken from the adehabitatLT package but was somewhat simplified for this activity. Instead of full date and time information, the point pattern is marked more simply as “Day Time” and “Night Time”, to distinguish between diurnal and nocturnal activity of the bear. Summarize the contents of this dataframe: summary(bear_df) ## x y marks ## Min. :515743 Min. :6812138 Day Time :502 ## 1st Qu.:518995 1st Qu.:6813396 Night Time:498 ## Median :519526 Median :6816724 ## Mean :519321 Mean :6816474 ## 3rd Qu.:519983 3rd Qu.:6818111 ## Max. :522999 Max. :6821440 The Min. and Max. of x and y give us an idea of the region covered by this dataset. We can use these values to approximate a window for the region (as an experiment, you could try changing these values to create regions of different sizes): W &lt;- owin(xrange = c(515000, 523500), yrange = c(6812000, 6822000)) Next, we can convert the dataframe into a ppp-class object suitable for analysis using the package spatstat: bear.ppp &lt;- as.ppp(bear_df, W = W) You can check the contents of the ppp object by means of summary: summary(bear.ppp) ## Marked planar point pattern: 1000 points ## Average intensity 1.176471e-05 points per square unit ## ## Coordinates are given to 1 decimal place ## i.e. rounded to the nearest multiple of 0.1 units ## ## Multitype: ## frequency proportion intensity ## Day Time 502 0.502 5.905882e-06 ## Night Time 498 0.498 5.858824e-06 ## ## Window: rectangle = [515000, 523500] x [6812000, 6822000] units ## (8500 x 10000 units) ## Window area = 8.5e+07 square units Now that you have loaded the dataframe and converted to a ppp object, you are ready for the next activity. 11.5 Activity *1. Analyze the point pattern for the movements of the bear using quadrat and kernel density methods. Experiment with different quadrat sizes and kernel bandwidths. Explain your choice of parameters (quadrat sizes and kernel bandwidths) to a fellow student. Decide whether these patterns are random, and support your decision. Do you see differences in the activity patterns of the bear by time of day? What could explain those differences, if any? Discuss the limitations of your conclusions, and of quadrat/kernel (density-based) approaches more generally. "]
]
