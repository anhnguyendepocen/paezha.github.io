[
["index.html", "Applied Spatial Statistics Preface", " Applied Spatial Statistics Antonio Paez 2018-12-03 Preface The objective of this book is to introduce selected topics in applied spatial statistics. The foundation for the book are the notes that I have developed over several years of teaching applied spatial statistics at McMaster University. This course is a senior level specialist course for geographers and students in other disciplines who are working towards specializations in Geographic Information Systems. Over the course of the years, my colleagues and I at McMaster have used at least three different textbooks for teaching spatial statistics. I have personally used McGrew and Monroe (2009) to introduce fundamental statistical concepts to geographers. This book (currently on its third edition with Lembo) does a fine job of introducing statistics as a tool for decision making, and is an very valuable resource to study matters of inference, for instance. Many of the examples in the book are geographical, however, the book is relatively limited in its coverage of spatial statistics (particularly models for spatial processes), which is a limitation for teaching a specialist course on this topic. My book of choice early on (approximately between 2003 and 2010) was the excellent book Interactive Spatial Data Analysis by Bailey and Gatrell (1995). I started using their book as a graduate student, but even then the limitations of the software that accompanied the book were apparent - in particular the absence of updates or a central repository for code. Despite the regretable obsolesence of the software, the book provided, and still does, a very accessible yet rigorous treatment of many topics of interest in spatial statistics. Bailey and Gatrell’s book was, I believe, the first attempt to bridge the need of teaching mid- and upper-level university courses in spatial statistics, and the challenges of doing so with very specialized texts on this topic, including the excellent but demanding Spatial Econometrics (Anselin 1988), Advanced Spatial Statistics (Griffith 1988), Spatial Data Analysis in the Social and Environmental Sciences (Haining 1990), not to mention Statistics for Spatial Data (Cressie 1993). Subsequently, my book of choice for teaching spatial statistics became O’Sullivan and Unwin’s Geographical Information Analysis (O’Sullivan and Unwin 2010). This book updates some topics that were not covered by Bailey and Gatrell. To give an example, much work happened in the mid- to late-nineties with the development of local forms of spatial analysis, including Getis and Ord pioneering research on concentration statistics (Getis and Ord 1992), Anselin’s Local Indicators of Spatial Association (Anselin 1995), and Brunsdon, Fotheringham, and Charlton’s research on geographically weighted regression (Brunsdon, Fotheringham, and Charlton 1996). These and related local forms of spatial analysis have become hugely influential in the intervening years, and are duly covered by O’Sullivan and Unwin in a way that merges well with a course focusing on spatial statistics - although other specialist texts also exist that delve in much more depth into this topic (e.g., Fotheringham and Brunsdon 1999; and Lloyd 2010). These resources, and many more, have proved invaluable for my teaching for the past few years, and I believe that their influence will be evident in the present book. So, if there are some excellent resources for teaching and learning spatial statistics, why am I moved to unleash on the world yet another book on this topic? introduces data analysis for applied spatial scientists. These could be geographers, earth scientists, environmental scientists, planners, or others who work with georeferenced datasets. My aim with this book has been to introduce key concepts and techniques in the analysis of spatial data in an intuitive way. While there are more advanced treatments of many of these topics, this book should be appealing to students or others who are approaching this topic for the first time. The book is organized thematically following the cannonical approach seen, for instance, in Bailey and Gatrell (1995) and O’Sullivan and Unwin (2010). This approach is to conceptualize data by their unit of support. Each chapter covers a topic that builds on previous material. In this way, this is not necessarily meant as a reference (there are better books for that). The chapters are followed by an activity. I have used these materials for teaching spatial data analysis in different settings. In a flipped classroom setting, the chapters are used as practice material by the students before class. The activity is then completed in class, with the instructor providing support and motivating some discussion. Used in a traditional lecture style, the materials provide structure and visual aids. The activities can be completed by the students as homework or during lab time. References "],
["basic-operations-in-r.html", "Chapter 1 Basic Operations in R 1.1 Learning objectives 1.2 RStudio Window 1.3 Some basic operations 1.4 Data Classes in R 1.5 Data Types in R 1.6 Indexing and Data Transformations 1.7 Visualization 1.8 Creating a simple map", " Chapter 1 Basic Operations in R NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. Now that you have installed R and RStudio we can begin with an overview of basic operations and data structures in this computing language. Please note that this document you are reading, called an R Notebook, is an example of what is called “literate programming”, a style of document that uses code to illustrate a discussion, as opposed to the traditional programming style that uses natural language to discuss/document the code. It flips around the usual technical writing approach to make it more intuitive and accessible. Whenever you see a chunk of code as follows, you can run it (by clicking the ‘play’ icon on the top right corner) to see the results. Try it! print(&quot;Hello, Geography 4GA3&quot;) ## [1] &quot;Hello, Geography 4GA3&quot; The chunk of code above instructed R (and trough R the computer) to print (or display on the screen) some text. 1.1 Learning objectives In this practice, you will learn: How to install R. Basic operations in R. Data classes, data types, and data transformations. About packages in R. Basic visualization. 1.2 RStudio Window If you are reading this, you probably already read the document ‘00 Installation of R’. We can now proceed to discuss some basic concepts of operations and data types. 1.3 Some basic operations R can perform many types of operations. Some simple operations are arithmetic. Other are logical. And so on. For instance, R can be instructed to conduct sums, as follows: 2 + 2 ## [1] 4 R can be instructed to do multiplications: 2 * 3 ## [1] 6 And sequences of operations, using brackets to indicate their order. Compare the following two expressions: 2 * 3 + 5 ## [1] 11 2 * (3 + 5) ## [1] 16 Other operations produce logical results (values of true and false): 3 &gt; 2 ## [1] TRUE 3 &lt; 2 ## [1] FALSE And of course, you can combine operations in an expression: 2 * 3 + 5 &lt; 2 * (3 + 5) ## [1] TRUE As you can see, R can be used as a calculator, but it is much more powerful than that. We can also create variables. You can think of a variable as a box with a name, whose contents can change. Variables are used to keep track of important stuff in your calculations, and to automate operations. To create a variable, a value is assigned to a name, using this notation &lt;-. You can read this x &lt;- 2 as “assign value of 2 to a variable called x”. For instance: x &lt;- 2 y &lt;- 3 z &lt;- 5 Check your “Global Environment”, the tab where the contents of your “Workspace” are displayed for you. You can also simply type the name of the variable in the Console to see its contents. Now that we have some variables with values, we can express operations as follows (same as above) x * y + z ## [1] 11 x * (y + z) ## [1] 16 However, if we wanted, we could change the values of any of x, y, and/or z and repeat the operations. This allows to automate some instructions: x &lt;- 4 x * y + z ## [1] 17 1.4 Data Classes in R R can work with different data classes, including: Numerical Character Logical Factor This allows you to store information in different forms, which can be useful. For instance, you may want to save some text: name &lt;- &quot;Hamilton&quot; Or numerical information: population &lt;- 551751 If you wish to check what class an object is, you can use the function class: class(name) ## [1] &quot;character&quot; class(population) ## [1] &quot;numeric&quot; 1.5 Data Types in R R can work with different data types, including scalars (essentially matrices with only one element), vectors (matrices with one dimension of size 1) and matrices (more generally. print(&#39;This is a scalar&#39;) ## [1] &quot;This is a scalar&quot; 1 ## [1] 1 print(&#39;This is a vector&#39;) ## [1] &quot;This is a vector&quot; c(1,2,3,4) ## [1] 1 2 3 4 print(&#39;This is a matrix&#39;) ## [1] &quot;This is a matrix&quot; matrix(c(1,2,3,4),nrow = 2, ncol=2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 The command c() is used to concatenate the arguments. The command matrix() creates a matrix with the specified number of rows and columns. An important data type in R is a data frame. A data frame is a table consisting of rows and columns - commonly a set of vectors that have been collected for convenience. A data frame is used to store data in digital format. (If you have used Excel or another spreadsheet software before, data frames will be familiar to you: they look a lot like a sheet in a spreadsheet.) A data frame can accommodate large amounts of information (several billion individual items). The data can be numeric, character, logical, and so on. Each grid cell in a data frame has an address that can be identified based on the row and column it belongs to. R can use these addresses to perform mathematical operations. R labels columns alphabetically and rows numerically (or less commonly alphabetically). To illustrate a data frame, let us first create the following vectors, that include names, populations, average salaries, and coordinates of some cities: Name &lt;- c(&#39;Hamilton&#39;,&#39;Waterloo&#39;,&#39;Toronto&#39;) Population &lt;- c(551751, 219153, 2731571) AvgSalary &lt;- c(45692, 57625, 48920) Latitude &lt;- c(43.255203, 43.4668, 43.6532) Longitude &lt;- c(-79.843826, -80.51639, -79.3832) Again, note that &lt;- is an assignment. In other words, it assigns the item on the right to the name on the left. After you execute the chunk of code above, you will notice that new values appear in your Environment. These are five vectors of size 1:3, one that is composed of alphanumeric information (or chr, for ‘character’) and four columns that are numeric (num). These vectors can be collected in a dataframe. This is done for convenience, so we know that all these data belong together in some way. Please note that to create a data frame, the vectors must have the same length. In other words, you cannot create a table with elements that have different numbers of rows (other data types allow you to do this, but not data frames). We will now create a data frame. We will call it “Cities”. There are rules for names, but in most cases it helps if the names are intuitive and easy to remember. The function used to create a data frame is data.frame() and the arguments are the vectors that we wish to collect there. Cities &lt;- data.frame(Name, Population, AvgSalary, Latitude, Longitude) After running the chunk above, now you have a new object in your environment, namely a data frame called Cities. If you double clic on Cities in the Environment tab, you will see that this data frame has five columns (labeled Name, Population, AvgSalary, Latitude, and Longitude), and three rows. You can enter data into a data frame and then use the many built-in functions of R to perform various types of analysis. Please note that Name, which was an alphanumeric vector, was converted to a factor in the data frame. A factor is a way to store nominal variables that may have two or more levels. In the present case, the factor variable has three levels, corresponding to three cities. If we had information for multiple years, each city might appear more than once, for each year that information was available. 1.6 Indexing and Data Transformations Data frames store information that is related in a compact way. To perform operations effectively, it is useful to understand the way R locates information in a data frame. As noted before, each grid cell has an address, or in other words an index, that can be referenced in several convenient ways. For instance, assume that you wish to reference the first value of the data frame, in other words, row 1 of column Name. To do this, you would go use the following instruction: Cities[1,1] ## [1] Hamilton ## Levels: Hamilton Toronto Waterloo This will recall the element in the first row and first column of Cities. As an alternative, you could type: Cities$Name[1] ## [1] Hamilton ## Levels: Hamilton Toronto Waterloo As you see, this has the same effect. The string sign $ is used to reference columns in a data frame. Therefore, R will call the first element of Name in data frame Cities. Cities[1,2] is identical to Cities$Name[2]. Try changing the code in the chunk and executing. If you type Cities$Name, R will recall the full column. Indexing is useful to conduct operations. Suppose for instance, that you wished to calculate the total population of two cities, say Hamilton and Waterloo. You can execute the following instructions: Cities$Population[1] + Cities$Population[2] ## [1] 770904 (More involved indexing is also possible, for example, if we use logical operators. Do not worry too much about the details, but you can verify that the results are identical) Cities$Population[Cities$Name==&#39;Hamilton&#39;] + Cities$Population[Cities$Name==&#39;Waterloo&#39;] ## [1] 770904 Suppose that you wanted to calculate the total population of the cities in your data frame. To do this, you would use the instruction sum: sum(Cities$Population) ## [1] 3502475 You have already seen how it allows you to store in memory the results of some instruction, by means of an assignment &lt;-. You can also perform many other useful operations. For instance, calculate the maximum value for a set of values: max(Cities$Population) ## [1] 2731571 And, if you wanted to find which city is the one with the largest population, you would use a logical statement as an index: Cities$Name[Cities$Population==max(Cities$Population)] ## [1] Toronto ## Levels: Hamilton Toronto Waterloo As you see, Toronto is the largest city (by population) in this dataset. Using indexing in imaginative ways provides a way to do fairly sophisticated data analysis. Likewise, the function for finding the minimum value for a set of values is min: min(Cities$Population) ## [1] 219153 Try calculating the average of the population of the cities, using the command mean. Use the empty chunk below for this (the result should be 1167492): Finding the maximum and minimum, aggregating (calculating the sum of a series of values), and finding the average are examples of transformations applied to the data. They give insights into aspects of the dataset that are not evident from the raw data. 1.7 Visualization The data frame, in essence a table, informative as it is, may not be the best way to learn from the data. Visualization is often a valuable complement to data analysis. Say, we might be interested in finding which city has the largest population and which city has the smallest population. We could achieve this by using similar instructions as before, for example: paste(&#39;The city with the largest population is&#39;,Cities$Name[Cities$Population==max(Cities$Population)]) ## [1] &quot;The city with the largest population is Toronto&quot; paste(&#39;The city with the smallest population is&#39;, Cities$Name[Cities$Population==min(Cities$Population)]) ## [1] &quot;The city with the smallest population is Waterloo&quot; (Note that paste is similar to print, except that it converts everything to characters before printing. We use this command because the contents of Name in data frame Cities are not characters, but levels.) A more convenient way of understanding these data is by visualizing them, using for instance a bar chart. We will proceed to create a bar chart, using a package called ggplot2. This package implements a grammar of graphics, and is a very flexible way of creating plots in R. Since ggplot2 is a package, we first must ensure that it is installed. You can install it using the command install as follows: install.packages(&quot;ggplot2&quot;) As an alternative, you can use the Packages tab in RStudio. Simply navigate to the tab, click install, and select ggplot2 from there. Note that you need to install the package only once! Essentially install adds it to your library of packages, where it will remain available. Once the package is installed, it becomes available, but to use it you must load it in memory. For this, we use the command library(), which is used to load a package, that is, to activate it for use. Assuming that you already have installed ggplot2, we proceed to load it: library(ggplot2) Now all commands from the ggplot2 package are available to you. This package works by layering a series of objects, beginning with a blank plot, to which we can add things. The command to create a plot is ggplot(). This command accepts different arguments. For instance, we can pass data to it in the form of a data frame. We can also indicate different aesthetic values, that is, the things that we wish to plot. None of this is plotted, though, until we indicate which kind of geom or geometric object we wish to plot. For a bar chart, we would use the following instructions: ggplot(data = Cities, aes(x = Name, y = Population)) + geom_bar(stat = &#39;identity&#39;) Let us break down these instructions. We are asking ggplot2 to create a plot that will use the data frame Cities. Furthermore, we tell it to use the values of Names in the x-axis, and the values of Population in the y-axis. Run the following chunk: ggplot(data = Cities, aes(x = Name, y = Population)) Notice how ggplot2 creates a blank plot, but it has yet to actually render any of the population information in there. We layer elements on a plot by using the + sign. It is only when we tell the package to add some geometric element that it renders something on the plot. In the previous case, we told ggplot2 to draw bars (by using the geom_bar command). The argument of geom_bar was stat = 'identity', to indicate that the data for the y-axis was to be used ‘as-is’ without further statistical transformations. There are many different geoms that can be used in ggplot2. You can always consult the help/tutorial files by typing ??ggplot2 in the console. See: ??ggplot2 ## starting httpd help server ... done 1.8 Creating a simple map We will see how maps are used in spatial statistical analysis. The simplest one that can be created is a so-called dot map that displays the location of an event of interest. A dot map is, in fact, simply a scatterplot of the coordinates of events. We can use ggplot2 to create a simple dot map of the cities in your simple dataset. For this, we create a ggplot object, and for the x and y aesthetics we use the coordinates. The geometric element that we want to render is a point: ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point() This is a simple dot map that simply shows the locations of the cities. We can add labels by means of the geometric element text: ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point() + geom_text(aes(label = Name)) A proportional symbol map changes the size of the symbols to add information to the plot. To create a proportional symbol map, we add to the aesthetics the instruction to use some variable for the size of the symbols: ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point(aes(size = Population)) + geom_text(aes(label = Name)) And fix the position of the labels by adding a vertical justification to the text (vjust) and expanding the limits of the plot (expand_limits): ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point(aes(size = Population)) + geom_text(aes(label = Name), vjust = 2) + expand_limits(x = c(-80.7, -79.2), y = c(43.2, 43.7)) You have now created a relatively simple proportional symbols map! You can see that creating a plot is simply a matter of instructing R (through ggplot2) to complete a series of instructions: Create a ggplot2 object using a dataset, which will render stuff at locations given by variable1 and variable 2: ggplot(data = dataset, aes(x = variable1, y = variable2)) Add stuff to the plot. For instance, to add points use geom_point, to add lines use geom_line, and so on. Check the ggplot2 Cheat Sheet for more information on how to use this package. A last note. Many other visualization alternatives (for instance, Excel) provide point-and-click functions for creating plots. In contrast, ggplot2 in R requires that the plot be created by meticulously instructing the package what to do. While this is more laborious, it also means that you have complete control over the creation of plots, which in turn allows you to create more flexible and inventive visuals. This concludes your basic overview of basic operations and data structures in R. You will have an opportunity to learn more about creating maps in R with your reading. "],
["introduction-to-mapping-in-r.html", "Chapter 2 Introduction to Mapping in R 2.1 Learning objectives 2.2 Preliminaries 2.3 Packages 2.4 Exploring dataframes and a simple proportional symbols map 2.5 Improving on the proportional symbols map 2.6 Some simple spatial analysis 2.7 Other resources 2.8 References", " Chapter 2 Introduction to Mapping in R NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. Spatial statistics is a sub-field of spatial analysis that has grown in relevance in recent years as a result of 1) the availability of information that is geo-coded, in other words, that has geographical references; and 2) the availability of software to analyze such information. A key technology fuelling this trend is that of Geographical Information Systems (GIS). GIS are, at their simplest, digital mapping for the 21st century. In most cases, however, GIS go beyond cartographic functions to also enable and enhance our ability to analyze data. There are many available packages for geographical information analysis. Some are very user friendly, and widely available in many institutional contexts, such as ESRI’s Arc software. Others are fairly specialized, such as Caliper’s TransCAD, which implements many operations of interest for transportation engineering and planning. Others packages have the advantage of being more flexible and/or free. Such is the case of the R statistial computing language. R has been adopted by many in the spatial analysis community, and a number of specialized libraries have been developed to support mapping and spatial data analysis functions. The objective of this note is to provide an introduction to mapping in R. Maps are one of the fundamental tools of spatial statistics and spatial analysis, and R allows for many GIS-like functions. To use this note you will need the following: This R markdown notebook. A package called geog4ga3 In the previous reading/practice you created a simple proportional symbols map. In this reading/practice you will learn how to create more sophisticated maps. 2.1 Learning objectives In this reading, you will: Revisit how to install and load a package. Learn how to invoke a data and view the data structure. Learn how to easily create maps using R. Think about how statistical maps help us understand patterns. 2.2 Preliminaries It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. 2.3 Packages According to Wickham (2015) packages are the basic units of reproducible code in the R multiverse. Now that your workspace is clear, you can proceed to load a package. In this case, the package is the one used for this book/course, called geog4ga3. The function used to load a package is library, as follows: library(geog4ga3) The package includes a few datasets that will be used throughout the book. You can check which datasets are available in this package by using the function data: data(&quot;snow_deaths&quot;) 2.4 Exploring dataframes and a simple proportional symbols map If you correctly loaded the library, you can now access the dataframes in the package geog4ga3. For this section, you will need two dataframes, namely snow_pumps and snow_deaths. You can examine the contents of these dataframes by means of the command head. This command displays the first few rows of the dataframe. Try it: head(snow_deaths) ## long lat Id Count ## 0 -0.1379301 51.51342 1 3 ## 1 -0.1378831 51.51336 2 2 ## 2 -0.1378529 51.51332 3 1 ## 3 -0.1378120 51.51326 4 1 ## 4 -0.1377668 51.51320 5 4 ## 5 -0.1375369 51.51318 6 2 These data are from the famous London cholera example. This is the study by John Snow (not the one from Game of Thrones, but the British physician) into the cholera outbreak of Soho, London, in 1854. John Snow is considered the father of spatial epidemiology, and his study mapping the outbreak is credited with helping find its cause. The dataframe snow_deaths includes the geocoded addresses of cholera deaths in long and lat, and the number of deaths (the Count) recorded at each address, as well as unique identifiers for the addresses (Id). A second dataframe snow_pumps includes the geocoded locations of water pumps in Soho: head(snow_pumps) ## long lat Id Count ## 01 -0.1366679 51.51334 251 1 ## 1100 -0.1395862 51.51388 252 1 ## 250 -0.1396710 51.51491 253 1 ## 310 -0.1316299 51.51235 254 1 ## 410 -0.1335944 51.51214 255 1 ## 510 -0.1359191 51.51154 256 1 As in your previous reading, it is possible to map the cases using ggplot2. Begin by loading the package you will need: library(tidyverse) ## -- Attaching packages -------------------------------------------------------------------- tidyverse 1.2.1 -- ## v ggplot2 3.1.0 v purrr 0.2.5 ## v tibble 1.4.2 v dplyr 0.7.7 ## v tidyr 0.8.2 v stringr 1.3.1 ## v readr 1.1.1 v forcats 0.3.0 ## -- Conflicts ----------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Now, you can create a blank ggplot2 object on which you can render the points for deaths and the pumps. ggplot() + geom_point(data = snow_deaths, aes(x = long, y = lat), color = &quot;blue&quot;, shape = 16) + geom_point(data = snow_pumps, aes(x = long, y = lat), color = &quot;black&quot;, shape = 17) This does a decent job of displaying the information, now using different colors and shapes for different types of events (deaths and pumps). However, it is not a very good quality map. 2.5 Improving on the proportional symbols map A package that extends the functionality of mapping in R is leaflet. We will see next how to enhance our proportional symbol map using this package. First you need to load the package (you need to install it first if you have not already): library(leaflet) The first step is to create a leaflet object, which will be saved in m. To do this we use the function leaflet and we indicate a dataset to use for our map, in this case the dataframe snow_deaths. In addition, we set the view for the map using the setView function, and indicate the center of the map (which we can get after inspecting the previous symbols map), as well as the zoom (16 is the zoom for a neighborhood): m &lt;- leaflet(data = snow_deaths) %&gt;% setView(lng = -0.136, lat = 51.513, zoom = 16) We can add a basemap or background map by means of the addTiles function: m &lt;- m %&gt;% addTiles() Finally, we add the cases of cholera deaths to the map. For this, we indicate the coordinates (preceded by ~), and set an option for clustering by means of the clusterOptions as follows: m &lt;- m %&gt;% addMarkers(~long, ~lat, clusterOptions = markerClusterOptions(), group = &quot;Deaths&quot;) m To the map above we could also add the location of the pumps (notice that the Broad Street Pump is already shown in the basemap!): m %&gt;% addMarkers(data = snow_pumps, ~long, ~lat, group = &quot;Pumps&quot;) The above results in a much nicer map. Is this map informative? What does it tell you about the incidence of cholera and the location of the pumps? 2.6 Some simple spatial analysis We could even begin to do some spatial analysis on this map! For instance, we could create a heatmap. You have probably seen heatmaps in many different situations before, as they are a popular visualization tool. Heatmaps are created based on a spatial analytical technique called kernel analysis. We will cover this technique in more detail later on. For the time being, it can be illustrated by taking advantage of the leaflet.extras package, which contains a heatmap function. Load the package as follows: library(leaflet.extras) Next, create a second leaflet object for this example, and call it m2. Notice that we are using the same setView parameters: m2 &lt;- leaflet(data = snow_deaths) %&gt;% setView(lng = -0.136, lat = 51.513, zoom = 16) %&gt;% addTiles() Then, add the heatmap. The function used to do this is addHeatmap. We specify the coordinates and the variable for the intensity (i.e., each case in the dataframe is representative of Count deaths at the address). Two parameters are important here, the blur and the radius. If you are working with the R notebook version of the book, experiment changing these parameters: m2 %&gt;% addHeatmap(lng = ~long, lat = ~lat, intensity = ~Count, blur = 40, max = 1, radius = 25) Lastly, you can also add markers for the pumps as follows: m2 %&gt;% addHeatmap(lng = ~long, lat = ~lat, intensity = ~Count, blur = 40, max = 1, radius = 25) %&gt;% addMarkers(data = snow_pumps, ~long, ~lat, group = &quot;Pumps&quot;) A heatmap (essentially a kernel density of spatial points; more on this in a later chapter) makes it very clear that most cases of cholera happend in the neighborhood of one (possibly contaminated) water pump! At the time, Snow noted with respect to this geographical pattern that: “It will be observed that the deaths either very much diminished, or ceased altogether, at every point where it becomes decidedly nearer to send to another pump than to the one in Broad street. It may also be noticed that the deaths are most numerous near to the pump where the water could be more readily obtained.” Snow’s analysis led to the closure of the pump, after which the cholera outbreak subsided. This illustrates how even some relatively simple spatial analysis can help to inform public policy and even save lives. You can read more about this case here. In this practice you have learned how to implement some simple mapping and spatial statistical analysis using R. In future readings we will further explore the potential of R for both. 2.7 Other resources If you would like to experiment some more with this dataset, visit a Shiny app that lets you explore this dataset. For more information on the functionality of leaflet, please check Leaflet for R 2.8 References "]
]
