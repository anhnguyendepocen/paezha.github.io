[
["index.html", "An Introduction to Discrete Choice Analysis A Course in R Preface Choices, choices, choices", " An Introduction to Discrete Choice Analysis A Course in R Antonio Paez 2019-02-04 Preface ‘Would you tell me, please, which way I ought to go from here?’ ‘That depends a good deal on where you want to get to,’ said the Cat. ‘I don’t much care where -’ said Alice. ‘Then it doesn’t matter which way you go,’ said the Cat. ‘- so long as I get SOMEWHERE,’ Alice added as an explanation. ‘Oh, you’re sure to do that,’ said the Cat, ‘if you only walk long enough.’ — Lewis Carroll, Alice in Wonderland “We are our choices.” — Jean-Paul Sartre Choices, choices, choices We are what we choose or we choose what we are. The choices we make have important implications for how we interact with the world. We live in a time when resources continue to become increasingly scarce. In this context, the way people make choices is informative about their preferences and where collectively we are going. This includes routine activities, such as deciding how to travel for everyday purposes, for instance walking or cycling, driving, or using transit. Or longer term decision, such as whether to contract more expensive but environmentally less damaging low impact energy sources; buy hybrid, electric, or gasoline vehicles; the frequency with which they travel by plane; to live in a distant suburb where rent is low, or in a central city where space is at a premium; and many others. Understanding decision-making is also important so that the world, or more accurately the social institutions that collectively represent us in our interactions with the world, can better accommodate and possibly even nudge our preferences towards socially desirable outcomes. What tradeoffs are members of the public willing to contemplate when choosing alternative mobility? Is it the range of vehicles? Their price? The time it takes to charge an electric vehicle? The satisfaction of being green? Should governments subdidize purchases of efficient heating? If so, what is the effect of a certain amount in subsidy? Do consumers prefer more range in electric vehicles, and how much are they willing to pay for it? Does the tradoff justify the increase in production cost? These questions are important as governments and businesses try to understand the way the public will respond to taxation, programs, engineering, or production decisions. In simple terms, discrete choice analysis is a way to understand behavior when there are implicit markets, and in this way it represents a form of hedonic price analysis. What are prices? In very general terms a price is a quantity offered in payment for one unit of a good or service. Early economies were based on bartering, a system of exchange that is limited use in even moderately complex economic systems. For instance, imagine a moderately sophisticated economy where people already have ceased being generalists to become specialists of some sort. Someone who spends time making shoes probably has limited time to tend chickens or understand the law of the land. Imagine that you make and sell shoes. People need shoes and may be willing to offer some quantity of something in exchange for them. The person who breeds chicken maybe willing to offer one chicken for a pair of shoes. Is that a fair price? How can you determine whether that exchange is sensible? Imagine now that you need a divorce, and a lawyer’s services are required for this. How many shoes should you offer the lawyer for a divorce? And what if the lawyer already has shoes?? Maybe the lawyer would prefer to be paid in chickens… The situation is further complicated by the reliance of bartering on some kind of trust system: as a seller, in a bartering system, there are no simple ways of ensuring the quality of the exchange! For instance, what if the lawyer is a crook, or the farmer gives you diseased chickens for your top-notch, high-quality shoes? In small systems, where agents can recognize each other, trust is enforced by reputation - if the lawyer is crooked, or the farmer is known to feed lead to the poultry, other actors can avoid transactions with them. If your shoes fall apart in the first rain, people will begin to avoid doing business with you! As these simple examples illustrate, bartering is a complicated way of setting prices, and this becomes increasingly complex (except in very exceptional situations) when an economy produces hundreds, thousands, or even more different products and services. (An interesting exception is the time when Pepsico bartered with the Soviet Union; see this news item from 1990. In this case, Pepsi-Cola was bartered for ships and vodka. Why did barter make sense in this situation? Hint: the ruble was not freely traded on world currency markets.) Far from being the root of all evil, it is almost certain that no complex economy can exist without the invention of money. The complexities of bartering explain why monetary systems were invented: the need for a common standard for exchange. Instead of needing to figure out how many shoes is worth a chicken and how many chickens a divorce, everything is measured using the same metric: shells, squares of deer skin, rupees, pesos, or dollars. "],
["price-mechanisms.html", "Chapter 1 Price Mechanisms Plan Audience Requisites", " Chapter 1 Price Mechanisms The limitations of bartering explain the necessity of monetary systems. A common currency frees the maker of shoes from the need to calculate in chickens the cost of his divorce. But it does not explain how prices are set in the common currency. Price mechanisms depend to a large extent of the institutional framework. Several such frameworks exist. For example, in a centrally planned economy, prices for goods or services are set by a designated agent. This could be the Elder of the Village. The Elder of the village decides how many rupees people should pay for your shoes (alternatively, how much you can charge for a pair of shoes), and how much you should pay a lawyer for each hour of work. Everyone in this kind of setup prays that the Elder of the Village knows what he is doing, and the potential for mistakes clearly is far from negligible. In the Soviet Union prices were set using so called material balances, balancing the inputs to the planned outputs. This approach was not successful for several reasons, including ideological limitations to the mathematical tools used to calculate balances, and the inherent limitations of such planning, which does not allow for deviations from the plan (what if you end up needing two divorces instead of only one or none?). In a free market economy, on the other hand, prices are left to float with no intervention from central planning agencies. There are voluminous literature explaining how this system can allocate resources efficiently. Prices, in this context, are signals of how desirable a good or service is, and how much of it is available. Economists explain this relationship using a relationship between demand and supply. The basic assumption (which happens to hold in many cases) is that the level of demand for a good or service (the quantity that consumers are willing to purchase) declines as the price increases. On the side of producers/providers of goods and services, the level of supply (the quantity that they are willing to produce) increases with price. Demand and supply, then, are influenced by price, but they do not happen in isolation - rather, they interact to set prices. A consumer cannot single-handedly demand that a good/service be sold at a certain (i.e., low) price when many consumers are willing to pay a somewhat higher price for the same (otherwise I would have bought my Nintendo Switch at 50 dollars). Likewise, a producer/provider cannot expect to set a high price for a good/service when other producers are willing to sell at a lower price. (There are aberrant situations, of course. Monopolies and cartels can manipulate prices on behalf of producers, whereas single-payer health care manipulates prices in favor of patients.) The intersection of the supply and demand curves determines simultaneously prices and the quantity of a good/service produced/consumed. Since prices “float”, they can adjust to changes in supply and/or demand. In Figure 1, for example, when demand for a good or service (say divorces) increases, there is an incentive for lawyers to work more hours. Since there is a limited number of hours that lawyers can work (there is scarcity), this is reflected in a higher price, since those who can afford it will be willing to pay more for a scarce but desirable service. Fig 1. Supply and Demand Relationship A third system is a mixed economy, where prices are left to float but within limits or with other corrective mechanisms, such as subsidies or taxes. The most familiar situation for most of us is price mechanisms in free or mixed economies. An underlying assumption is that a market exists for the good or service, that is, a medium where goods or services can be exchanged. Markets exist for many things: for milk, for bread, for insurance, and for complex financial instruments that no one really understands, such as derivatives. Markets do not explicitly exist for composite products or services, and therefore the willingnes to pay of consumers for elements of a specific good. Imagine, for instance, a good such as an automobile. Automobiles are composite goods in the sense that, even if their purpose is to provide transportation, they can do this in many different ways to satisfy a diversity of needs or tastes: with variations in leg room, acceleration, and fuel consumption, to name a few. Pricing mechanisms for the whole leave the question of willingness to pay for specific components in the dark. Are consumers willing to pay more or less for extra leg room, more spacious seats, or horsepower? The markets for each of these items are implicit in the market for automobiles. In fact, hedonic price analysis analysis was invented by an economist named Andrew Court to address such an issue. Court was an economist for the Automobile Manufacturers’ Association in Detroit from 1930 to 1940. He realized that price indexing procedures were not satisfactory for describing the relative importance of various components of automobiles in determining their price. This in turn was important to understand consumer preferences and to differentiate products. Court used the term “hedonic” to express the usefulness and desirability (related to pleasure) that consumers attach to different aspects of a composite product. Although he invented his method in the late 1930s, it lay dormant for approximately twenty years until it was popularized by Zvi Griliches in the 1960s, with work on fertilizers and automobiles. Later on, Sherwin Rosen explained implicit markets within an economic foundation of supply and demand in equilibrium - that is, not just as willingness to pay on the side of consumers, but also as the result of decisions by producers. In brief, Rosen explained how the differentiated hedonic price function represents the envelope of a family of “value” functions (willingness to pay) and a family of offer functions (willingess to sell). Since then, many uses have been found for hedonic price analysis, with applications ranging from the pricing of computers, personal digital assistants, and wine to online purchases and farmland. The field of discrete choice analysis is concerned with the analysis of implicit markets when the outcome of the choice process is discrete. This requires a few things: DESCRIBE THE CONDITIONS FOR A PROCESS TO BE A CHOICE. Plan The plan with these notes is to introduce discrete choice analysis in an intuitive way. To achieve this, I use examples and coding, lots of coding. There are some classical references, for instance Ben-Akiva and Lerman (1985) and Train (2009), and then more specialized books such as Louviere et al. (2000). Other books cover discrete choice analysis as one component of modelling systems (such as transportation; see Ortúzar and Willumsen 2011), or cover related topics but from a statistical perspective (Maddala 1983). The present notes should be appealing to students or others who are approaching this topic for the first time, I strongly encourage readers to become acquainted with these books if they have not already. Each author organizes topics in a way that is logical to them. Some texts begin with a coverage of fundamental mathematics, probability, and statistics. Others with an introduction to a substantive topic (e.g., the context of travel demand analysis). In the book Applied Choice Analysis: A Primer by Hensher et al. (2005), the title of Chapter 10 is “Getting Started Modeling”. Train (2009), in contrast, begins by discussing the properties of discrete choice models and discussing the logit model right away. For presentation I have in the past relied heavily on Train’s book to organize my graduate seminar. I find this style of presentation sufficiently intuitive, when combined with some relevant topics introduced at key points. For example, I find that it makes sense to have a discussion of specification and estimation of models after introducing the logit model. In this way, the details of specifying utility functions can be presented in the context of an operational model. Readers will notice that this notes tend to follow Train closely, using a thematic approach, moving from the fundamentals, introducing the logit model, and then by families of models, i.e., GEV, probit, and so on. Beginning early in the text, readers are asked to get their hands dirty with code. This is a very deliberate decision. Most books on discrete choice analysis are software-independent, meaning that they cover the topics without making reference to a particular statistical package for analysis. Others rely for presentation on a specific software. For instance, Hensher et al. (2005) refer extensively to the software NLOGIT, a software package sold by Econometric Software, Inc.. Yet other packages were originally developed independently of a statistical computing project. One example is Michel Bierlaire’s BIOGEME. Not being associated with a statistical computing project means that synergies with other packages cannot be realized. Newer versions of BIOGEME now exist written almost exclusively in Python and that allow the package to benefit from the Python Data Analysis Library Pandas. For this text, I have chosen R. R is a generalist statistical language with a very broad user base. I personally find R more accessible than Python, for example, as an introduction to statistical and data analysis computing, particularly with the support of a good Interactive Development Environment such as RStudio. Packages (the fundamental units of shareable code in R) benefit from the synergyes of many developers and users sharing their code in a transparent and open way. Ten years ago it would have been very difficult to write a book on discrete choice modelling based on R: the earliest version of Croissant’s mlogit package (Croissant 2018) dates from 2009; the earliest version of Sarrias and Daziano’s gmnl package (Sarrias and Daziano 2017) dates from 2015. Secondly, R and related packages are free. It is my conviction that research can be accelerated by the generous contributions of developers who graciously share their code with the world. By doing this, they help to maintain the cost of research low, and thus enable more people around the world to engage in it. That said, there is a potential disadvantage: unlike more established (especially commercial) packages that have been kicking around for years if not decades, newer R packages may still have some limitations. To mention one, the current versions of the packages mlogit and gmnl are implemented exclusively for universal choice set, in other words, under the assumption that all alternatives are available to all decision-makers. There are situations, For example, suppose My plan for this text, therefore, is to cover a topic in each section that builds on previous material. I have used the materials presented in these notes (in different incarnations) for teaching discrete choice analysis in different settings. Primarily, these notes have been used in the course GEOG 738 Discrete Choice Analysis at McMaster University. This course is a full (Canadian) graduate term, which typically means 11 or 12 weeks of classes. The course is organized as a 2-hour seminar that is offered once per week. Accordingly, each section is designed to cover very approximately the material that I am used to cover in a 2 hour seminar. As I continue to work on these notes, I hope to be able to add optional (or bonus) chapters, that could be used 1) to extend a course on discrete choice analysis beyond the 12 week horizon of the Canadian graduate school term, and/or 2) to offer more advanced material to interested readers. Audience The notes were designed for a graduate course in geography, but are not necessarily limited to geographers, and could indeed be a valuable resource to graduate students and others interested in discrete choice analysis and applications in economics, planning, transportation engineering, public health, etc. The prerequisites are an introductory college/university level course on multivariate statistics, ideally covering the fundamentals of probability theory and hypothesis testing. Working knowledge of multivariate linear regression analysis is a bonus but not strictly required. Requisites This book is not a course to learn R. The language is introduced progressively, and assumes that readers are computer-literate and have possible done some basic coding in the past. For readers who wish to learn R there are other sources such as Wickham and Grolemund (2016)(https://r4ds.had.co.nz/) or Albert and Rizzo (Albert and Rizzo 2012). To fully benefit from this text, up-to-date copies of R and RStudio are highly recommended. There are different packages that implement discrete choice methods in R. I will particularly rely on the packages mlogit and gmnl. References "],
["preliminaries-installing-r-and-rstudio.html", "Chapter 2 Preliminaries: Installing R and RStudio 2.1 Introduction 2.2 Learning Objectives 2.3 R: The Open Statistical Computing Project 2.4 Packages in R", " Chapter 2 Preliminaries: Installing R and RStudio 2.1 Introduction The course makes extensive use of R and RStudio. Here are the instructions to install these two tools. 2.2 Learning Objectives In this reading, you will learn: How to install R. About the RStudio Interactive Development Environment. About packages in R. 2.3 R: The Open Statistical Computing Project 2.3.1 What is R? R is an open-source language for statistical computing. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland, in New Zealand, as a way to offer their students an accessible, no-cost tool for their courses. R is now maintained by the R Development Core Team, and developed by hundreds of contributors around the globe. R is an attractive alternative to other software applications for data analysis (e.g., Microsoft Excel, STATA) due to its open-source character (i.e., it is free), its flexibility, and large and dedicated user community, which means if there’s something you want to do (for instance, linear regression), it is very likely that someone has already developed functionality for it in R. A good way to think about R is as a core package, to which a library, consisting of additional packages, can be attached to increase its functionality. R can be downloaded for free at: https://cran.rstudio.com/ R comes with a built-in console (a user graphical interface), but better alternatives to the basic interface exist, including RStudio, an Integrated Development Environment, or IDE for short. RStudio can also be downloaded for free, by visiting the website: https://www.rstudio.com/products/rstudio/download/ R requires you to work using the command line, which is going to be unfamiliar to many of you accustomed to user-friendly graphical interfaces. Do not fear. People worked for a long time using the command line, or even more cumbersome, punched cards in early computers. Graphical user interfaces are convenient, but they have a major drawback, namely their inflexibility. A program that functions based on graphical user interfaces allows you to do only what is hard-coded in the user interface. Command line, as we will see, is somewhat more involved, but provides much more flexibility in operation. Go ahead. Install R and RStudio in your computer. Before introducing some basic functionality in R, lets quickly take a tour R Studio. 2.3.2 The RStudio IDE The RStudio IDE provides a complete interface to interact with the language R. It consists of a window with several panes. Some panes include in addition several tabs. There are the usual drop-down menus for common operations, such as creating new files, saving, common commands for editing, etc. See Figure 2.1 below. Figure 2.1: The RStudio IDE The editor pane allows you to open and work with text and other files, where you can write instructions that can be passed on to the program. Writing something in the editor does not execute any instructions, it merely records them for possible future use. In fact, much of what is written in the editor will not be instructions, but rather comments, discussion, and other text that is useful to understand code. The console pane is where instructions are passed on to the program. When an instruction is typed (or copied and pasted) there, R will understand that it needs to do something. The instructions must be written in a way that R understands, otherwise errors will occur. If you have typed instructions in the editor, you can use “ctrl-Enter” (in Windows) or “cmd-Enter” (in Mac) to send to the console and execute. The environment is where all data that is currently in memory is reported. The History tab acts like a log: it keeps track of the instructions that have been executed in the console. The last pane includes a number of useful tabs. The File tab allows you to navigate your computer, change the working directory, see what files are where, and so on. The Plot tab is where plots are rendered, when instructions require R to do so. The Packages tab allows you to manage packages, which as mentioned above, are pieces of code that can augment the functionality of R. The Help tab is where you can consult the documentation for functions/packages/see examples, and so on. The Viewer tab is for displaying local web content, for instance, to preview a Notebook (more on Notebooks soon). This brief introduction should have allowed you to install both R and RStudio. The next thing that you will need is packages. 2.4 Packages in R Packages are the basic units of reproducible code in the R multiverse. Packages allow a developer to create a self-contained unit of code that often is meant to achieve some task. For instance, there are packages in R that specialize in statistical techniques, such as cluster analysis, visualization, or data manipulation. Some packages can be miscellaneous tools, or contain mostly datasets. Packages are a very convenient way of maintaining code, data, and documentation, and also of sharing all these resources. Packages can be obtained from different sources (including making them!). One of the reasons why R has become so successful is the relative facility with which packages can be distributed. A package that I use frequently is called tidyverse. The tidyverse is a collection of functions for data manipulation, analysis, and visualization. This package can be downloaded and installed in your personal library of R packages by using the function install.packages, as follows: install.packages(&quot;tidyverse&quot;) The function install.packages retrieves packages from the Comprehensive R Archive Network, or CRAN for short. CRAN is a collection of sites (accessible via the internet) that carry identical materials for distribution for R. "],
["chapter-1.html", "Chapter 3 Data and stuff 3.1 What are models? 3.2 How to use this note 3.3 Learning objectives 3.4 Suggested readings 3.5 Ways of measuring stuff 3.6 Importing data 3.7 Data Classes in R 3.8 More on indexing and data manipulation 3.9 Visualization 3.10 Exercise", " Chapter 3 Data and stuff “Essentially, all models are wrong, but some are useful.” — George E.P. Box “You can have data without information, but you cannot have information without data.” — Daniel Keys Moran 3.1 What are models? Model building requires three things: Raw materials. Tools. Technical expertise (hopefully!). This is true whether the model is physical (for instance a sculpture), conceptual (a mental map), or statistical/mathematical (the gravity model or a regression model). In the case of a sculpture, the raw materials can be marble, wood, or clay; the tools chisels, mallet, and spatula; and the technique the mastery of the sculptor when working with the tools and the materials. Anyone can try sculpture, and most people can create sculptures. These kind of models are evaluated by their aesthetic value, not necessarily their usefulness. But if the scultpure is poorly balanced and falls and breaks, then its value is limited by its structural integrity - the skill of the sculptor matters even if only in this sense. In the case of a mental map, the raw materials are ideas, the tools are a drawing surface and tools for writing, or maybe an app, and the technical expertise is the ability of the modeler to organize ideas in a useful way. There are useful conceptual models, and conceptual models that are anything but. Figure 3.1 shows two example of conceptual models. Figure 3.1: Two Examples of Conceptual Models In the case of mathematical/statistical models, the raw materials are data; the tools are descriptive statistics and statistical plots, and various forms of regression analysis; and the technical expertise is the ability of the modeler to select tools that are appropriate to the data, and to convince the data to “speak”: in other words, to extract information from the data. As Moran said in the aphorism quoted at the top of this section: you can have data without information, but no information without data. Technical mastery is the degree to which a modeller can obtain information from data that is useful, accurate, and precise, to the extent that the raw materials permit. Before moving on to the technical skills required for modeling, it is important to understand the raw materials and the tools. The objective of this note is to introduce some important concerning data and data manipulation, and some useful tools. 3.2 How to use this note The source for the document you are reading is an R Notebook. Notebooks are a form “literate programming”, a style of document that uses code to illustrate a discussion, as opposed to the traditional programming style that uses natural language to discuss/document the code. It flips around the usual technical writing approach to make it more intuitive and accessible. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Hello, Discrete Choice Analysis!&quot;) ## [1] &quot;Hello, Discrete Choice Analysis!&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. Whichever way you are working, you might want to give it a try now! You will see that the chunk of code above instructed R (and trough R the computer) to print (or display on the screen) some text. 3.3 Learning objectives In this practice, you will learn about: Different ways to measure stuff. Basic operations in R. Data classes, data types, and data transformations. The use of packages in R. Basic visualization. 3.4 Suggested readings Grolemund, G., Wickham, H. (2016) R for Data Science, Chapters 3-5, O’Reilly Media. 3.5 Ways of measuring stuff Previously we said that data are the raw material for modeling, but we did not say precisely what we meant by ‘data’. You probably already have a working understanding of what ‘data’ means, but nonetheless lets begin with a definition. According to Mirriam-Webster, data are: factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation As an aside, it is interesting to note that Tukey’s classic Exploratory Data Analysis (Tuckey 1977) does not define ‘data’ in the glossary! Measurement theory is a branch of mathematics concerned with observing the facts about something. It is important to note that measurements are not the same as the thing being measured; however, we would like the measurements to be a reasonably close approximation of the thing being measured - otherwise the measurements might be pretty useless, an inadequate way to learn anything valuable from the thing we are measuring. One fundamental contribution of the scientific method has been to produce standardized ways of measuring things. How would you measure the following things? The temperature at which water freezes. The tempreature at which nitrogen freezes. The length of a trip. Blood donations. Different brands of peanut butter. The value of two bedroom apartment. Someone’s opinion regarding taxes. Generally, there are multiple ways of measuring something, but not all of them are necessarily appropriate, partly because the scales of measurement may result in some loss of information. The interpretation of a measurement, as well, depends on what the scale is. Two broad scales of measurement are as follows: 3.5.1 Categorical Categorical measurements assign a label or category to the thing being measured. For example, a way to measure different brands of peanut butter could be to measure their sugar content, their fat content, their consistency, and so on, and in this way describe what makes each brand unique. A different way to do this would be to label one brand “Spooky” and another “Peter’s”. This has the effect of reducing a lot of information to a much simpler category. Is this loss of information inappropriate? Well, it really depends on what is the intended use of data! Categorical measurements are interesting because they may tell us something about the power of brands! Within the class of categorical variables there are two distinct scales of measurement: Nominal scale. When the categories do not follow a natural order. For example, there is no reason to say that “Spooky” brand precedes “Peter’s” or vice versa. Similarly, when measuring modes of travel “walking” is not intrinsically higher or lower or better or worse than “cycling” or “riding bus”. Ordinal scale. When the categories follow a natural or logical sequence. A common way of measuring opinions is by means of the Likert scale, which classifies responses for instance as “strongly disagree”, “disagree”, “neutral”, “agree”, “strongly agree”. In this case, it is sensible to order the responses, since “strongly agree” is probably closer to “agree” than to “strongly disagree”. Responses of this type are often represented by numbers, say, from 1 to 5. It is a mistake to treat the measurements as numbers instead of lables. When treated as numbers there is a temptation to thing of the difference between 4 and 5 and the difference between 3 and 4 as being equivalent, when in fact the strength of disagreement could be stronger than the strength of agreement. In other words, the interval between “strongly disagree” and “disagree” may not be the same as “agree” and “strongly agree”. With ordinal scales we do not know that, all that we know is that they measure a different opinion. Sometimes, different measurement scales might represent different behavioral mechanisms, as Bhat and Pulugurta discuss in their comparison of categorical and ordinal measurements for vehicle ownership (Bhat and Pulugurta 1998). 3.5.2 Quantitative Quantitative measurements assign a number to an attribute, and the number quantifies the presence of the attribute. Within this class of variables, there are also two ways of measuring things. Interval scale. A quantity can be assigned to an attribute, the values follow an order, and their differences can be computed and remain constant. Temperature is typically measured in interval scale. The difference between \\(10\\,^{\\circ}\\mathrm{C}\\) and \\(11\\,^{\\circ}\\mathrm{C}\\) is the same as the difference between \\(25\\,^{\\circ}\\mathrm{C}\\) and \\(26\\,^{\\circ}\\mathrm{C}\\). The intervals are meaningful. However, \\(0\\,^{\\circ}\\mathrm{C}\\) does not imply the absence of temperature! Which is why measurements in Celsius and Farenheit do not coincide at zero. The lack of a natural zero for these scales means that the ratios between two values are not meaningful: \\(4\\,^{\\circ}\\mathrm{C}\\) is not twice as hot as \\(2\\,^{\\circ}\\mathrm{C}\\), and \\(-12\\,^{\\circ}\\mathrm{C}\\) is not four times as cold as \\(-3\\,^{\\circ}\\mathrm{C}\\). Ratio scale. When there is an absolute value of zero to the thing being measured (to indicate absence!), attributes can be measured in a ratio scale. This combines the features of the previous scales of measurement: a number is esentially a label that follows a logical order and with differences that are meaningful. In addition to that, the ratios of variables are meaningful. For example, twenty dollars are twice as valuable as ten, and zero is the absence of value. Weight is a way of measuring mass, and zero is the absence of mass. Two hundred kilograms is twice as much as one hundred kilograms. It is important to understand the different scales of measurement to be able to choose the appropriate tools for each. More on this below. But first, lets bring some actual data to play with. 3.6 Importing data There are several different ways of importing data in R. For this example, we will use part of a dataset that was analyzed by Whalen et al. (Whalen, Páez, and Carrasco 2013). At the very beginning, it is good practice to clear the workspace, to ensure that there are no extraneous items there. The workspace is where objects reside in memory during a session with R. The function for removing variables from the workspace is rm(). Another useful function is `ls, which retrieves a list of things in the workspace. So essentially we are asking R to remove all things in the workspace: rm(list = ls()) Once that the workspace is empty, we can proceed to load a few packages that are useful. Packages are the basic units of reproducible code in the R multiverse. Packages allow a developer to create a self-contained unit of code that often is meant to achieve some task. For instance, there are packages in R that specialize in statistical techniques, such as cluster analysis, visualization, or data manipulation. Some packages can be miscellaneous tools, or contain mostly datasets. Packages are a very convenient way of maintaining code, data, and documentation, and also of sharing all these resources. Packages can be obtained from different sources (including making them!). One of the reasons why R has become so successful is the relative facility with which packages can be distributed. A package that I use frequently is called tidyverse. The tidyverse is a collection of functions for data manipulation, analysis, and visualization. This package can be downloaded and installed in your personal library of R packages by using the function install.packages, as follows: install.packages(&quot;tidyverse&quot;) The function install.packages retrieves packages from the Comprehensive R Archive Network, or CRAN for short. CRAN is a collection of sites (accessible via the internet) that carry identical materials for distribution for R. Installing a package is similar to acquiring a book for your library. The book is there, but if you want to use it, you need to bring it to your workspace, so to speak. The function for retrieving a package from the library is naturally enough library(). For the moment, we need the following packages. If you have not done so, take a moment to install them, as illustrated in the previous chunk. library(tidyverse) library(readr) #library(caret) library(mlogit) Ocassionally there are messages displayed when loading a package. These messages are informative (they ask you to cite them in a certain style) or may give you warnings, for instance that identically named functions exist in several packages. The function that we need to read the sample dataset is read_csv(), which is part of the tidyverse package. Note that you can name the value (or output) of a function by using &lt;-. In this case, we wish to read an external file, and assign the results to an object called mc_mode_choice: mc_mode_choice &lt;- read_csv(&quot;Commute Mac.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. It is possible to quickly examine the contents of the object by means of the function head(), which prints the top few rows of the object, if appropriate. For example: head(mc_mode_choice) ## # A tibble: 6 x 39 ## RespondentID choice avcycle avwalk avhsr avcar timecycle timewalk ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 566872636 3 0 1 1 0 6.21 21.3 ## 2 566873140 3 0 1 1 1 3.73 12.8 ## 3 566874266 3 0 0 1 1 100000 100000 ## 4 566874842 2 1 1 1 0 5.83 20 ## 5 566881170 2 1 1 1 0 5.83 20 ## 6 566907438 2 0 1 1 0 100000 10 ## # ... with 31 more variables: accesshsr &lt;dbl&gt;, waitingtimehsr &lt;dbl&gt;, ## # transfer &lt;dbl&gt;, timehsr &lt;dbl&gt;, timecar &lt;dbl&gt;, parking &lt;dbl&gt;, ## # vehind &lt;dbl&gt;, owncycle &lt;dbl&gt;, gender &lt;dbl&gt;, work &lt;dbl&gt;, visa &lt;dbl&gt;, ## # age &lt;dbl&gt;, solo &lt;dbl&gt;, shared &lt;dbl&gt;, family &lt;dbl&gt;, child &lt;dbl&gt;, ## # primary_caregiver &lt;dbl&gt;, LAT &lt;dbl&gt;, LONG &lt;dbl&gt;, DAUID &lt;dbl&gt;, ## # mhi &lt;dbl&gt;, dwell_den &lt;dbl&gt;, lum &lt;dbl&gt;, st_den &lt;dbl&gt;, inter_den &lt;dbl&gt;, ## # SF_P_ratio &lt;dbl&gt;, side_den &lt;dbl&gt;, Shelters_SD &lt;dbl&gt;, Shelters_D &lt;dbl&gt;, ## # Shelters_A &lt;dbl&gt;, Shelters_SA &lt;dbl&gt; Here we can see that what we just read is a table with several variables: an id, a variable called choice, some variables for time, etc. Hopefully, when reading data there is also a metadata file, a data dictionary or something that defines what the data are. For example, what does it mean for choice to be “3” or “1”? Was time measured in hours, seconds, minutes, or something else? These variables will be described below. Before that, however, we can use a different function to get further insights into the contents of the table by means of the summary() function: summary(mc_mode_choice) ## RespondentID choice avcycle avwalk ## Min. :566872636 Min. :1.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:567814188 1st Qu.:2.000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :568682048 Median :2.000 Median :0.0000 Median :1.0000 ## Mean :570566454 Mean :2.618 Mean :0.2747 Mean :0.6613 ## 3rd Qu.:574925212 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :587675235 Max. :4.000 Max. :1.0000 Max. :1.0000 ## avhsr avcar timecycle timewalk ## Min. :0.0000 Min. :0.0000 Min. : 0.29 Min. : 1.00 ## 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.: 3.79 1st Qu.: 13.66 ## Median :1.0000 Median :1.0000 Median : 5.83 Median : 20.00 ## Mean :0.9608 Mean :0.5472 Mean : 34014.86 Mean : 37364.73 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:100000.00 3rd Qu.:100000.00 ## Max. :1.0000 Max. :1.0000 Max. :100000.00 Max. :100000.00 ## accesshsr waitingtimehsr transfer timehsr ## Min. : 0.00 Min. : 0.00 Min. : 0 Min. : 1.00 ## 1st Qu.: 2.48 1st Qu.:10.23 1st Qu.: 0 1st Qu.: 4.75 ## Median : 6.21 Median :10.23 Median : 0 Median : 10.00 ## Mean :11.06 Mean :10.25 Mean : 3925 Mean : 3940.57 ## 3rd Qu.:12.42 3rd Qu.:10.23 3rd Qu.: 1 3rd Qu.: 25.00 ## Max. :62.11 Max. :50.00 Max. :100000 Max. :100000.00 ## timecar parking vehind owncycle ## Min. : 1 Min. :0.00000 Min. :0.0000 Min. :0.0000 ## 1st Qu.: 8 1st Qu.:0.00000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median : 30 Median :0.00000 Median :0.0000 Median :0.0000 ## Mean : 45283 Mean :0.08358 Mean :0.2565 Mean :0.4513 ## 3rd Qu.:100000 3rd Qu.:0.00000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :100000 Max. :1.00000 Max. :1.0000 Max. :1.0000 ## gender work visa age ## Min. :0.0000 Min. :0.000 Min. :0.0000 Min. :17.00 ## 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.:1.0000 1st Qu.:20.00 ## Median :0.0000 Median :0.000 Median :1.0000 Median :21.00 ## Mean :0.4012 Mean :0.492 Mean :0.9622 Mean :22.08 ## 3rd Qu.:1.0000 3rd Qu.:1.000 3rd Qu.:1.0000 3rd Qu.:23.00 ## Max. :1.0000 Max. :1.000 Max. :1.0000 Max. :60.00 ## solo shared family child ## Min. :0.0000 Min. :0.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :0.0000 Median :1.000 Median :0.0000 Median :0.0000 ## Mean :0.1272 Mean :0.625 Mean :0.2478 Mean :0.2115 ## 3rd Qu.:0.0000 3rd Qu.:1.000 3rd Qu.:0.0000 3rd Qu.:0.0000 ## Max. :1.0000 Max. :1.000 Max. :1.0000 Max. :1.0000 ## primary_caregiver LAT LONG DAUID ## Min. : 0 Min. :43.08 Min. :-80.09 Min. :35250031 ## 1st Qu.:100000 1st Qu.:43.25 1st Qu.:-79.92 1st Qu.:35250540 ## Median :100000 Median :43.26 Median :-79.91 Median :35250670 ## Mean : 75218 Mean :43.25 Mean :-79.90 Mean :35250612 ## 3rd Qu.:100000 3rd Qu.:43.26 3rd Qu.:-79.90 3rd Qu.:35250677 ## Max. :100000 Max. :43.28 Max. :-79.64 Max. :35250970 ## mhi dwell_den lum st_den ## Min. : 0.000 Min. : 0.0 Min. :0.0000 Min. : 0.00 ## 1st Qu.: 4.577 1st Qu.: 488.7 1st Qu.:0.2808 1st Qu.:10.36 ## Median : 5.491 Median : 950.0 Median :0.4501 Median :14.29 ## Mean : 6.168 Mean : 1373.0 Mean :0.4183 Mean :13.27 ## 3rd Qu.: 7.556 3rd Qu.: 1688.6 3rd Qu.:0.5038 3rd Qu.:16.18 ## Max. :14.595 Max. :45209.9 Max. :0.9081 Max. :25.22 ## inter_den SF_P_ratio side_den Shelters_SD ## Min. : 0.00 Min. :0.0000 Min. : 0.00 Min. :0.00000 ## 1st Qu.: 25.82 1st Qu.:0.2309 1st Qu.:18.19 1st Qu.:0.00000 ## Median : 41.04 Median :0.2709 Median :22.63 Median :0.00000 ## Mean : 52.09 Mean :0.2625 Mean :24.18 Mean :0.04433 ## 3rd Qu.: 73.08 3rd Qu.:0.3134 3rd Qu.:35.70 3rd Qu.:0.00000 ## Max. :645.86 Max. :0.8808 Max. :59.41 Max. :1.00000 ## Shelters_D Shelters_A Shelters_SA ## Min. :0.0000 Min. :0.0000 Min. :0.00000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.00000 ## Median :0.0000 Median :0.0000 Median :0.00000 ## Mean :0.2289 Mean :0.3576 Mean :0.02762 ## 3rd Qu.:0.0000 3rd Qu.:1.0000 3rd Qu.:0.00000 ## Max. :1.0000 Max. :1.0000 Max. :1.00000 This function will print a set of summary statistics for the variables in the table. The statistics are appropriate for the scale of measurement of the variable - that is, the way the variable is coded. Presently, all summary statistics are calculated for quantitative variables. We don’t know if this makes sense, until we know what the variables are supposed to measure. For example, the variable choice measures the use of one mode of transportation. There are four values in this scale: 1 through 4, with each indicating one of “Cycle”, “Walk”, “Car”, or “HSR” (the local transit agency in Hamilton, ON, Canada, where these data were collected). Check the results of the summary. What does it mean to say that the mean of choice is 2.618? Does this number make sense? 3.7 Data Classes in R To understand why 2.618 of mode of transportation is not an appropriate summary measure for the variable mode, we need to know that R can work with different data classes, which include the following: Numerical Character Logical Factor The ability to store information in different forms is important, because is allows R to distinguish what kind of operations are appropriate for a certain variable. Consider the following example (using indexing): mc_mode_choice$choice[1] - mc_mode_choice$choice[4] ## [1] 1 Lets unpack what the chunk above did. First, we call our table mc_mode_choice. The string sign $ is used to reference columns in the table. Therefore, we asked R to go and look up the column choice in the table mc_mode_choice. Finally, the value between square brackets [] asks R to retrieve a value out of the column, in this example the first value in that column and then the fourth value. This system of referring to elements in tables is called indexing. Most computer languages use it, but the syntax is different. Again: $ refers to a column, and [] is used to call values in that column. As we can see, the difference between the two values retrieved is \\(1\\). But what is the meaning of “cycle” minus “walk”, for instance? In reality, the variable choice was measured as a nominal variable: it just corresponds to a label indicating what mode was chosen by a respondent. But R does not know this. Before R can treat it as a nominal variable, the numbers need to be converted to a factor. Factors are the way R stores categorical variables (both nominal and ordinal). To convert the variable choice to a factor, we use the factor() function: mc_mode_choice$choice &lt;- factor(mc_mode_choice$choice, labels = c(&quot;Cycle&quot;, &quot;Walk&quot;, &quot;HSR&quot;, &quot;Car&quot;)) In the chunk above, we ask R to replace the contents of mc_mode_choice$choice with the value (output) of the function factor. Factor takes the contents of mc_mode_choice$choice and converts to a factor with labels as indicated by the argument labels = (the function c() is used to concatenate several values). Lets summarize the result, by using the summary() function but only for this variable: summary(mc_mode_choice$choice) ## Cycle Walk HSR Car ## 48 711 336 281 Now the summary is appropriate categorical variable, and is a table of frequencies: as seen there, there were 48 respondents who chose “Cycle”, 711 who chose “Walk”, and so on. What if we tried to calculate the difference? mc_mode_choice$choice[1] - mc_mode_choice$choice[4] ## Warning in Ops.factor(mc_mode_choice$choice[1], mc_mode_choice$choice[4]): ## &#39;-&#39; not meaningful for factors ## [1] NA The message indicates that the operation we tried to perform is not meaningful for factors. As long as R knows the appropirate measurement scale for your variables, it will try to steer you away from doing silly things with them. Other variables included in this table are for time. These variables measure the duration in minutes (actual or imputed) for trips by different modes. For example, timecycle is the duration of a trip by bicycle for the journey reported by the respondent. Lets summarize this variable again: summary(mc_mode_choice$timecycle) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.29 3.79 5.83 34014.86 100000.00 100000.00 Notice that the shortest trip by bicycle would be less than a minute long, whereas the maximum is \\(100,000\\) minutes long. Wait, what? That is over \\(1,600\\) hours long. Is that even possible? In fact, no, it is not. The reason for these values is that when the original data were coded, whenever a respondent said that cycling was not a mode that was available to them, the time was coded as a very large and distinctive value. There were no trips taking \\(100,000\\) minutes, this is just a code for “information not available”. One problem with this manner of coding is that R does not know that the information is actually missing, but rather thinks it is a legitimate quantity. As a consequence, the mean is tens of thousands of minutes, despite the fact that half of all trips by bicycle were measured at less than 6 minutes long (see the median). Next we will see a way to address this. One last thing before doing so: you can check the class an object with the function class class(mc_mode_choice$choice) ## [1] &quot;factor&quot; class(mc_mode_choice$timecycle) ## [1] &quot;numeric&quot; 3.8 More on indexing and data manipulation Indexing is a way of making reference to elements in a data object. There are numerous indexing methods in R that are appropriate for specific objects. Tables such as mc_mode_choice (called data frames) can be indexed in a few different ways. For example the next three chunks are equivalent in that they call the second column (choice) and in that column the second element: mc_mode_choice[2, 2] ## # A tibble: 1 x 1 ## choice ## &lt;fct&gt; ## 1 HSR mc_mode_choice$choice[2] ## [1] HSR ## Levels: Cycle Walk HSR Car mc_mode_choice[[&quot;choice&quot;]][2] ## [1] HSR ## Levels: Cycle Walk HSR Car It is also possible to index by ranges of values. For example, the next chunks retrieves rows 2 to 5 from columns 7 and 8: mc_mode_choice[2:5, 7:8] ## # A tibble: 4 x 2 ## timecycle timewalk ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.73 12.8 ## 2 100000 100000 ## 3 5.83 20 ## 4 5.83 20 Indexing is useful to subset data selectively. For example, we know that travel times coded as \\(100,000\\) are actually cases where the corresponding mode was not available. Lets say that we wanted to summarize travel time by bicycle but without those cases. We can use logical statements when indexing. We could tell R to retrieve only those values that meet a certain condition. In the next chunk, we save the results of this to a new variable: time.Cycle.clean &lt;- mc_mode_choice$timecycle[mc_mode_choice$timecycle != 100000] where != is R for not. In other words, “find all values not 100000, and retrieve them”. The result of this is a numeric object: class(time.Cycle.clean) ## [1] &quot;numeric&quot; If we summarize this object now: summary(time.Cycle.clean) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2914 2.9141 4.3711 4.8957 5.8282 45.0000 The summary statistics are much more sensible: the longest trip by bicycle was measured at 45 minutes, and the mean trip at less than 5 minutes. Indexing is a powerful technique, but can be cumbersome (mc_mode_choice$timecycle[mc_mode_choice$timecycle != 100000]!). The package dplyer (part of the tidyverse) provides a grammar for data manipulation that is more intuitive. We will explore three of its elements here, namely the pipe operator (%&gt;%), select, and filter. Suppose that we wanted to select two of the time variables, for cycling and walking, and wanted to retrieve only values other than the offending \\(100,000\\), and save these values in a new object called time.Active.clean. In the grammar of dplyr, this is done as follows: time.Active.clean &lt;- mc_mode_choice %&gt;% select(c(&quot;timecycle&quot;, &quot;timewalk&quot;)) %&gt;% filter(timecycle != 100000 &amp; timewalk != 100000) In natural language this would be something like “take mc_mode_choice and select columns timecycle and timewalk; pass the result to filter and retrieve all rows that meet the conditions timecycle != 100000 AND timewalk != 100000”. The verb select is used to select columns from a data frame, and the verb filter to filter rows. The alternative, using indexing would look something like this: time.Active.clean.the.hard.way &lt;- mc_mode_choice[mc_mode_choice$timecycle != 100000 &amp; mc_mode_choice$timewalk != 100000, 7:8] The expression becomes more convoluted and not as easy to read. It is also easier to make mistakes when writing it. Compare the summaries of the two data frames, to make sure that they are identical: summary(time.Active.clean) ## timecycle timewalk ## Min. : 0.2914 Min. : 1.00 ## 1st Qu.: 2.9141 1st Qu.:10.00 ## Median : 4.3711 Median :15.00 ## Mean : 4.5852 Mean :16.10 ## 3rd Qu.: 5.8282 3rd Qu.:20.00 ## Max. :17.4845 Max. :62.11 summary(time.Active.clean.the.hard.way) ## timecycle timewalk ## Min. : 0.2914 Min. : 1.00 ## 1st Qu.: 2.9141 1st Qu.:10.00 ## Median : 4.3711 Median :15.00 ## Mean : 4.5852 Mean :16.10 ## 3rd Qu.: 5.8282 3rd Qu.:20.00 ## Max. :17.4845 Max. :62.11 The grammar of data manipulation in dplyr is a powerful way of working with data in an intuitive way. We will find other aspects of this, but for the time being you are welcome to consult more about dplyr here 3.9 Visualization The last item in this section is related to visualization. Humans are very much visual creatures, and much can be learned from seeing the data. For example, the data frame, in essence a table, is informative in many ways, but not particularly conducive to observe trends or regularities in the data. The summary statistics are also informative, but partial, and do not convey information to the same effect as a statistical plot. Take the following list of summary statistics: summary(time.Active.clean) ## timecycle timewalk ## Min. : 0.2914 Min. : 1.00 ## 1st Qu.: 2.9141 1st Qu.:10.00 ## Median : 4.3711 Median :15.00 ## Mean : 4.5852 Mean :16.10 ## 3rd Qu.: 5.8282 3rd Qu.:20.00 ## Max. :17.4845 Max. :62.11 Now, compare to the following plot: ggplot(data = time.Active.clean) + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) + geom_area(aes(x = timewalk), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;yellow&quot;, color = &quot;yellow&quot;, alpha = 0.6) The plot above was created using a package called ggplot2, also part of tidyverse. This package implements a grammar of graphics, and offers a very flexible way of creating plots in R. ggplot2 works by layering a series of objects, beginning with a blank plot, to which we can add things. The command to create a plot is ggplot(). This command accepts different arguments. For instance, we can pass data to it in the form of a data frame. We can also indicate different aesthetic values, that is, the things that we wish to plot. None of this is plotted, though, until we indicate which kind of geom or geometric object we wish to plot. For instance, you can see above that to create the figure we used geom_area. This geom is essentially a smoothed histogram. Lets break down these instructions. First we ask ggplot2 to create a plot that will use the data frame time.Active.clean. We will name this object p: p &lt;- ggplot(data = time.Active.clean) Notice how ggplot2 creates a blank plot, but it has yet to actually render any of the population information in there: p We have yet to tell ggplot2 what the x axis is, what the y axis is, what should be plotted in the x axis, and so on. We layer elements on a plot by using the + sign. It is only when we tell the package to add some geometric element that it renders something on the plot. In the previous case, we told ggplot2 to use the geom_area to create a smoothed histogram. Next, we need to indicate which aes (short for aesthetics) we wish to plot. The aesthetics map aspect of the dataset to the plot. For instance, by saying that the aesthetics include something for x, we tell ggplot2 that that something should be mapped to the x axis (in this case one of the time variables in the data frame). The second argument is stat = 'bin', which indicates that there is a statistical operation that happens, namely the data are binned for smoothing (small bins lead to less smoothing, large bins lead to more smoothing; try it!). After playing around with a few bin values, I selected 5: p + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5) To improve the appearence of the plot, we also asked that the geom be rendered using a named color (blue for the color of the line, and also blue for the fill), and that it be transparent (the argument alpha controls opacity; a value of zero is transparent, a value of 1 is solid): p + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) The final plot was obtained by layering a second geom (in yellow) for a different variable (time by walking), so that we could compare them: ggplot(data = time.Active.clean) + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) + geom_area(aes(x = timewalk), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;yellow&quot;, color = &quot;yellow&quot;, alpha = 0.6) Notice that the x axis is labeled as “timecycle” despite the fact that the plot also includes time by walking. This can be fixed by changing the label as follows: ggplot(data = time.Active.clean) + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) + geom_area(aes(x = timewalk), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;yellow&quot;, color = &quot;yellow&quot;, alpha = 0.6) + xlab(&quot;Time (in minutes)&quot;) What do we learn from this plot? Would it have been possible to learn the same from the summary statistics? Which was more effective, the plot or the summary statistics? The plot above is an example of a univariate plot, since it is created to display the distribution of a single variable, not the way two or more variables relate. Imagine now that you would like to see how mode choice and sidewalk density at the place of residence relate. An appropriate statistical plot for two variables, one of which is nominal (choice) and another that is continuous (side_den), is the boxplot. Before creating the plot, lets summarize these two variables (notice the use of the pipe operator): mc_mode_choice %&gt;% select(c(&quot;choice&quot;, &quot;side_den&quot;)) %&gt;% summary() ## choice side_den ## Cycle: 48 Min. : 0.00 ## Walk :711 1st Qu.:18.19 ## HSR :336 Median :22.63 ## Car :281 Mean :24.18 ## 3rd Qu.:35.70 ## Max. :59.41 Sidewalk density is measured in \\(km/km^2\\). Lets create the boxplot next. We begin by defining a ggplot2 object with the data frame and aesthetics that we wish to use. In this case, we want to plot the categorical variable in the x axis and the quantitative variable in the y axis: ggplot(data = mc_mode_choice, aes(x = choice, y = side_den)) + geom_boxplot() What do we learn from this plot? Could we have derived a similar insight from the summary statistics? There are many different geoms that can be used in ggplot2. You can always consult the help/tutorial files by typing ??ggplot2 in the console. See: ??ggplot2 You can also check the ggplot2 Cheat Sheet for more information on how to use this package. A last note. Many other visualization alternatives (for instance, Excel) provide point-and-click functions for creating plots. In contrast, ggplot2 in R requires that the plot be created by meticulously instructing the package what to do. While this is more laborious, it also means that you have complete control over the creation of plots, which in turn allows you to create more flexible and creative visuals. 3.10 Exercise 3.10.1 Questions Define “model”. Why are models on a 1-to-1 scale undesirable? Invoke dataset Mode from package mlogit. To do this you need to first load the package. Once you have done so, usage of datasets is as follows: data(&quot;Mode&quot;) This is a dataset with choices about mode of transportation. Describe this dataset. How many variables are there and of which type (i.e., categorical/quantitative)? How many different modes of transportation are in this data set? What is the most popular mode? What is the least popular mode? In general, what is the most expensive mode? The least expensive? Create a plot showing the univariate distributions of time by car and time by bus. Discuss. How do choices relate to cost by the different modes? References "],
["chapter-2.html", "Chapter 4 Fundamental concepts 4.1 Why modelling choices? 4.2 How to use this note 4.3 Learning objectives 4.4 Suggested readings 4.5 Preliminaries 4.6 Utility maximization 4.7 What about those random terms? 4.8 Probability distribution functions (PDFs) 4.9 A simple random utility discrete choice model 4.10 Other choice mechanisms 4.11 Exercise", " Chapter 4 Fundamental concepts “Ignorance gives one a large range of probabilities.” — George Eliot 4.1 Why modelling choices? In Chapter 3 we discussed in a general fashion the use of models. There, we argued that modelling is an activity that helps us to isolate in a systematic way certain aspects of a process or thing, by way of abstraction and generalization. There are many kinds of models: analog (like sculptures, maquettes, scale models), conceptual (like mental maps), and mathematical/statistical models. The raw materials of mathematical/statistical models are observations about the process or thing of interest, usually measurements that provide data. The tools are the statistical and mathematical techniques used to convert data into information. And the technical expertise is the knowledge and ability of the modeler to use the appropriate tools to the data, in order to extract as much information as possible, given the characteristics of the data and the process or thing. Modelling choices is simply a specialized field in the much broader field of mathematical and statistical modelling. The task of modelling choices is in many way similar to the modelling of limited-dependent and qualitative variables in statistics (Maddala 1983), however it is distinguished from models in that field by a strong behavioral foundations. Indeed, where statistical models deal with probabilities of an item of interest being in a certain state, choice modelling deals with the probability of an agent choosing an alternative. This is a subtle but important difference that we will highlight in due course. For the time being, it is sufficient to say that to model choices we need a conceptual model first on which to build the rest of the apparatus required for applied choice modelling. Before delving into the technical details, we can pause for a philosophical moment to think about human behavior and decision-making. There are different perspectives on human behavior. Some schools of thought affirm that events are predetermined. A famous thought experiment Laplace’s Demon, is as follows: We may regard the present state of the universe as the effect of its past and the cause of its future. An intellect [Laplace’s Deman] which at a certain moment would know all forces that set nature in motion, and all positions of all items of which nature is composed, if this intellect were also vast enough to submit these data to analysis, it would embrace in a single formula the movements of the greatest bodies of the universe and those of the tiniest atom; for such an intellect nothing would be uncertain and the future just like the past would be present before its eyes. — Pierre Simon Laplace, A Philosophical Essay on Probabilities Some schools of sociological thought (see for example the discussion in Degenne and Fors‚ 1999) see social interactions as a predominant, and even a determinant, factor that affects behavior. In their more extreme form, structuralism views social networks as structures that limit the ability of the individual to exercise independent agency, and therefore determine behavior. Laplace’s Demon and other forms of causal determinism assume that all preceding events set the conditions for present and future events via immutable rules. Nowadays determinism is not seriously considered for several reasons, of which it is useful to highlight two: The practical impossibility of knowing at a certain moment all forces that set nature in motion, as well as the positions of all of nature’s items. With respect to physical processes, the uncertainty principle of quantum physics put paid to the notion that we can know all that there is to know about the fundamental items of nature. In terms of human behavior, this is complicated by the inability of an external observer to know the state of mind of a person who acts. On the other hand, it is possible that existing social structures influence behavior (and there is now a wealth of literature that makes this argument; see A. Páez and Scott 2007). However, social determinism seems as implausible as physical determinism, for similar reasons: the difficulties of knowing the state of a system with complete omniscience. The assumption of immutable rules. This assumption has been challenged by studies that suggest some important physical constants can change with the age of the universe (Webb et al. 2001). In terms of human behavior, the assumption is even more problematic, if for no other reason that humans can in general act in a contrarian way simply to demonstrate that there are no immutable social rules. This is only one of many reasons why behavioral detection (for instance, in airports; Kirschenbaum 2013) is problematic: if one knows the rules used for profiling, acting otherwise renders profiling ineffective. Does this mean that the state of the universe is not determined by the past? Not at all. For all we know, from the perspective of a hypothetical all-knowing being, it is. However, in practical terms, and for the reasons described briefly above, we humble non-all-knowing beigns, cannot rely on determinism for making statements about the state of the universe. In particular, we will make a distinction that is useful as part of developing a conceptual model of choice-making: 1) that there is an observer who typically lacks all relevant information about a choice process (let alone about the state of the universe); and 2) that the rules of decision making are not completely known and/or humans can, for idiosincratic reasons, alter them at whim. 4.2 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Well, hello there, Juan de Dios!&quot;) ## [1] &quot;Well, hello there, Juan de Dios!&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 4.3 Learning objectives In this practice, you will learn about: Choice mechanisms: Utility maximization. Probabilities and integration. How to derive a simple choice model. Other choice mechanisms. 4.4 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapter 3, MIT Press. Hensher, D.A., Rose, J.M., Greene, W.H (2005) Applied Choice Analysis: A Primer, Chapter 3, Cambridge University Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 7, John Wiley and Sons. 4.5 Preliminaries Load the packages used in this section: library(tidyverse) library(evd) 4.6 Utility maximization We will begin by defining a conceptual model of choice based on neo-classical economics, fundamentally consumer choice. The conceptual framework in neo-classical economics is based on the concept of utility. What is utility? In simple terms, utility is a summary indicator of the pleasure, usefulness, enjoyment, or attractiveness associated with making a choice (for instance, buying a new phone). Lets begin by positing a very simple choice situation, in which a decision maker chooses between one of two different alternatives. These alternatives constitute the choice set, and provide the context for the decision-making process. Imagine then that this simple choice example is as follows: Alternative 1: Do nothing (keep using current phone) Alternative 2: Buy new phone (for simplicity, think of a generic model). Further, assume that each alternative can be described by means of a vector of attributes \\(X\\) as follows: \\[ X = [x_1, x_2, \\dots, x_k] \\] The attributes describe each alternative in a way that is relevant to the decision maker. In the present example, two relevant attributes are the cost of each alternative and the characteristics of the current and new phones, for instance, their download speeds. In this way, the two choices can be described by their attributes as follows: \\[ \\begin{array}{cc} \\text{Do-Nothing:} &amp; X_A = [\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{New-Phone}}]\\\\ \\text{New-Phone:} &amp; X_B = [\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}]\\\\ \\end{array} \\] If the decision-maker currently owns a phone that is fully paid, the out-of-pocket cost of doing nothing would be zero. Buying a new phone, on the other hand, would have a positive (and possibly substantial cost). The new phone, on the other hand is faster than the older, currently owned model. The decision-maker can likewise be described by a vector of attributes, say \\(Z\\): \\[ Z = [z_1, z_2, \\dots, z_k] \\] Suppose for example, that decision-maker \\(i\\) can be described in terms of their income, as follows: \\[ Z_i = [\\text{income}_i] \\] The attributes of the decision maker help to capture heterogeneities in behavior: for instance, a decision-maker with a lower income may be more sensitive to cost, since buying a new phone is relatively more expensive. A utility function is a way of summarizing the attributes of the choices and the attributes of the decision-makers in a single quantity, which is what the decision-maker is trying to maximize. We assume that each course of action gives this consumer a level of utility: in other words, he will be more or less happy with each alternative, taking into account their characteristics and his own condition or status: \\[ \\begin{array}{c} U_{i, \\text{Do-Nothing}} = U(\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{Do-Nothing}}, \\text{ income}_i)\\\\ U_{i, \\text{New-Phone}} = U(\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}, \\text{ income}_i)\\\\ \\end{array} \\] Notice that the utility function is specific to a decision maker \\(i\\) and an alternative. Here we define a decision-making rule. The decision-maker considers the utility of the alternatives, and chooses the one that gives the highest utility. In other words decision-maker \\(i\\) will choose to keep the current phone if: \\[ U_{i,\\text{Do-Nothing}} &gt; U_{i,\\text{New-Phone}}, \\] If the reverse is true, then the decision-maker will choose to buy a new phone (in the case of a tie, the decision-maker is indifferent between the two alternatives). We assume that decision-makers are rational and that they do an analysis of the costs and the benefits of each alternative before making the choice. The analyst, however, may fail to observe all aspects of the decision making process. For instance, a decision-maker may need faster speeds because she lacks internet at home. Or a decision-maker just received a large gift from a relative. Or younger people may be more willing to buy new phones than older people. The analyst may observe a decision-maker with a relatively low income buying a new phone. While income alone would have suggested that the decision-maker would be better off keeping the old phone, the analyst has no way of knowing the idiosyncratic factor of the gift. For this reason, it is convenient to decompose the utility into 1) a systematic component, that is, the part that explains the decision-makers’ response to the attributes of the alternative; and 2) a random component, which captures other aspects of the decision making process that the analyst did not observe: \\[ \\begin{array}{c} U_A = V_A + \\epsilon_A\\\\ U_B = V_B + \\epsilon_B\\\\ \\end{array} \\] The random part of the function is called the random utility. If there was no uncertainty at all, if we knew precisely all there is to know about the decision-making process, we would have that \\(\\epsilon_{\\text{Do-Nothing}}\\) and \\(\\epsilon_{\\text{New-Phone}}\\). Accordingly, \\(U_{\\text{Do-Nothing}} = V_{\\text{Do-Nothing}}\\) and \\(U_{\\text{New-Phone}} = V_{\\text{New-Phone}}\\), and we could predict with complete certainty the choice. However, the presence of the random components means that we cannot be certain whether \\(U_A &gt; U_B\\). While this is unfortunate, the presence of the random terms does allow us to make a probabilistic statement, such as: \\[ P_{\\text{Do-Nothing}} = P(U_{\\text{Do-Nothing}} &gt; U_{\\text{New-Phone}}) \\] In other words, the probability of doing nothing equals the probability that the utility of doing nothing is greater than the utility of buying a new phone. After rearranging things, this is equivalent to: \\[ P_{\\text{Do-Nothing}} = P(V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} &lt; \\epsilon_{\\text{New-Phone}} - \\epsilon_{\\text{Do-Nothing}}) \\] The expression above is the foundation of random utility modelling. Before we make more progress, however, we have to answer an important question. 4.7 What about those random terms? library(tidyverse) library(evd) A probabilistic expression is clearly better than being unable to say anything at all regarding choices. To make this expression of practical use, we must assume some distribution for the random terms. Which means that we need to define some probability distribution function. 4.8 Probability distribution functions (PDFs) A candidate for a probability distribution function is any function that satisfies the following two conditions: \\[ \\begin{array}{l} \\text{Condition 1: }f(x)\\ge 0\\text{ for all }x\\\\ \\text{Condition 2: }\\int_{-\\infty}^{\\infty}f(x)dx=1\\\\ \\end{array} \\] These two conditions say that the function must take values of at least zero for the interval of \\(x\\) of interest, and that the area under the curve (that is what the integral means) must equal 1. Lets use an example to illustrate these properties. We will define the following function: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le -L \\\\ \\frac{1}{2L} &amp; \\quad -L&gt; x &gt; L \\\\ 0 &amp; \\quad x \\ge L \\\\ \\end{array} \\right. \\] This function is shown in Figure 4.1 below, with \\(L=2\\): # Define a function uniform &lt;- function(x, L) ifelse( x &lt;= -L, 0, ifelse( x &gt; -L &amp; x &lt;= L, 1/(2 * L), 0 )) # Define parameter L for the distribution L &lt;- 2 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -(L+1), to = L+1, by = 0.01)) %&gt;% mutate(y = uniform(x, L)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 4.1: Uniform distribution It is easy to see that the value of the function is always equal to or greater than zero. You can also verify that the area under the curve in this case is simply the area of the rectangle \\(b \\times h\\), where the base of the rectangle is \\(b = L - (-L)\\) and the height is \\(h=\\frac{1}{2L}\\): (L - (-L)) / (2 * L) ## [1] 1 If you are working with the R Notebook file, try changing the value of the parameter \\(L\\) to see what happens! What is the implication of larger values of L? And of smaller values of L? Since the function above satisfies the two necessary conditions, we conclude that it is a valid probability distribution function. In fact, it turns out to be a form of the uniform probability distribution function, which more generally is defined as: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le b \\\\ \\frac{1}{a - b} &amp; \\quad b&gt; x &gt; a \\\\ 0 &amp; \\quad x \\ge a \\\\ \\end{array} \\right. \\] Given a probability distribution function, we can calculate the probability of a random variable \\(x\\) being contained in a defined interval. For instance, the probability of \\(x &lt; -L\\) is zero, since the area under the curve in that case is zero. The probability of \\(x \\le X\\) is: \\[ \\int_{-\\infty}^{X}f(x)dx \\] In the case of our uniform distribution function, this is simply the area of the rectangle defined by the limits of the integral: # Define L L &lt;- 2 # Define an upper limit for calculating the probability X &lt;- 0 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -(L+1), to = L+1, by = 0.01)) %&gt;% mutate(y = uniform(x, L)) df_p &lt;- data.frame(x =seq(from = -(L+1), to = X, by = 0.01)) %&gt;% mutate(y = uniform(x, L)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + # Plot distribution function geom_area(data = df_p, fill = &quot;orange&quot;, alpha = 1) + # Plot area under the curve ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis What is the probability that \\(x \\le 0\\)? Try changing the upper limit to see what happens. How does the value of the area under the curve change? Associated to a probability distribution function we can define a cumulative distribution function \\(F_X(x) = P(x \\le X)\\), which maps how the probability changes as we change the interval. The cumulative distribution function of our uniform distribution is as follows: \\[ F(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le -L \\\\ \\frac{x + L}{2L} &amp; \\quad -L&gt; x &gt; L \\\\ 1 &amp; \\quad x \\ge L \\\\ \\end{array} \\right. \\] The cumulative distribution function for our uniform distribution appears in Figure 4.2: # Define the cumulative distribution function cuniform &lt;- function(x, L) ifelse( x &lt;= -L, 0, ifelse( x &gt; -L &amp; x &lt;= L, (x + L)/(2 * L), 1 )) # Define L L &lt;- 2 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -(L+1), to = L+1, by = 0.01)) %&gt;% mutate(y = cuniform(x, L)) # Plot ggplot(data = df, aes(x, y)) + geom_step(color = &quot;orange&quot;) + # Plot cumulative distribution function ylim(c(0, 1)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;F(x)&quot;) # Label the y axis Figure 4.2: Uniform cumulative distribution function As you can see, the probability of \\(x \\le -L\\) is zero, the probability of \\(x \\le 0\\) is 0.5, and the probability of \\(x \\le L\\) is one. Lets consider a second example, with a function as follows: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le 0 \\\\ 2x &amp; \\quad 0 &gt; x &gt; 1 \\\\ 0 &amp; \\quad x \\ge 1 \\\\ \\end{array} \\right. \\] This function is shown in Figure 4.3. # Define a function linear &lt;- function(x) ifelse( x &lt;= 0, 0, ifelse( x &gt; 0 &amp; x &lt;= 1, 2 * x, 0 )) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = 0, to = 1, by = 0.01)) %&gt;% mutate(y = linear(x)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + ylim(c(0, 2)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 4.3: Cubic distribution Clearly, \\(f(x) \\ge 0\\) for all values of \\(x\\) in the interval \\(0 \\le x \\le 1\\). We can verify that the area under the curve is 1. In this case the area is that of a triangle, i.e., \\(\\frac{b \\times h}{2}\\). Since the base of the triange is \\(b=1\\) and the height is \\(h=2\\), we see that the area under the curve is 1. Since this is a valid probability distribution function, we can use it to calculate the probability of \\(x \\le X\\) as above. The area under the curve when \\(x \\le X\\) is given by: \\[ F(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le 0 \\\\ \\frac{x \\times 2x}{2} = x^2 &amp; \\quad 0 &gt; x &gt; 1 \\\\ 1 &amp; \\quad x \\ge 1 \\\\ \\end{array} \\right. \\] The plot of the cumulative distribution function in this case is shown in Figure 4.4. # Define a function clinear &lt;- function(x) ifelse( x &lt;= 0, 0, ifelse( x &gt; 0 &amp; x &lt;= 1, x^2, 1 )) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -0.2, to = 1.2, by = 0.001)) %&gt;% mutate(y = clinear(x)) # Plot ggplot(data = df, aes(x, y)) + geom_step(color = &quot;orange&quot;) + # Plot cumulative distribution function ylim(c(0, 1)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 4.4: Linear cumulative distribution function It should be clear from the examples above that calculating probabilities is nothing more than finding the area under the curve of a function. When the function is relatively simple, as the uniform or the linear distributions that we used for the examples, calculating the areas is also straightforward, since the functions describe simple geometric shapes. When the function is more involved, that becomes less straighforward. For example, consider the following function: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le 0 \\\\ 4x^3 &amp; \\quad 0 &gt; x &gt; 1 \\\\ 0 &amp; \\quad x \\ge 0 \\\\ \\end{array} \\right. \\] This function is plotted in Figure 4.5. # Define a function cubic &lt;- function(x) ifelse( x &lt;= 0, 0, ifelse( x &gt; 0 &amp; x &lt;= 1, 4 * x^3, 0 )) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = 0, to = 1, by = 0.01)) %&gt;% mutate(y = cubic(x)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + ylim(c(0, 4)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 4.5: Cubic distribution Unlike the rectangle of the uniform distribution and the triangle of the linear distribution, the area under the curve for this distribution needs to be obtained by integration as follows (do not worry if ): \\[ \\int_{0}^{1}4x^3dx =4\\Big[\\frac{x^4}{4} \\Big]_{0}^{1} = \\Big[x^4 \\Big]_{0}^{1} = 1^4-0 = 1 \\] This shows that the function is a valid probability distribution function. However, the integration makes things more interesting, to say the least! Fortunately, for most applied discrete choice analysis we do not need to solve integrals manually (the monster minds have already done this for us!). The key here is to remember: given a valid probability distribution function the probability that a random variable \\(x \\le X\\) is the area under the curve in the interval \\(-\\infty\\) to \\(X\\). 4.9 A simple random utility discrete choice model We are now ready to deploy a probability distribution function to the probability of choosing an alternative. Returning to our binary choice example, lets assume that the difference in the random utility terms follows the uniform distribution with parameters \\(-L\\) and \\(L\\), that is: \\[ \\epsilon_{\\text{New-Phone}} - \\epsilon_{\\text{Do-Nothing}} \\sim U(-L, L) \\] The probability of choosing the “do-nothing alternative” is: \\[ P_{\\text{Do-Nothing}} = P(V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} &lt; \\epsilon_{\\text{New-Phone}} - \\epsilon_{\\text{Do-Nothing}}) \\] Since we know the probabilities of a random variable being less than a certain value in the uniform distribution, we have that: \\[ P_{\\text{Do-Nothing}} = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} \\le -L \\\\ \\frac{V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} + L}{2L} &amp; \\quad -L&gt; V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} &gt; L \\\\ 1 &amp; \\quad V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} \\ge L \\\\ \\end{array} \\right. \\] Lets unpack this expression. When the systematic utility of a new phone is greater than the systematic utility of doing nothing, the difference between these two terms is negative. The more negative this value is, the lower the probability of doing nothing. When the difference is more negative than \\(-L\\), the probability of doing nothing becomes zero. When the systematic utility of a new phone is identical to the systematic utility of doing nothing, the difference between these two terms is zero, in which case the probability of doing nothing is \\(0.5\\). In other words, there is a 50% chance that the decision maker will do nothing. Finally, when the systematic utility of a new phone is less than the systematic utility of doing nothing, the difference between these two terms is positive. The more the more positive this value is, the higher the probability of doing nothing. When the difference is greater than \\(L\\), the probability of doing nothing becomes one. Now, since the choice set is an exhaustive collection of courses of action, it follows that the probability of the two courses of action must add up to one (the decision-maker does nothing OR buys a new phone): \\[ P_{\\text{Do-Nothing}} + P_{\\text{New-Phone}} = 1 \\] This implies that once we know the probability of doing nothing, the probability of buying a new phone is simply the complement: \\[ P_{\\text{New-Phone}} = 1 - P_{\\text{Do-Nothing}} \\] These probabilities are a discrete choice model. In fact, this is called the linear probability model (see Ben-Akiva and Lerman 1985, 66–68). Other models can be obtained by selecting different probability distribution functions, as we will see in later chapters. The procedure followed here will be the same, even if the probability distribution function selected for the model is different: given a valid probability distribution function, and given the systematic utilities of the alternatives, it is possible to evaluate the probabilistic statement associated with the choice of an alternative. One final note, before discussing other choice mechanisms. The simple example used here was for binary choice, i.e., for a situation with only two alternatives. This was done for convenience of exposition, and we will see how the same ideas generalize for situations with more than two alternatives, that is, for multinomial choice situations. 4.10 Other choice mechanisms Utility maximization is only one of several plausible mechanisms. The utility functions assume that trade-offs among different attributes are possible; for example, the way the utility functions were formulated assumes that a decision-maker is willing to pay more for higher download speeds. While such trade-offs are plausible in many situations, other choice mechanisms could exist in other cases. Ortuzar and Willumsen (2011, Fourth Edition:241–43). For example, a user who is shopping for smartphones may have low tolerance for download speeds below a certain threshold, or may have a budget limit that prevents her from considering certain models. Some alternatives from the choice set may be eliminated or ranked based on some dominant attribute. This kind of choice mechanism is called lexicographic choice, or elimination by attributes. Another plausible choice mechanism is a form of satisficing behavior. Again, a user shopping for a smartphone might find a model that does not maximize her utility, but that is otherwise satisfactory. For example, the decision-maker may consider that the additional time spent finding an even better model is not worth her while, so she stops her search at a suboptimal point. Another choice mechanism seen recently in the literature is regret-minimization (Chorus 2010). In addition to these various mechanisms, it is possible that a decision-maker deploys combinations of them: for example, lexicographic choice to reduce the number of alternatives in a choice set, followed by utility maximization or regret minimization. Despite progress on these models, utility maximization remains the most widely used approach for the analysis of discrete choices. 4.11 Exercise Answer the following questions. 4.11.1 Questions Define utility. Describe in your own words the behavior described by utility maximization. What conditions are necessary for a function to be a valid probability distribution function? Consider the function shown in Figure 4.6. This is called the triangle or tent function. Figure 4.6: Triangle (or tent) function Show that the triangle function in the figure is a valid probability distribution function. Next, consider the following utility functions for two alternatives, namely \\(i\\) and \\(j\\): \\[ \\begin{array}{c} U_i = V_i + \\epsilon_i\\\\ U_j = V_j + \\epsilon_j\\\\ \\end{array} \\] Assume that the difference between the error terms below follows the triangle distribution: \\[ \\epsilon_q = \\epsilon_i - \\epsilon_j \\] Parting from the assumption above, derive a binary choice model for the probability of selecting alternative \\(j\\). References "],
["chapter-3.html", "Chapter 5 Logit 5.1 Modelling choices 5.2 How to use this note 5.3 Learning objectives 5.4 Suggested readings 5.5 Preliminaries 5.6 Once again those random terms 5.7 Now, about those parameters \\(\\mu\\) and \\(\\sigma\\)… 5.8 Multinomial logit 5.9 Properties of the logit model 5.10 Revisiting the systematic utilities 5.11 Exercise", " Chapter 5 Logit “I believe that we do not know anything for certain, but everything probably.” — Christiaan Huygens 5.1 Modelling choices In Chapter 4 a conceptual framework was described to model choice-making behavior. This framework is based on the economic notion of utility, basically that which decision-makers wish to maximize when making choices. The concept of utility has many flaws - key among them that it is not directly observable. If utility could be measured directly by an external observer (or analyst), behavior would seem deterministic. However, unlike Laplace’s Demon, an external observer with only human capabilities has limited knowledge of the conditions under which choices are made, if for no other reason that she cannot possibly know the frame of mind of the decision-maker at the moment when choices are made. A way to implement the conceptual framework unders such conditions involved an acknowledgement that although the decision-maker tries to maximize his utility, some part of it will look random to the observer - therefore the term random utility modelling. This allows the analyst to make probabilitistic statements about the behavior of decision-makers. Accordingly, the analyst does not know with certainty the outcome of a choice process, but can quantify her uncertainty in a fairly precise way. Based on these concepts, Chapter 4 concluded by deriving a simple model for discrete choices, namely the lineary probability model (see Ben-Akiva and Lerman 1985, 66–68). This model is useful for illustrative purposes. However, it suffers from an important limitation: the linear probabilities are a stepwise function, which makes their mathematical treatment unfun, and also imply that certain outcome are certain (i.e., it can return probabilities of exactly one or exactly zero). This would preclude certain behaviors, which seems a somewhat arrogant thing to do on the part of the analyst. A better approach would be to allow any behavior, but assign very small probabilities to more extreme choices. In this chapter we will revisit those random utility terms to derive an alternative to the linear probability model. This will be the logit model, one of the most popular models in discrete choice analysis for reasons that will be discussed below. 5.2 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Hello, Dr. Train&quot;) ## [1] &quot;Hello, Dr. Train&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 5.3 Learning objectives In this practice, you will learn about: The Extreme Value distribution. The binary logit model. The multinomial logit model. Properties of the logit model. 5.4 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapters 4 and 5, MIT Press. Hensher, D.A., Rose, J.M., Greene, W.H (2005) Applied Choice Analysis: A Primer, Chapter 10, Cambridge University Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 7, John Wiley and Sons. Train (2009) Discrete Choice Methods with Simulation, Second Edition, Chapter 3, Cambridge University Press. 5.5 Preliminaries Load the packages used in this section: library(tidyverse) library(evd) 5.6 Once again those random terms Recall that in order to implement the probabilitistic statement at the heart of a discrete choice model requires the analyst to make assumptions about the random utility terms. Previously, a number of probability distributions were explored, and one in particular (the uniform distribution) was used to derive a simple discrete choice model. But, is the uniform distribution an appropriate choice for the purpose of modeling the random utility? The uniform distribution (and some of the other stepwise distributions seen in Chapter 4) are useful to illustrate the concept of probability, and more specifically the need to calculate the area under the curve of the distribution. The area under the curve of the uniform distribution is simply the area of a rectangle, which makes this extremely simple. On the other hand, it precludes certain outcomes, which limits its practical usefulness. The reality is that, since the utility is in principle unobservable, there is little theoretical support for any specific distribution of the random utility terms. For this reason, the choice of distribution tends to be very pragmatic, particularly attending the convenience for estimation purposes, i.e., retrieving parameters from a sample of observations. The parameters include the parameters used in the systematic utility function \\(U_{ij}\\) (more on this later), as well as any parameters needed for the distribution itself. For instance, the uniform distribution is defined by two parameters, \\(a\\) and \\(b\\): \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le b \\\\ \\frac{1}{a - b} &amp; \\quad b&gt; x &gt; a \\\\ 0 &amp; \\quad x \\ge a \\\\ \\end{array} \\right. \\] These parameters define the dispersion of the distribution. The dispersion controls the shape of the distribution, in this case how wide or narrow it is. The greater the difference between \\(a\\) and \\(b\\) the greater the range of values with non-zero probability, but the lower the probability for a constant interval of values. These two parameters also determine the center of the distribution. In this way, the uniform distribution is centered at \\(\\frac{a-b}{2}\\). Other distributions also have parameters that determine their shape and position. A convenient choice of distribution is the Extreme Value type I (EV Type I) probability distribution function. This function is defined as: \\[ f(x; \\mu,\\sigma) = e^{-(x + e^{-(x-\\mu)/\\sigma})} \\] The EV Type I distribution has two parameters, namely \\(\\mu\\) and \\(\\sigma\\), which determine the location (i.e., the center) and the dispersion of the distribution, respectively. The shape of this distribution is shown in Figure 5.1 with \\(\\mu = 0\\) and \\(\\sigma = 1\\): # Define parameters for the distribution mu &lt;- 0 sigma &lt;- 1 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %&gt;% mutate(y = dgumbel(x, loc = mu, scale = sigma)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 5.1: Extreme Value Type I distribution If you are working with the R Notebook, you can try changing the parameters to see how the function behaves (remember to adjust the limits if you change the center of the distribution!). The EV Type I has a very interesting property: the difference of two EV Type I distributions follows the logistic distribution. In other words, if \\(X \\sim \\text{EVI}(\\alpha_Y,\\sigma)\\) and \\(Z \\sim \\text{EVI}(\\alpha_Z,\\sigma)\\), then: \\[ X - Y \\sim \\text{Logistic}(\\alpha_X - \\alpha_Y,\\sigma) \\] If we define the difference of the random utility terms as \\(\\epsilon_n = \\epsilon_j - \\epsilon_k\\), the logistic distribution, in turn, is defined as follows: \\[ f(x; \\mu,\\sigma) = \\frac{e^{-(x-\\mu)/\\sigma}}{\\sigma(1 + e^{-(x-\\mu)/\\sigma})^2} \\] Whereas the EV Type I distribution was not symmetric, the shape of the logistic distribution is. The logistic distribution is, in fact, similar to the normal distribution but it has fatter tails, which means that the probability of extreme values is higher. This is illustrated in Figure 5.2: # Define parameters for the distribution mu &lt;- 0 sigma &lt;- 1 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %&gt;% mutate(logistic = dlogis(x, location = mu, scale = sigma), normal = dnorm(x, mean = mu, sd = sigma)) # Plot ggplot() + geom_area(data = df, aes(x, logistic), fill = &quot;blue&quot;, alpha = 0.5) + geom_area(data = df, aes(x, normal), fill = &quot;black&quot;, alpha = 0.5) + #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 5.2: Comparison of the logistic (blue) and normal (grey) distributions Since the probability expression is given in terms of the difference of the random utilities, if we assume that the random terms \\(\\epsilon\\) follow the EV Type I distribution, their difference (i.e., \\(\\epsilon_n = \\epsilon_j - \\epsilon_i\\)) follows the logistic distribution. As before, the area under the curve of the function needs to be calculated to obtain a probability. Unfortunately, this needs to be done by integration. Fortunately, this integral has an analytical solution, or so-called closed form: \\[ F(x; \\mu,\\sigma) = \\frac{1}{1 + e^{-(\\epsilon_n-\\mu)/\\sigma}} \\] Accordingly, the probability expression is as follows: \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{1}{1 + e^{-(\\epsilon_n-\\mu)/\\sigma}} = \\frac{1}{1 + e^{-(V_j-V_k-\\mu)/\\sigma}} \\] Which, after some manipulation, can be rewritten as: \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{e^{V_j/\\sigma}}{e^{V_j/\\sigma} + e^{(V_k+\\mu)/\\sigma}} \\] The above is called the logit probability and the resulting model is called the logit model. As seen, the probability of choosing alternative \\(j\\) is the area under the curve of the logistic distribution function, as seen in Figure 5.3 (assuming \\(mu = 0\\) and \\(\\sigma = 1\\)): # Define parameters for the distribution mu &lt;- 0 sigma &lt;- 1 # Define an upper limit for calculating the probability X &lt;- -1 # Create data frames for plotting df &lt;- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %&gt;% mutate(y = dlogis(x, location = mu, scale = sigma)) df_p &lt;- data.frame(x =seq(from = -5, to = X, by = 0.01)) %&gt;% mutate(y = dlogis(x, location = mu, scale = sigma)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + # Plot distribution function geom_area(data = df_p, fill = &quot;orange&quot;, alpha = 1) + # Plot area under the curve #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis xlab(expression(paste(epsilon[n]))) + # Label the y axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 5.3: Logit probability Try changing the upper limit in the figure above to explore the behavior of the logit probability. What is the probability of choosing \\(j\\) when \\(V_j - V_k = 0\\)? What is the probability of choosing \\(j\\) when \\(V_j &gt;&gt; V_k\\)? And when \\(V_j &lt;&lt; V_k\\)? Is this as expected? The cumulative distribution function is shown in Figure 5.4. Notice that this function tends asymptotically to 0 when \\(x\\) tends to \\(-\\infty\\) and to 1 when \\(x\\) tends to \\(\\infty\\). This function never assigns values of exactly 0 or exactly 1. # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %&gt;% mutate(y = plogis(x)) # Plot logit_plot &lt;- ggplot(data = df, aes(x, y)) + geom_line(color = &quot;orange&quot;) + # Plot cumulative distribution function ylim(c(0, 1)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) # Add x axis logit_plot + xlab(expression(paste(V[j], &quot; - &quot;, V[k], sep=&quot;&quot;))) + # Label the x axis ylab(expression(paste(P[j]))) # Label the y axis Figure 5.4: Linear cumulative distribution function The logit probability exhibits a shape usually called a sigmoid (for its resemblance to the letter “s”). This shape is shared by most other discrete choice models - the uniform distribution in Chapter 4, for instance, resembled an angular letter “s”, whereas the linear and quadratic distribution functions started to display the non-linear aspect of the logit probability function. Sigmoid functions are of interest in many fields. The study of technology adoption is a case in point; new technologies are initially adopted slowly, then go through a rapid growth stage, before reaching saturation. Population growth is often represented by similar curves, with population growing slowly, then explosively, before reaching a carrying capacity limit. In the case of discrete choice analysis, the shape of the function is interesting from a policy perspective. In the vast majority of cities in North America, for example, the two main modes of transportation are cars and transit. However, the shares of transit tend to be very low, sometimes lower than 10% or even 5%. This suggests that the underlying probabilities of choosing transit at the individual level are very low too. Suppose that the logit curve in Figure 5.4 is for the probability of choosing transit. If the initial probability of choosing transit is low, large increases in the utility of transit result in relatively modest gains in probability (see solid blue line in Figure 5.5). If the starting probability of transit had been instead 0.5, an identical increase in the utility of transit would result in a much larger gain in the probability (see dashed red line in Figure 5.5). logit_plot + xlab(expression(paste(V[transit], &quot; - &quot;, V[car], sep=&quot;&quot;))) + # Label the x axis ylab(expression(paste(P[j]))) + # Label the y axis annotate(&quot;segment&quot;, x = -3.75, xend = -2.5, y = 0.024, yend = 0.024, colour = &quot;blue&quot;, linetype = &quot;solid&quot;) + annotate(&quot;segment&quot;, x = -2.5, xend = -2.5, y = 0.024, yend = 0.075, colour = &quot;blue&quot;, linetype = &quot;solid&quot;) + annotate(&quot;segment&quot;, x = 0, xend = 1.25, y = 0.5, yend = 0.5, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;segment&quot;, x = 1.25, xend = 1.25, y = 0.5, yend = 0.77, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) Figure 5.5: Implication of the sigmoid shape The implication is that when the penetration of an alternative (think transit, hybrid vehicles, clean energy, and other new technologies) is still low, the incentives needed to raise the probabilities need to be very strong even for modest gains. When penetration has increased, the incentives may be eased since their impact is now more than proportional, until reaching saturation, where again large gains in utility result in modest increases in the probability of adoption. 5.7 Now, about those parameters \\(\\mu\\) and \\(\\sigma\\)… Figure 5.3 above was created assuming that \\(\\mu=0\\) and \\(\\sigma=1\\). Can we really set these values in such an arbitrary fashion? The answer is no and yes. In the case of the centering parameter \\(\\mu\\), setting it arbitrarily to zero is not appropriate. The reason is that we can think of this parameter as being key to calculating the difference between the systematic utilities. As seen above, the logit probability is: \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{1}{1 + e^{-(V_j-V_k-\\mu)/\\sigma}} \\] Assume that we let one of the utility functions absorb \\(\\mu\\), that is we let either: \\[ V^*_k = V_k + \\mu \\] or: \\[ V^*_j = V_j - \\mu \\] It does not really matter which utility function we choose to absorb \\(\\mu\\) (the only thing that changes is the sign). For convenience, we will say that it is \\(V_k\\), in which case the logit probability can be written as: \\[ P_j = P(V_j - V^*_k \\le \\epsilon_n) = \\frac{1}{1 + e^{-(V_j-V^*_k)/\\sigma}} \\] The difference in other words depends on the value of \\(\\mu\\). When \\(\\mu\\) is a large positive number, the effect is to increase the utility of alternative \\(k\\) (or conversely, since it would enter with a negative sign in \\(V^*_j\\), it would decrease the utility of alternative \\(j\\)). When \\(\\mu\\) is a large negative number, the effect is to increase the utility of \\(j\\) - or alternatively to reduce the utility of \\(k\\). For this reason we do not want to arbitrarily set the value of \\(\\mu\\) to zero, because this parameter contains information about the relative differences between \\(V_j\\) and \\(V_k\\). The utility function that does not contain the centering parameter \\(\\mu\\) is called the reference function. For simplicity of presentation, I will drop the notation \\(V^*\\) and will assume henceforth that one of the utility functions has absorbed parameter \\(\\mu\\). Now, with respect to the dispersion parameter \\(\\sigma\\), this parameter is common to the two utility functions in the logit probability and, as it turns out, it can be arbitrarily set to one. Consider two utility functions as follows: \\[ V_j - V_k \\] Multiplying (alternatively dividing) by a constant greater than zero changes the magnitude of their difference, since: \\[ \\theta(V_j - V_k) = \\theta V_j - \\theta V_k \\] In other words, mutliplying two quantities by a positive constant changes the cardinality of the difference. If you are working with the R Notebook, you might want to try changing the value of theta below, keeping in mind that the value must be greater than zero: V_j &lt;- -4 V_k &lt;- 8 theta &lt;- 0.8 theta * V_j - theta * V_k ## [1] -9.6 You will notice that the difference changes as you change the value of theta. But what about the sign? On the other hand, multiplying two quantities by a positive constant does not affect their ordinality. That is, if \\(V_j &gt; V_k\\) then it is always true that \\(\\theta V_j &gt; \\theta V_k\\). Recall the decision making rule: an alternative is chosen if its utility is greater than that of the competing alternatives. The rule is purely ordinal, it does not matter if the difference between them is small or large - in other words, their cardinality is irrelevant. This is convenient because it allows us to simplify the logit probability as follows, by arbitrarily setting \\(\\sigma=1\\): \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{1}{1 + e^{-(V_j-V_k)}} = \\frac{e^{V_j}}{e^{V_j} + e^{V_k}} \\] 5.8 Multinomial logit The logit model above was derived assuming a choice set with only two alternatives. This, of course, is very restrictive, and there are many situations where more than two alternatives are of interest. Fortunately, a multinomial version of the logit model can be derived without much difficulty, and it also results in a closed form expression, as follows: \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{e^{V_j}}{\\sum_k^Je^{V_k}} \\] Notice that in this case there are \\(J-1\\) parameters \\(\\mu\\) that are absorbed by all but one of the utility functions. As before, it does not matter which utility is selected to act as the reference, since the signs (and magnitudes) of the centering parameters adjust accordingly. More on this later. 5.9 Properties of the logit model The logit model is the workhorse of discrete choice analysis, in good measure because of its closed form which does not require numerical evaluation of the integrals involved in calculating probabilities (i.e., the “area under the curve”, although in multinomial situations this actually is a volume under the surface!) One important property of the logit model is the way it handles substitution patterns. Consider the ratio of odds for any two alternatives according to the multinomial logit model: \\[ \\frac{P_j}{P_m}=\\frac{\\frac{e^{V_j}}{\\sum_ke^{V_k}}}{\\frac{e^{V_m}}{\\sum_ke^{V_k}}} =\\frac{e^{V_j}}{e^{V_m}} =e^{V_j - V_m} \\] As seen, the ratio of the odds of \\(P_j\\) to \\(P_m\\) depends only on the difference in the utilities of alternatives \\(j\\) and \\(m\\) and nothing more. Furthermore, recall that the choice set is by design an exhaustive set of possible alternatives, and therefore the sum of the probabilities over this set is one: \\[ P_1 + P_2+\\cdots+P_J=1 \\] The above means that if the probability of choosing one alternative, say \\(j\\), increases, then the probabilities of choosing some or all of the other alternatives must decline. But since the ratio of odds for any two alternatives is independent of other alternatives in the choice set, the way the probabilities change depends on the change on the probability that triggered the adjustments. This property is called, quite fittingly, independence from irrelevant alternatives or IIA. Suppose, for instance, that a choice set consists of three alternatives products, say margarine (\\(m\\)) by Naturally, and salted butter butter (\\(sb\\)) and low-sodium butter (\\(lb\\)) by Happy Farms. The initial probabilities of choosing these alternatives are as follows: \\[ \\left\\{ \\begin{array}{ll} P^0_{m}=&amp;\\frac{1}{3}\\\\ P^0_{sb}=&amp;\\frac{1}{3}\\\\ P^0_{lb}=&amp;\\frac{1}{3}\\\\ \\end{array} \\right. \\] Next, suppose that a change in the attribute set of salted butter (\\(sb\\)), for instance a reduction in price, leads to an increase in the probability of choosing this product. Now the probability of choosing salted butter is: \\[ P^1_{sb}=\\frac{1}{2} \\] How do the other probabilities change? On the one hand, we know that the sum of the new probabilities must be one: \\[ P^1_{m} + P^1_{sb} + P^1_{lb} = 1 \\] Since the attributes of margarine and low-sodium butter did not change, we know that their utilities remain unchanged, and therefore: \\[ \\frac{P^1_{m}}{P^1_{lb}} = \\frac{\\frac{1}{3}}{\\frac{1}{3}} = 1 \\] In other words, the probability of \\(P^1_m = P^1_{lb}\\). Substituting: \\[ P^1_{m} + P^1_{sb} + P^1_{lb} = 2P^1_{m} + P^1_{sb} = 1 \\] Solving for \\(P^1_lb\\): \\[ P^1_m = \\frac{1 - P^1_{sb}}{2} = \\frac{1 - \\frac{1}{2}}{2} = \\frac{1}{4} \\] Therefore the new probabilities are: \\[ \\left\\{ \\begin{array}{ll} P^1_{m}=&amp;\\frac{1}{4}\\\\ P^1_{sb}=&amp;\\frac{1}{2}\\\\ P^1_{lb}=&amp;\\frac{1}{4}\\\\ \\end{array} \\right. \\] Notice that the increase in probability of choosing sunflower-based margarine draws proportionally from the other alternatives (i.e., butter and olive oil-based margarine) - in fact, 12.5% from each. Does this result make sense? What is now the market share of Happy Farms-brand line of butter? The property of Independence from Irrelevant Alternatives leads to proportional substitution patterns. Consider the following initial probabilities: \\[ \\left\\{ \\begin{array}{ll} P^0_{m}=0.5\\\\ P^0_{sb}=0.3\\\\ P^0_{lb}=0.2\\\\ \\end{array} \\right. \\] The new probability of \\(sb\\) changes to \\(P^1_{sb}=0.5\\). Following the same logic: \\[ \\frac{P^1_{m}}{P^1_{lb}} = \\frac{0.5}{0.2} = \\frac{5}{2} \\] And: \\[ P^1_m = \\frac{5}{7}(1 - P^1_{sb}) = \\Big(\\frac{5}{7}\\Big)\\Big(\\frac{1}{2}\\Big) = \\frac{5}{14} = 0.3571 \\] So the final probabilities are: \\[ \\left\\{ \\begin{array}{ll} P^1_{m}=\\frac{5}{14}=0.3571\\\\ P^1_{sb}=\\frac{1}{2}=0.5000\\\\ P^1_{lb}=\\frac{2}{14}=0.1429\\\\ \\end{array} \\right. \\] Now, the increase in \\(P1_{sb}\\) to \\(1/2\\) from \\(P^0_{sb}=1/5\\) is drawing more from \\(P^0_m\\) than from \\(P^0_{lb}\\). However, the pattern of substitution is still proportional, as it can be verified: \\[ \\begin{array}{ll} \\frac{P^1_{m}}{P^0_{m}}=\\frac{\\frac{5}{14}}{\\frac{1}{2}}=\\frac{10}{14}\\\\ \\frac{P^1_{lb}}{P^0_{lb}}=\\frac{\\frac{2}{14}}{\\frac{2}{10}}=\\frac{10}{14}\\\\ \\end{array} \\] Proportional substitution patterns are a consequence of the lack of correlation among the random utilities. The logit model considers that the alternatives are all independent. However, in this example, this condition is suspect: the two kinds of margarine are more similar between them than either are to butter. Indeed, if consumers choose butter for flavor, lowering the price of one kind of margarine is likely to draw less than proportionally from the probability of choosing butter - and more than proportionally from the probability of the other kind of margarine if, for instance, consumers prefer margarine for health reasons but respond to price changes. In this case, the correlation between the two kinds of margarine is a consequence of a missing attribute - say flavor, or health, that is necessary to discriminate among the alternatives. In this way, the logit model can be seen as the ideal model - its closed form being a very attractive feature - as long as the systematic utilities are properly and completely specified. When this is not the case, the results can lead to unrealistic and even unreasonable substition patterns. This issue suggests two possible courses of action: Working to ensure that the systematic utility functions are properly and completely specified. Modifying the modelling apparatus to accommodate correlations among the random utilities. As will become clear in later chapters, much work in the field of discrete choice analysis has been concerned with the latter. 5.10 Revisiting the systematic utilities Much of the discussion above has concentrated on the random utility; however, specifying the systematic utility is key. Recall that the utility is a function of the attributes of the alternatives and possibly the attributes of the decision-makers to allow the model to capture heterogeneity in decision-making styles by individuals. the utility function is a convenient way of summarizing all those attributes. Think again of the example of buying a new phone (see Chapter 4). In that simple example, the utilities were a function of three attributes, namely cost, speed, and income - to which we can add the random utility: \\[ \\begin{array}{c} U_{i, \\text{Do-Nothing}} = U(\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{Do-Nothing}}, \\text{ income}_i) = V(\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{Do-Nothing}}, \\text{ income}_i) + \\epsilon_{i, \\text{Do-Nothing}}\\\\ U_{i, \\text{New-Phone}} = U(\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}, \\text{ income}_i) = V(\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}, \\text{ income}_i) + \\epsilon_{i, \\text{New-Phone}}\\\\ \\end{array} \\] A common way of specifying the systematic utility is as linear-in-parameters, something that will be familiar to users of regression analysis: \\[ \\begin{array}{c} V(\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{Do-Nothing}}, \\text{ income}_i) = \\beta_1\\text{cost}_{\\text{Do-Nothing}} + \\beta_2\\text{ speed}_{\\text{Do-Nothing}} + \\beta_3\\text{ income}_i\\\\ V(\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}, \\text{ income}_i) = \\mu + \\beta_1\\text{cost}_{\\text{New-Phone}} + \\beta_2\\text{ speed}_{\\text{New-Phone}} + \\beta_3\\text{ income}_i\\\\ \\end{array} \\] Notice how the location parameter of the logistic function is absorbed by one of the utility functions! The additive form of the utilities reflects a compensatory choice-making strategy: higher costs may be offset by higher speeds, for example. An important consideration is the way attributes enter the utility functions. Recall that one way of writing the logit probability was: \\[ P_j = \\frac{1}{1 + e^{-(V_j-V_k)}} \\] This formulation makes it clear that the probability is a function of the differences between utilities (this remains true in the multinomial logit, even if it is not as clear to see). Now consider what happens when the differences in utility are calculated: \\[ V_{i,\\text{Do-Nothing}} - V_{i,\\text{New-Phone}}= \\beta_1\\text{cost}_{i,\\text{Do-Nothing}} + \\beta_2\\text{ speed}_{i,\\text{Do-Nothing}} + \\beta_3\\text{income}_i - \\mu - \\beta_1\\text{cost}_{i,\\text{New-Phone}} - \\beta_1\\text{speed}_{\\text{i, New-Phone}} - \\beta_1\\text{income}_i\\\\ = \\beta_1(\\text{cost}_{i, \\text{Do-Nothing}} - \\text{cost}_{i, \\text{New-Phone}}) + \\beta_2(\\text{ speed}_{i, \\text{Do-Nothing}} - \\text{ speed}_{i, \\text{New-Phone}}) + \\beta_3(\\text{ income}_i - \\text{ income}_i) - \\mu \\] The income attribute vanishes! It is useful to distinguish between attributes that vary across utility functions and those that do not. Level of service attributes, those that describe the alternatives, generally vary by utility function - indeed, it is those attributes that help discriminate between alternatives. In this instance, income is invariant across utility functions. Personal attributes of the decision-makers, in general, are invariant across utility functions. The most common way of dealing with attributes that are constant across utility functions is to select one utility to act as a reference and set that attribute to zero there. This is illustrated below: \\[ \\begin{array}{c} V_{\\text{Do-Nothing}} = \\beta_1\\text{cost}_{\\text{Do-Nothing}} + \\beta_2\\text{ speed}_{\\text{Do-Nothing}} + \\beta_3(0)\\\\ V_{\\text{New-Phone}} = \\mu + \\beta_1\\text{cost}_{\\text{New-Phone}} + \\beta_2\\text{ speed}_{\\text{New-Phone}} + \\beta_3\\text{ income}_i\\\\ \\end{array} \\] The difference in utilities then becomes: \\[ V_{i,\\text{Do-Nothing}} - V_{i,\\text{New-Phone}}= \\beta_1(\\text{cost}_{i, \\text{Do-Nothing}} - \\text{cost}_{i, \\text{New-Phone}}) + \\beta_2(\\text{ speed}_{i, \\text{Do-Nothing}} - \\text{ speed}_{i, \\text{New-Phone}}) - \\beta_3(\\text{ income}_i) - \\mu \\] When the effect of income is positive (i.e., \\(\\beta_3&gt;0\\)) higher incomes reduce the probability of doing nothing, and when the effect of income is negative (i.e., \\(\\beta_3&lt;0\\)) higher incomes reduce the probability of buying a new phone. The effect of income is relative to the reference alternative. When there are more than two alternatives, the attribute can be entered in all but the reference utility, as shown next: \\[ \\begin{array}{llll} V_{\\text{Do-Nothing}} = &amp;0 &amp;+ 0 &amp;+ \\beta_1\\text{cost}_{\\text{Do-Nothing}} &amp;+ \\beta_2\\text{ speed}_{\\text{Do-Nothing}} &amp;+ \\beta_3(0)&amp; + \\beta_4(0) \\\\ V_{\\text{uPhone}} = &amp;\\mu_{\\text{uPhone}} &amp;+ 0 &amp;+ \\beta_1\\text{cost}_{\\text{uPhone}} &amp;+ \\beta_2\\text{ speed}_{\\text{uPhone}} &amp;+ \\beta_3\\text{ income}_i &amp; + \\beta_4(0)\\\\ V_{\\text{zPhone}} = &amp;0 &amp;+ \\mu_{\\text{zPhone}} &amp;+ \\beta_1\\text{cost}_{\\text{zPhone}} &amp;+ \\beta_2\\text{ speed}_{\\text{zPhone}} &amp;+ \\beta_3(0) &amp;+ \\beta_4\\text{ income}_i\\\\ \\end{array} \\] The above also illustrates how location parameters are absorbed by \\(J-1\\) utility functions. Another way to introduce attributes that do not vary across utility functions is reminiscent of Casetti’s expansion method (Casetti 1972). The expansion method is a systematic approach to introduce variable interactions that proceeds by defining an initial model whose coefficients are subsequently expanded using contextual variables. Suppose that the initial model is comprised of the utility functions with only level of service variables: \\[ \\begin{array}{c} V_{\\text{Do-Nothing}} = \\beta_1\\text{cost}_{\\text{Do-Nothing}} + \\beta_2\\text{ speed}_{\\text{Do-Nothing}}\\\\ V_{\\text{New-Phone}} = \\mu + \\beta_1\\text{cost}_{\\text{New-Phone}} + \\beta_2\\text{ speed}_{\\text{New-Phone}}\\\\ \\end{array} \\] The coefficients are expanded by a contextual variable, in this case income: \\[ \\beta_1 = \\beta_{11} + \\beta_{12}\\text{income}_i\\\\ \\beta_2 = \\beta_{21} + \\beta_{22}\\text{income}_i \\] Substituting the expanded coefficients in the initial model: \\[ \\begin{array}{c} V_{\\text{Do-Nothing}} = (\\beta_{11} + \\beta_{12}\\text{income}_i)\\text{cost}_{\\text{Do-Nothing}} + (\\beta_{21} + \\beta_{22}\\text{income}_i)\\text{ speed}_{\\text{Do-Nothing}}\\\\ V_{\\text{New-Phone}} = \\mu + (\\beta_{11} + \\beta_{12}\\text{income}_i)\\text{cost}_{\\text{New-Phone}} + (\\beta_{21} + \\beta_{22}\\text{income}_i)\\text{ speed}_{\\text{New-Phone}}\\\\ \\end{array} \\] The expanded model then becomes: \\[ \\begin{array}{c} V_{\\text{Do-Nothing}} = \\beta_{11}\\text{cost}_{\\text{Do-Nothing}} + \\beta_{12}\\text{income}_i\\cdot\\text{cost}_{\\text{Do-Nothing}} + \\beta_{21}\\text{ speed}_{\\text{Do-Nothing}} + \\beta_{22}\\text{income}_i\\cdot\\text{ speed}_{\\text{Do-Nothing}}\\\\ V_{\\text{New-Phone}} = \\mu + \\beta_{11}\\text{cost}_{\\text{New-Phone}} + \\beta_{12}\\text{income}_i\\cdot\\text{cost}_{\\text{New-Phone}} + \\beta_{21}\\text{ speed}_{\\text{New-Phone}} + \\beta_{22}\\text{income}_i\\cdot\\text{ speed}_{\\text{New-Phone}}\\\\ \\end{array} \\] The difference of the two utilities in turn is: \\[ \\begin{array}{l} V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} =\\\\ \\beta_{11}(\\text{cost}_{\\text{Do-Nothing}} - \\text{cost}_{\\text{New-Phone}}) + \\beta_{12}\\text{income}_i\\cdot(\\text{cost}_{\\text{Do-Nothing}} - \\text{cost}_{\\text{New-Phone}} ) \\\\ + \\beta_{21}(\\text{ speed}_{\\text{Do-Nothing}} - \\text{ speed}_{\\text{New-Phone}}) + \\beta_{22}\\text{income}_i\\cdot(\\text{ speed}_{\\text{Do-Nothing}} - \\text{ speed}_{\\text{New-Phone}}) - \\mu \\end{array} \\] Specifying the utility functions is more art than technique. We will return to this when we begin the practice of model estimation. 5.11 Exercise Answer the following questions. 5.11.1 Questions What do we mean when we say that the logit probability has a closed form? Why is it that we can set the dispersion parameter in the logit probabilities to one? Suppose that a choice set consists of two alternatives, travel by car (\\(c\\)) and travel by blue bus (\\(bb\\)). The utilities of these two modes are the same, that is: \\[ V_c = V_{bb} \\] What are the probabilities of choosing these two modes? Suppose that the transit operator decides to introduce a new service, namely a red bus. This red bus is identical to the blue bus in every respect except the color. Under these new conditions, what are the logit probabilities of choosing these modes? Discuss the results of introducing a new mode in the choice process above. References "],
["references.html", "References", " References "]
]
