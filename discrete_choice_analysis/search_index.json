[
["index.html", "Discrete Choice Analysis with R Preface Choices choices choices Plan Audience Requisites", " Discrete Choice Analysis with R Antonio Paez 2019-01-27 Preface ‘Would you tell me, please, which way I ought to go from here?’ ‘That depends a good deal on where you want to get to,’ said the Cat. ‘I don’t much care where -’ said Alice. ‘Then it doesn’t matter which way you go,’ said the Cat. ‘- so long as I get SOMEWHERE,’ Alice added as an explanation. ‘Oh, you’re sure to do that,’ said the Cat, ‘if you only walk long enough.’ — Lewis Carroll, Alice in Wonderland “We are our choices.” — Jean-Paul Sartre Choices choices choices The field of spatial statistics has experienced phenomenal growth in the past 20 years. From being a niche subdiscipline in quantitative geography, statistics, regional science, and ecology at the beginning of the 1990s, it is now a mainstay of applications in a multitude of fields, including medical imaging, remote sensing, civil engineering, geology, statistics and probability, spatial epidemiology, end ecology, to name just a few disciplines. The growth in research and applications in spatial statistics has been in good measure fueled by the explosive growth in geotechnologies: technologies for sensing and describing the natural, social, and built environments on Earth. An outcome of this is that spatial data are, to an unprecedented level, within the reach of multitudes. Hardware and software have become cheaper and increasingly powerful, and we have transitioned from a data poor environment (in all respects, but particularly in terms of spatial data) to a data rich environment. Twenty years ago, for instance, technical skills in spatial analysis included tasks such as digitizing. As a Masters student, I spent many boring hours digitizing paper maps before I could do any analysis on our single-seat (and relatively expensive) Geographic Information System (GIS). I was more or less a freak: altough there was an institutional push to adopt GIS, relatively few in my academic environment saw the value of spending hours doing what nowadays we would think of as low-level technical work. Surely, the time of a Master student, let alone a professional researcher or business analyst, is now more valuable than that. Indeed, very little time is spent anymore in such tasks low-level tasks, as data increasingly are collected and disseminated in a digital format. Instead, there is a huge appetite for what could be called the brainware of spatial analysis, the intelligence counterpart of the hardware, software, and data provided by geotechnologies. So what is spatial statistics? Very quickly, I will define spatial statistics as the application of statistical techniques to data that have geographical references - in other words, to the statistical analysis of maps. Like statistics more generally, spatial statistics is interested in hypothesis testing and inference. What distiguishes it as a branch of the broader field of statistics is its explicit interest in situations where data are not independent from each other (like throws of a dice) but rather display systemic associations. These associations, when seen through the lens of cartography, can manifest themselves as patterns of similarities (birds of a feather flock together) or disimilarities (repulsion due spatial competition among firms) - as two common examples of spatial patterns. Spatial statistics covers a broad array of techniques for the analysis of spatial patterns, including testing whether patterns are random or not, and a wide variety of modelling approaches as well. These tools enhance the brainware of analysts by allowing them to identify and possibly model patterns for inferring processes and/or for making spatial predictions. Plan The plan with these notes is to introduce discrete choice analysis in an intuitive way. To achieve this, I use examples and coding, lots of coding. There are some classical references, for instance Ben-Akiva and Lerman [REF:1985] and Train [REF:], and then more specialized books such as Hensher et al. [REF:Stated Choice]. While these notes should be appealing to students or others who are approaching this topic for the first time, I strongly encourage becoming acquainted with these books. In particular, I have relied heavily on Train’s text to organize the present notes, and thus the notes follow a thematic approach, moving from the fundamentals, introducing the logit model, and then by families of models, i.e., GEV, probit, etc. Each section covers a topic that builds on previous material. At the moment, my aim is that every section is followed by an activity. I have used the materials presented in this texts (in different incarnations) for teaching discrete choice analysis in different settings. Primarily, these notes have been used in the course GEOG 738 Discrete Choice Analysis at McMaster University. This course is a full (Canadian) graduate term, which typically means 11 or 12 weeks of classes. The course is organized as a 2-hour seminar that is offered once per week. Accordingly, each section is designed to cover very approximately the material that I am used to cover in a 2 hour seminar. As I continue to work on these notes, I hope to be able to add optional (or bonus) chapters, that could be used 1) to extend a course on spatial statistics beyond the 13 week horizon of the Canadian term, or 2) to offer more advanced material to interested readers. Audience The notes were designed for a graduate course in geography, but are not necessarily limited to use by geographers, and could be a valuable resource to graduate students and others interested in discrete choice analysis and applications in economics, planning, transportation engineering, public health, etc. The prerequisites are an introductory college/university level course on multivariate statistics, ideally covering the fundamentals of probability theory and hypothesis testing. Working knowledge of multivariate linear regression analysis is a bonus but not strictly required. Requisites To fully benefit from this text, up-to-date copies of R and RStudio are highly recommended. There are different packages that implement discrete choice methods in R. I will particularly rely on the packages mlogit and gmnl. "],
["preliminaries-installing-r-and-rstudio.html", "Chapter 1 Preliminaries: Installing R and RStudio 1.1 Introduction 1.2 Learning Objectives 1.3 R: The Open Statistical Computing Project 1.4 Packages in R", " Chapter 1 Preliminaries: Installing R and RStudio 1.1 Introduction The course makes extensive use of R and RStudio. Here are the instructions to install these two tools. 1.2 Learning Objectives In this reading, you will learn: How to install R. About the RStudio Interactive Development Environment. About packages in R. 1.3 R: The Open Statistical Computing Project 1.3.1 What is R? R is an open-source language for statistical computing. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland, in New Zealand, as a way to offer their students an accessible, no-cost tool for their courses. R is now maintained by the R Development Core Team, and developed by hundreds of contributors around the globe. R is an attractive alternative to other software applications for data analysis (e.g., Microsoft Excel, STATA) due to its open-source character (i.e., it is free), its flexibility, and large and dedicated user community, which means if there’s something you want to do (for instance, linear regression), it is very likely that someone has already developed functionality for it in R. A good way to think about R is as a core package, to which a library, consisting of additional packages, can be attached to increase its functionality. R can be downloaded for free at: https://cran.rstudio.com/ R comes with a built-in console (a user graphical interface), but better alternatives to the basic interface exist, including RStudio, an Integrated Development Environment, or IDE for short. RStudio can also be downloaded for free, by visiting the website: https://www.rstudio.com/products/rstudio/download/ R requires you to work using the command line, which is going to be unfamiliar to many of you accustomed to user-friendly graphical interfaces. Do not fear. People worked for a long time using the command line, or even more cumbersome, punched cards in early computers. Graphical user interfaces are convenient, but they have a major drawback, namely their inflexibility. A program that functions based on graphical user interfaces allows you to do only what is hard-coded in the user interface. Command line, as we will see, is somewhat more involved, but provides much more flexibility in operation. Go ahead. Install R and RStudio in your computer. Before introducing some basic functionality in R, lets quickly take a tour R Studio. 1.3.2 The RStudio IDE The RStudio IDE provides a complete interface to interact with the language R. It consists of a window with several panes. Some panes include in addition several tabs. There are the usual drop-down menus for common operations, such as creating new files, saving, common commands for editing, etc. See Figure 1.1 below. Figure 1.1: The RStudio IDE The editor pane allows you to open and work with text and other files, where you can write instructions that can be passed on to the program. Writing something in the editor does not execute any instructions, it merely records them for possible future use. In fact, much of what is written in the editor will not be instructions, but rather comments, discussion, and other text that is useful to understand code. The console pane is where instructions are passed on to the program. When an instruction is typed (or copied and pasted) there, R will understand that it needs to do something. The instructions must be written in a way that R understands, otherwise errors will occur. If you have typed instructions in the editor, you can use “ctrl-Enter” (in Windows) or “cmd-Enter” (in Mac) to send to the console and execute. The environment is where all data that is currently in memory is reported. The History tab acts like a log: it keeps track of the instructions that have been executed in the console. The last pane includes a number of useful tabs. The File tab allows you to navigate your computer, change the working directory, see what files are where, and so on. The Plot tab is where plots are rendered, when instructions require R to do so. The Packages tab allows you to manage packages, which as mentioned above, are pieces of code that can augment the functionality of R. The Help tab is where you can consult the documentation for functions/packages/see examples, and so on. The Viewer tab is for displaying local web content, for instance, to preview a Notebook (more on Notebooks soon). This brief introduction should have allowed you to install both R and RStudio. The next thing that you will need is packages. 1.4 Packages in R Packages are the basic units of reproducible code in the R multiverse. Packages allow a developer to create a self-contained unit of code that often is meant to achieve some task. For instance, there are packages in R that specialize in statistical techniques, such as cluster analysis, visualization, or data manipulation. Some packages can be miscellaneous tools, or contain mostly datasets. Packages are a very convenient way of maintaining code, data, and documentation, and also of sharing all these resources. Packages can be obtained from different sources (including making them!). One of the reasons why R has become so successful is the relative facility with which packages can be distributed. A package that I use frequently is called tidyverse. The tidyverse is a collection of functions for data manipulation, analysis, and visualization. This package can be downloaded and installed in your personal library of R packages by using the function install.packages, as follows: install.packages(&quot;tidyverse&quot;) The function install.packages retrieves packages from the Comprehensive R Archive Network, or CRAN for short. CRAN is a collection of sites (accessible via the internet) that carry identical materials for distribution for R. "],
["chapter-1.html", "Chapter 2 Data and stuff 2.1 What are models? 2.2 How to use this note 2.3 Learning objectives 2.4 Suggested readings 2.5 Ways of measuring stuff 2.6 Importing data 2.7 Data Classes in R 2.8 More on indexing and data manipulation 2.9 Visualization 2.10 Exercise", " Chapter 2 Data and stuff “Essentially, all models are wrong, but some are useful.” — George E.P. Box “You can have data without information, but you cannot have information without data.” — Daniel Keys Moran 2.1 What are models? Model building requires three things: Raw materials. Tools. Technical expertise (hopefully!). This is true whether the model is physical (for instance a sculpture), conceptual (a mental map), or statistical/mathematical (the gravity model or a regression model). In the case of a sculpture, the raw materials can be marble, wood, or clay; the tools chisels, mallet, and spatula; and the technique the mastery of the sculptor when working with the tools and the materials. Anyone can try sculpture, and most people can create sculptures. These kind of models are evaluated by their aesthetic value, not necessarily their usefulness. But if the scultpure is poorly balanced and falls and breaks, then its value is limited by its structural integrity - the skill of the sculptor matters even if only in this sense. In the case of a mental map, the raw materials are ideas, the tools are a drawing surface and tools for writing, or maybe an app, and the technical expertise is the ability of the modeler to organize ideas in a useful way. There are useful conceptual models, and conceptual models that are anything but. Figure 2.1 shows two example of conceptual models. Figure 2.1: Two Examples of Conceptual Models In the case of mathematical/statistical models, the raw materials are data; the tools are descriptive statistics and statistical plots, and various forms of regression analysis; and the technical expertise is the ability of the modeler to select tools that are appropriate to the data, and to convince the data to “speak”: in other words, to extract information from the data. As Moran said in the aphorism quoted at the top of this section: you can have data without information, but no information without data. Technical mastery is the degree to which a modeller can obtain information from data that is useful, accurate, and precise, to the extent that the raw materials permit. Before moving on to the technical skills required for modeling, it is important to understand the raw materials and the tools. The objective of this note is to introduce some important concerning data and data manipulation, and some useful tools. 2.2 How to use this note The source for the document you are reading is an R Notebook. Notebooks are a form “literate programming”, a style of document that uses code to illustrate a discussion, as opposed to the traditional programming style that uses natural language to discuss/document the code. It flips around the usual technical writing approach to make it more intuitive and accessible. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Hello, Discrete Choice Analysis!&quot;) ## [1] &quot;Hello, Discrete Choice Analysis!&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. Whichever way you are working, you might want to give it a try now! You will see that the chunk of code above instructed R (and trough R the computer) to print (or display on the screen) some text. 2.3 Learning objectives In this practice, you will learn about: Different ways to measure stuff. Basic operations in R. Data classes, data types, and data transformations. The use of packages in R. Basic visualization. 2.4 Suggested readings Grolemund, G., Wickham, H. (2016) R for Data Science, Chapters 3-5, O’Reilly Media. 2.5 Ways of measuring stuff Previously we said that data are the raw material for modeling, but we did not say precisely what we meant by ‘data’. You probably already have a working understanding of what ‘data’ means, but nonetheless lets begin with a definition. According to Mirriam-Webster, data are: factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation As an aside, it is interesting to note that Tukey’s classic Exploratory Data Analysis (Tuckey 1977) does not define ‘data’ in the glossary! Measurement theory is a branch of mathematics concerned with observing the facts about something. It is important to note that measurements are not the same as the thing being measured; however, we would like the measurements to be a reasonably close approximation of the thing being measured - otherwise the measurements might be pretty useless, an inadequate way to learn anything valuable from the thing we are measuring. One fundamental contribution of the scientific method has been to produce standardized ways of measuring things. How would you measure the following things? The temperature at which water freezes. The tempreature at which nitrogen freezes. The length of a trip. Blood donations. Different brands of peanut butter. The value of two bedroom apartment. Someone’s opinion regarding taxes. Generally, there are multiple ways of measuring something, but not all of them are necessarily appropriate, partly because the scales of measurement may result in some loss of information. The interpretation of a measurement, as well, depends on what the scale is. Two broad scales of measurement are as follows: 2.5.1 Categorical Categorical measurements assign a label or category to the thing being measured. For example, a way to measure different brands of peanut butter could be to measure their sugar content, their fat content, their consistency, and so on, and in this way describe what makes each brand unique. A different way to do this would be to label one brand “Spooky” and another “Peter’s”. This has the effect of reducing a lot of information to a much simpler category. Is this loss of information inappropriate? Well, it really depends on what is the intended use of data! Categorical measurements are interesting because they may tell us something about the power of brands! Within the class of categorical variables there are two distinct scales of measurement: Nominal scale. When the categories do not follow a natural order. For example, there is no reason to say that “Spooky” brand precedes “Peter’s” or vice versa. Similarly, when measuring modes of travel “walking” is not intrinsically higher or lower or better or worse than “cycling” or “riding bus”. Ordinal scale. When the categories follow a natural or logical sequence. A common way of measuring opinions is by means of the Likert scale, which classifies responses for instance as “strongly disagree”, “disagree”, “neutral”, “agree”, “strongly agree”. In this case, it is sensible to order the responses, since “strongly agree” is probably closer to “agree” than to “strongly disagree”. Responses of this type are often represented by numbers, say, from 1 to 5. It is a mistake to treat the measurements as numbers instead of lables. When treated as numbers there is a temptation to thing of the difference between 4 and 5 and the difference between 3 and 4 as being equivalent, when in fact the strength of disagreement could be stronger than the strength of agreement. In other words, the interval between “strongly disagree” and “disagree” may not be the same as “agree” and “strongly agree”. With ordinal scales we do not know that, all that we know is that they measure a different opinion. Sometimes, different measurement scales might represent different behavioral mechanisms, as Bhat and Pulugurta discuss in their comparison of categorical and ordinal measurements for vehicle ownership (Bhat and Pulugurta 1998). 2.5.2 Quantitative Quantitative measurements assign a number to an attribute, and the number quantifies the presence of the attribute. Within this class of variables, there are also two ways of measuring things. Interval scale. A quantity can be assigned to an attribute, the values follow an order, and their differences can be computed and remain constant. Temperature is typically measured in interval scale. The difference between \\(10\\,^{\\circ}\\mathrm{C}\\) and \\(11\\,^{\\circ}\\mathrm{C}\\) is the same as the difference between \\(25\\,^{\\circ}\\mathrm{C}\\) and \\(26\\,^{\\circ}\\mathrm{C}\\). The intervals are meaningful. However, \\(0\\,^{\\circ}\\mathrm{C}\\) does not imply the absence of temperature! Which is why measurements in Celsius and Farenheit do not coincide at zero. The lack of a natural zero for these scales means that the ratios between two values are not meaningful: \\(4\\,^{\\circ}\\mathrm{C}\\) is not twice as hot as \\(2\\,^{\\circ}\\mathrm{C}\\), and \\(-12\\,^{\\circ}\\mathrm{C}\\) is not four times as cold as \\(-3\\,^{\\circ}\\mathrm{C}\\). Ratio scale. When there is an absolute value of zero to the thing being measured (to indicate absence!), attributes can be measured in a ratio scale. This combines the features of the previous scales of measurement: a number is esentially a label that follows a logical order and with differences that are meaningful. In addition to that, the ratios of variables are meaningful. For example, twenty dollars are twice as valuable as ten, and zero is the absence of value. Weight is a way of measuring mass, and zero is the absence of mass. Two hundred kilograms is twice as much as one hundred kilograms. It is important to understand the different scales of measurement to be able to choose the appropriate tools for each. More on this below. But first, lets bring some actual data to play with. 2.6 Importing data There are several different ways of importing data in R. For this example, we will use part of a dataset that was analyzed by Whalen et al. (Whalen, Páez, and Carrasco 2013). At the very beginning, it is good practice to clear the workspace, to ensure that there are no extraneous items there. The workspace is where objects reside in memory during a session with R. The function for removing variables from the workspace is rm(). Another useful function is `ls, which retrieves a list of things in the workspace. So essentially we are asking R to remove all things in the workspace: rm(list = ls()) Once that the workspace is empty, we can proceed to load a few packages that are useful. Packages are the basic units of reproducible code in the R multiverse. Packages allow a developer to create a self-contained unit of code that often is meant to achieve some task. For instance, there are packages in R that specialize in statistical techniques, such as cluster analysis, visualization, or data manipulation. Some packages can be miscellaneous tools, or contain mostly datasets. Packages are a very convenient way of maintaining code, data, and documentation, and also of sharing all these resources. Packages can be obtained from different sources (including making them!). One of the reasons why R has become so successful is the relative facility with which packages can be distributed. A package that I use frequently is called tidyverse. The tidyverse is a collection of functions for data manipulation, analysis, and visualization. This package can be downloaded and installed in your personal library of R packages by using the function install.packages, as follows: install.packages(&quot;tidyverse&quot;) The function install.packages retrieves packages from the Comprehensive R Archive Network, or CRAN for short. CRAN is a collection of sites (accessible via the internet) that carry identical materials for distribution for R. Installing a package is similar to acquiring a book for your library. The book is there, but if you want to use it, you need to bring it to your workspace, so to speak. The function for retrieving a package from the library is naturally enough library(). For the moment, we need the following packages. If you have not done so, take a moment to install them, as illustrated in the previous chunk. library(tidyverse) library(readr) #library(caret) library(mlogit) Ocassionally there are messages displayed when loading a package. These messages are informative (they ask you to cite them in a certain style) or may give you warnings, for instance that identically named functions exist in several packages. The function that we need to read the sample dataset is read_csv(), which is part of the tidyverse package. Note that you can name the value (or output) of a function by using &lt;-. In this case, we wish to read an external file, and assign the results to an object called mc_mode_choice: mc_mode_choice &lt;- read_csv(&quot;Commute Mac.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. It is possible to quickly examine the contents of the object by means of the function head(), which prints the top few rows of the object, if appropriate. For example: head(mc_mode_choice) ## # A tibble: 6 x 39 ## RespondentID choice avcycle avwalk avhsr avcar timecycle timewalk ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 566872636 3 0 1 1 0 6.21 21.3 ## 2 566873140 3 0 1 1 1 3.73 12.8 ## 3 566874266 3 0 0 1 1 100000 100000 ## 4 566874842 2 1 1 1 0 5.83 20 ## 5 566881170 2 1 1 1 0 5.83 20 ## 6 566907438 2 0 1 1 0 100000 10 ## # ... with 31 more variables: accesshsr &lt;dbl&gt;, waitingtimehsr &lt;dbl&gt;, ## # transfer &lt;dbl&gt;, timehsr &lt;dbl&gt;, timecar &lt;dbl&gt;, parking &lt;dbl&gt;, ## # vehind &lt;dbl&gt;, owncycle &lt;dbl&gt;, gender &lt;dbl&gt;, work &lt;dbl&gt;, visa &lt;dbl&gt;, ## # age &lt;dbl&gt;, solo &lt;dbl&gt;, shared &lt;dbl&gt;, family &lt;dbl&gt;, child &lt;dbl&gt;, ## # primary_caregiver &lt;dbl&gt;, LAT &lt;dbl&gt;, LONG &lt;dbl&gt;, DAUID &lt;dbl&gt;, ## # mhi &lt;dbl&gt;, dwell_den &lt;dbl&gt;, lum &lt;dbl&gt;, st_den &lt;dbl&gt;, inter_den &lt;dbl&gt;, ## # SF_P_ratio &lt;dbl&gt;, side_den &lt;dbl&gt;, Shelters_SD &lt;dbl&gt;, Shelters_D &lt;dbl&gt;, ## # Shelters_A &lt;dbl&gt;, Shelters_SA &lt;dbl&gt; Here we can see that what we just read is a table with several variables: an id, a variable called choice, some variables for time, etc. Hopefully, when reading data there is also a metadata file, a data dictionary or something that defines what the data are. For example, what does it mean for choice to be “3” or “1”? Was time measured in hours, seconds, minutes, or something else? These variables will be described below. Before that, however, we can use a different function to get further insights into the contents of the table by means of the summary() function: summary(mc_mode_choice) ## RespondentID choice avcycle avwalk ## Min. :566872636 Min. :1.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:567814188 1st Qu.:2.000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :568682048 Median :2.000 Median :0.0000 Median :1.0000 ## Mean :570566454 Mean :2.618 Mean :0.2747 Mean :0.6613 ## 3rd Qu.:574925212 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :587675235 Max. :4.000 Max. :1.0000 Max. :1.0000 ## avhsr avcar timecycle timewalk ## Min. :0.0000 Min. :0.0000 Min. : 0.29 Min. : 1.00 ## 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.: 3.79 1st Qu.: 13.66 ## Median :1.0000 Median :1.0000 Median : 5.83 Median : 20.00 ## Mean :0.9608 Mean :0.5472 Mean : 34014.86 Mean : 37364.73 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:100000.00 3rd Qu.:100000.00 ## Max. :1.0000 Max. :1.0000 Max. :100000.00 Max. :100000.00 ## accesshsr waitingtimehsr transfer timehsr ## Min. : 0.00 Min. : 0.00 Min. : 0 Min. : 1.00 ## 1st Qu.: 2.48 1st Qu.:10.23 1st Qu.: 0 1st Qu.: 4.75 ## Median : 6.21 Median :10.23 Median : 0 Median : 10.00 ## Mean :11.06 Mean :10.25 Mean : 3925 Mean : 3940.57 ## 3rd Qu.:12.42 3rd Qu.:10.23 3rd Qu.: 1 3rd Qu.: 25.00 ## Max. :62.11 Max. :50.00 Max. :100000 Max. :100000.00 ## timecar parking vehind owncycle ## Min. : 1 Min. :0.00000 Min. :0.0000 Min. :0.0000 ## 1st Qu.: 8 1st Qu.:0.00000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median : 30 Median :0.00000 Median :0.0000 Median :0.0000 ## Mean : 45283 Mean :0.08358 Mean :0.2565 Mean :0.4513 ## 3rd Qu.:100000 3rd Qu.:0.00000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :100000 Max. :1.00000 Max. :1.0000 Max. :1.0000 ## gender work visa age ## Min. :0.0000 Min. :0.000 Min. :0.0000 Min. :17.00 ## 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.:1.0000 1st Qu.:20.00 ## Median :0.0000 Median :0.000 Median :1.0000 Median :21.00 ## Mean :0.4012 Mean :0.492 Mean :0.9622 Mean :22.08 ## 3rd Qu.:1.0000 3rd Qu.:1.000 3rd Qu.:1.0000 3rd Qu.:23.00 ## Max. :1.0000 Max. :1.000 Max. :1.0000 Max. :60.00 ## solo shared family child ## Min. :0.0000 Min. :0.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :0.0000 Median :1.000 Median :0.0000 Median :0.0000 ## Mean :0.1272 Mean :0.625 Mean :0.2478 Mean :0.2115 ## 3rd Qu.:0.0000 3rd Qu.:1.000 3rd Qu.:0.0000 3rd Qu.:0.0000 ## Max. :1.0000 Max. :1.000 Max. :1.0000 Max. :1.0000 ## primary_caregiver LAT LONG DAUID ## Min. : 0 Min. :43.08 Min. :-80.09 Min. :35250031 ## 1st Qu.:100000 1st Qu.:43.25 1st Qu.:-79.92 1st Qu.:35250540 ## Median :100000 Median :43.26 Median :-79.91 Median :35250670 ## Mean : 75218 Mean :43.25 Mean :-79.90 Mean :35250612 ## 3rd Qu.:100000 3rd Qu.:43.26 3rd Qu.:-79.90 3rd Qu.:35250677 ## Max. :100000 Max. :43.28 Max. :-79.64 Max. :35250970 ## mhi dwell_den lum st_den ## Min. : 0.000 Min. : 0.0 Min. :0.0000 Min. : 0.00 ## 1st Qu.: 4.577 1st Qu.: 488.7 1st Qu.:0.2808 1st Qu.:10.36 ## Median : 5.491 Median : 950.0 Median :0.4501 Median :14.29 ## Mean : 6.168 Mean : 1373.0 Mean :0.4183 Mean :13.27 ## 3rd Qu.: 7.556 3rd Qu.: 1688.6 3rd Qu.:0.5038 3rd Qu.:16.18 ## Max. :14.595 Max. :45209.9 Max. :0.9081 Max. :25.22 ## inter_den SF_P_ratio side_den Shelters_SD ## Min. : 0.00 Min. :0.0000 Min. : 0.00 Min. :0.00000 ## 1st Qu.: 25.82 1st Qu.:0.2309 1st Qu.:18.19 1st Qu.:0.00000 ## Median : 41.04 Median :0.2709 Median :22.63 Median :0.00000 ## Mean : 52.09 Mean :0.2625 Mean :24.18 Mean :0.04433 ## 3rd Qu.: 73.08 3rd Qu.:0.3134 3rd Qu.:35.70 3rd Qu.:0.00000 ## Max. :645.86 Max. :0.8808 Max. :59.41 Max. :1.00000 ## Shelters_D Shelters_A Shelters_SA ## Min. :0.0000 Min. :0.0000 Min. :0.00000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.00000 ## Median :0.0000 Median :0.0000 Median :0.00000 ## Mean :0.2289 Mean :0.3576 Mean :0.02762 ## 3rd Qu.:0.0000 3rd Qu.:1.0000 3rd Qu.:0.00000 ## Max. :1.0000 Max. :1.0000 Max. :1.00000 This function will print a set of summary statistics for the variables in the table. The statistics are appropriate for the scale of measurement of the variable - that is, the way the variable is coded. Presently, all summary statistics are calculated for quantitative variables. We don’t know if this makes sense, until we know what the variables are supposed to measure. For example, the variable choice measures the use of one mode of transportation. There are four values in this scale: 1 through 4, with each indicating one of “Cycle”, “Walk”, “Car”, or “HSR” (the local transit agency in Hamilton, ON, Canada, where these data were collected). Check the results of the summary. What does it mean to say that the mean of choice is 2.618? Does this number make sense? 2.7 Data Classes in R To understand why 2.618 of mode of transportation is not an appropriate summary measure for the variable mode, we need to know that R can work with different data classes, which include the following: Numerical Character Logical Factor The ability to store information in different forms is important, because is allows R to distinguish what kind of operations are appropriate for a certain variable. Consider the following example (using indexing): mc_mode_choice$choice[1] - mc_mode_choice$choice[4] ## [1] 1 Lets unpack what the chunk above did. First, we call our table mc_mode_choice. The string sign $ is used to reference columns in the table. Therefore, we asked R to go and look up the column choice in the table mc_mode_choice. Finally, the value between square brackets [] asks R to retrieve a value out of the column, in this example the first value in that column and then the fourth value. This system of referring to elements in tables is called indexing. Most computer languages use it, but the syntax is different. Again: $ refers to a column, and [] is used to call values in that column. As we can see, the difference between the two values retrieved is \\(1\\). But what is the meaning of “cycle” minus “walk”, for instance? In reality, the variable choice was measured as a nominal variable: it just corresponds to a label indicating what mode was chosen by a respondent. But R does not know this. Before R can treat it as a nominal variable, the numbers need to be converted to a factor. Factors are the way R stores categorical variables (both nominal and ordinal). To convert the variable choice to a factor, we use the factor() function: mc_mode_choice$choice &lt;- factor(mc_mode_choice$choice, labels = c(&quot;Cycle&quot;, &quot;Walk&quot;, &quot;HSR&quot;, &quot;Car&quot;)) In the chunk above, we ask R to replace the contents of mc_mode_choice$choice with the value (output) of the function factor. Factor takes the contents of mc_mode_choice$choice and converts to a factor with labels as indicated by the argument labels = (the function c() is used to concatenate several values). Lets summarize the result, by using the summary() function but only for this variable: summary(mc_mode_choice$choice) ## Cycle Walk HSR Car ## 48 711 336 281 Now the summary is appropriate categorical variable, and is a table of frequencies: as seen there, there were 48 respondents who chose “Cycle”, 711 who chose “Walk”, and so on. What if we tried to calculate the difference? mc_mode_choice$choice[1] - mc_mode_choice$choice[4] ## Warning in Ops.factor(mc_mode_choice$choice[1], mc_mode_choice$choice[4]): ## &#39;-&#39; not meaningful for factors ## [1] NA The message indicates that the operation we tried to perform is not meaningful for factors. As long as R knows the appropirate measurement scale for your variables, it will try to steer you away from doing silly things with them. Other variables included in this table are for time. These variables measure the duration in minutes (actual or imputed) for trips by different modes. For example, timecycle is the duration of a trip by bicycle for the journey reported by the respondent. Lets summarize this variable again: summary(mc_mode_choice$timecycle) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.29 3.79 5.83 34014.86 100000.00 100000.00 Notice that the shortest trip by bicycle would be less than a minute long, whereas the maximum is \\(100,000\\) minutes long. Wait, what? That is over \\(1,600\\) hours long. Is that even possible? In fact, no, it is not. The reason for these values is that when the original data were coded, whenever a respondent said that cycling was not a mode that was available to them, the time was coded as a very large and distinctive value. There were no trips taking \\(100,000\\) minutes, this is just a code for “information not available”. One problem with this manner of coding is that R does not know that the information is actually missing, but rather thinks it is a legitimate quantity. As a consequence, the mean is tens of thousands of minutes, despite the fact that half of all trips by bicycle were measured at less than 6 minutes long (see the median). Next we will see a way to address this. One last thing before doing so: you can check the class an object with the function class class(mc_mode_choice$choice) ## [1] &quot;factor&quot; class(mc_mode_choice$timecycle) ## [1] &quot;numeric&quot; 2.8 More on indexing and data manipulation Indexing is a way of making reference to elements in a data object. There are numerous indexing methods in R that are appropriate for specific objects. Tables such as mc_mode_choice (called data frames) can be indexed in a few different ways. For example the next three chunks are equivalent in that they call the second column (choice) and in that column the second element: mc_mode_choice[2, 2] ## # A tibble: 1 x 1 ## choice ## &lt;fct&gt; ## 1 HSR mc_mode_choice$choice[2] ## [1] HSR ## Levels: Cycle Walk HSR Car mc_mode_choice[[&quot;choice&quot;]][2] ## [1] HSR ## Levels: Cycle Walk HSR Car It is also possible to index by ranges of values. For example, the next chunks retrieves rows 2 to 5 from columns 7 and 8: mc_mode_choice[2:5, 7:8] ## # A tibble: 4 x 2 ## timecycle timewalk ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.73 12.8 ## 2 100000 100000 ## 3 5.83 20 ## 4 5.83 20 Indexing is useful to subset data selectively. For example, we know that travel times coded as \\(100,000\\) are actually cases where the corresponding mode was not available. Lets say that we wanted to summarize travel time by bicycle but without those cases. We can use logical statements when indexing. We could tell R to retrieve only those values that meet a certain condition. In the next chunk, we save the results of this to a new variable: time.Cycle.clean &lt;- mc_mode_choice$timecycle[mc_mode_choice$timecycle != 100000] where != is R for not. In other words, “find all values not 100000, and retrieve them”. The result of this is a numeric object: class(time.Cycle.clean) ## [1] &quot;numeric&quot; If we summarize this object now: summary(time.Cycle.clean) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2914 2.9141 4.3711 4.8957 5.8282 45.0000 The summary statistics are much more sensible: the longest trip by bicycle was measured at 45 minutes, and the mean trip at less than 5 minutes. Indexing is a powerful technique, but can be cumbersome (mc_mode_choice$timecycle[mc_mode_choice$timecycle != 100000]!). The package dplyer (part of the tidyverse) provides a grammar for data manipulation that is more intuitive. We will explore three of its elements here, namely the pipe operator (%&gt;%), select, and filter. Suppose that we wanted to select two of the time variables, for cycling and walking, and wanted to retrieve only values other than the offending \\(100,000\\), and save these values in a new object called time.Active.clean. In the grammar of dplyr, this is done as follows: time.Active.clean &lt;- mc_mode_choice %&gt;% select(c(&quot;timecycle&quot;, &quot;timewalk&quot;)) %&gt;% filter(timecycle != 100000 &amp; timewalk != 100000) In natural language this would be something like “take mc_mode_choice and select columns timecycle and timewalk; pass the result to filter and retrieve all rows that meet the conditions timecycle != 100000 AND timewalk != 100000”. The verb select is used to select columns from a data frame, and the verb filter to filter rows. The alternative, using indexing would look something like this: time.Active.clean.the.hard.way &lt;- mc_mode_choice[mc_mode_choice$timecycle != 100000 &amp; mc_mode_choice$timewalk != 100000, 7:8] The expression becomes more convoluted and not as easy to read. It is also easier to make mistakes when writing it. Compare the summaries of the two data frames, to make sure that they are identical: summary(time.Active.clean) ## timecycle timewalk ## Min. : 0.2914 Min. : 1.00 ## 1st Qu.: 2.9141 1st Qu.:10.00 ## Median : 4.3711 Median :15.00 ## Mean : 4.5852 Mean :16.10 ## 3rd Qu.: 5.8282 3rd Qu.:20.00 ## Max. :17.4845 Max. :62.11 summary(time.Active.clean.the.hard.way) ## timecycle timewalk ## Min. : 0.2914 Min. : 1.00 ## 1st Qu.: 2.9141 1st Qu.:10.00 ## Median : 4.3711 Median :15.00 ## Mean : 4.5852 Mean :16.10 ## 3rd Qu.: 5.8282 3rd Qu.:20.00 ## Max. :17.4845 Max. :62.11 The grammar of data manipulation in dplyr is a powerful way of working with data in an intuitive way. We will find other aspects of this, but for the time being you are welcome to consult more about dplyr here 2.9 Visualization The last item in this section is related to visualization. Humans are very much visual creatures, and much can be learned from seeing the data. For example, the data frame, in essence a table, is informative in many ways, but not particularly conducive to observe trends or regularities in the data. The summary statistics are also informative, but partial, and do not convey information to the same effect as a statistical plot. Take the following list of summary statistics: summary(time.Active.clean) ## timecycle timewalk ## Min. : 0.2914 Min. : 1.00 ## 1st Qu.: 2.9141 1st Qu.:10.00 ## Median : 4.3711 Median :15.00 ## Mean : 4.5852 Mean :16.10 ## 3rd Qu.: 5.8282 3rd Qu.:20.00 ## Max. :17.4845 Max. :62.11 Now, compare to the following plot: ggplot(data = time.Active.clean) + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) + geom_area(aes(x = timewalk), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;yellow&quot;, color = &quot;yellow&quot;, alpha = 0.6) The plot above was created using a package called ggplot2, also part of tidyverse. This package implements a grammar of graphics, and offers a very flexible way of creating plots in R. ggplot2 works by layering a series of objects, beginning with a blank plot, to which we can add things. The command to create a plot is ggplot(). This command accepts different arguments. For instance, we can pass data to it in the form of a data frame. We can also indicate different aesthetic values, that is, the things that we wish to plot. None of this is plotted, though, until we indicate which kind of geom or geometric object we wish to plot. For instance, you can see above that to create the figure we used geom_area. This geom is essentially a smoothed histogram. Lets break down these instructions. First we ask ggplot2 to create a plot that will use the data frame time.Active.clean. We will name this object p: p &lt;- ggplot(data = time.Active.clean) Notice how ggplot2 creates a blank plot, but it has yet to actually render any of the population information in there: p We have yet to tell ggplot2 what the x axis is, what the y axis is, what should be plotted in the x axis, and so on. We layer elements on a plot by using the + sign. It is only when we tell the package to add some geometric element that it renders something on the plot. In the previous case, we told ggplot2 to use the geom_area to create a smoothed histogram. Next, we need to indicate which aes (short for aesthetics) we wish to plot. The aesthetics map aspect of the dataset to the plot. For instance, by saying that the aesthetics include something for x, we tell ggplot2 that that something should be mapped to the x axis (in this case one of the time variables in the data frame). The second argument is stat = 'bin', which indicates that there is a statistical operation that happens, namely the data are binned for smoothing (small bins lead to less smoothing, large bins lead to more smoothing; try it!). After playing around with a few bin values, I selected 5: p + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5) To improve the appearence of the plot, we also asked that the geom be rendered using a named color (blue for the color of the line, and also blue for the fill), and that it be transparent (the argument alpha controls opacity; a value of zero is transparent, a value of 1 is solid): p + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) The final plot was obtained by layering a second geom (in yellow) for a different variable (time by walking), so that we could compare them: ggplot(data = time.Active.clean) + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) + geom_area(aes(x = timewalk), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;yellow&quot;, color = &quot;yellow&quot;, alpha = 0.6) Notice that the x axis is labeled as “timecycle” despite the fact that the plot also includes time by walking. This can be fixed by changing the label as follows: ggplot(data = time.Active.clean) + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) + geom_area(aes(x = timewalk), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;yellow&quot;, color = &quot;yellow&quot;, alpha = 0.6) + xlab(&quot;Time (in minutes)&quot;) What do we learn from this plot? Would it have been possible to learn the same from the summary statistics? Which was more effective, the plot or the summary statistics? The plot above is an example of a univariate plot, since it is created to display the distribution of a single variable, not the way two or more variables relate. Imagine now that you would like to see how mode choice and sidewalk density at the place of residence relate. An appropriate statistical plot for two variables, one of which is nominal (choice) and another that is continuous (side_den), is the boxplot. Before creating the plot, lets summarize these two variables (notice the use of the pipe operator): mc_mode_choice %&gt;% select(c(&quot;choice&quot;, &quot;side_den&quot;)) %&gt;% summary() ## choice side_den ## Cycle: 48 Min. : 0.00 ## Walk :711 1st Qu.:18.19 ## HSR :336 Median :22.63 ## Car :281 Mean :24.18 ## 3rd Qu.:35.70 ## Max. :59.41 Sidewalk density is measured in \\(km/km^2\\). Lets create the boxplot next. We begin by defining a ggplot2 object with the data frame and aesthetics that we wish to use. In this case, we want to plot the categorical variable in the x axis and the quantitative variable in the y axis: ggplot(data = mc_mode_choice, aes(x = choice, y = side_den)) + geom_boxplot() What do we learn from this plot? Could we have derived a similar insight from the summary statistics? There are many different geoms that can be used in ggplot2. You can always consult the help/tutorial files by typing ??ggplot2 in the console. See: ??ggplot2 You can also check the ggplot2 Cheat Sheet for more information on how to use this package. A last note. Many other visualization alternatives (for instance, Excel) provide point-and-click functions for creating plots. In contrast, ggplot2 in R requires that the plot be created by meticulously instructing the package what to do. While this is more laborious, it also means that you have complete control over the creation of plots, which in turn allows you to create more flexible and creative visuals. 2.10 Exercise 2.10.1 Questions Define “model”. Why are models on a 1-to-1 scale undesirable? Invoke dataset Mode from package mlogit. To do this you need to first load the package. Once you have done so, usage of datasets is as follows: data(&quot;Mode&quot;) This is a dataset with choices about mode of transportation. Describe this dataset. How many variables are there and of which type (i.e., categorical/quantitative)? How many different modes of transportation are in this data set? What is the most popular mode? What is the least popular mode? In general, what is the most expensive mode? The least expensive? Create a plot showing the univariate distributions of time by car and time by bus. Discuss. How do choices relate to cost by the different modes? References "],
["chapter-2.html", "Chapter 3 Fundamental concepts 3.1 Why modelling choices? 3.2 How to use this note 3.3 Learning objectives 3.4 Suggested readings 3.5 Preliminaries 3.6 Utility maximization 3.7 What about those random terms? 3.8 Probability distribution functions (PDFs) 3.9 A simple random utility discrete choice model 3.10 Other choice mechanisms 3.11 Exercise", " Chapter 3 Fundamental concepts “Ignorance gives one a large range of probabilities.” — George Eliot 3.1 Why modelling choices? In 2 we discussed in a general fashion the use of models. There, we argued that modelling is an activity that helps us to isolate in a systematic way certain aspects of a process or thing, by way of abstraction and generalization. There are many kinds of models: analog (like sculptures, maquettes, scale models), conceptual (like mental maps), and mathematical/statistical models. The raw materials of mathematical/statistical models are observations about the process or thing of interest, usually measurements that provide data. The tools are the statistical and mathematical techniques used to convert data into information. And the technical expertise is the knowledge and ability of the modeler to use the appropriate tools to the data, in order to extract as much information as possible, given the characteristics of the data and the process or thing. Modelling choices is simply a specialized field in the much broader field of mathematical and statistical modelling. The task of modelling choices is in many way similar to the modelling of limited-dependent and qualitative variables in statistics (Maddala 1983), however it is distinguished from models in that field by a strong behavioral foundations. Indeed, where statistical models deal with probabilities of an item of interest being in a certain state, choice modelling deals with the probability of an agent choosing an alternative. This is a subtle but important difference that we will highlight in due course. For the time being, it is sufficient to say that to model choices we need a conceptual model first on which to build the rest of the apparatus required for applied choice modelling. Before delving into the technical details, we can pause for a philosophical moment to think about human behavior and decision-making. There are different perspectives on human behavior. Some schools of thought affirm that events are predetermined. A famous thought experiment Laplace’s Demon, is as follows: We may regard the present state of the universe as the effect of its past and the cause of its future. An intellect [Laplace’s Deman] which at a certain moment would know all forces that set nature in motion, and all positions of all items of which nature is composed, if this intellect were also vast enough to submit these data to analysis, it would embrace in a single formula the movements of the greatest bodies of the universe and those of the tiniest atom; for such an intellect nothing would be uncertain and the future just like the past would be present before its eyes. — Pierre Simon Laplace, A Philosophical Essay on Probabilities Some schools of sociological thought (see for example the discussion in Degenne and Fors‚ 1999) see social interactions as a predominant, and even a determinant, factor that affects behavior. In their more extreme form, structuralism views social networks as structures that limit the ability of the individual to exercise independent agency, and therefore determine behavior. Laplace’s Demon and other forms of causal determinism assume that all preceding events set the conditions for present and future events via immutable rules. Nowadays determinism is not seriously considered for several reasons, of which it is useful to highlight two: The practical impossibility of knowing at a certain moment all forces that set nature in motion, as well as the positions of all of nature’s items. With respect to physical processes, the uncertainty principle of quantum physics put paid to the notion that we can know all that there is to know about the fundamental items of nature. In terms of human behavior, this is complicated by the inability of an external observer to know the state of mind of a person who acts. On the other hand, it is possible that existing social structures influence behavior (and there is now a wealth of literature that makes this argument; see A. Páez and Scott 2007). However, social determinism seems as implausible as physical determinism, for similar reasons: the difficulties of knowing the state of a system with complete omniscience. The assumption of immutable rules. This assumption has been challenged by studies that suggest some important physical constants can change with the age of the universe (Webb et al. 2001). In terms of human behavior, the assumption is even more problematic, if for no other reason that humans can in general act in a contrarian way simply to demonstrate that there are no immutable social rules. This is only one of many reasons why behavioral detection (for instance, in airports; Kirschenbaum 2013) is problematic: if one knows the rules used for profiling, acting otherwise renders profiling ineffective. Does this mean that the state of the universe is not determined by the past? Not at all. For all we know, from the perspective of a hypothetical all-knowing being, it is. However, in practical terms, and for the reasons described briefly above, we humble non-all-knowing beigns, cannot rely on determinism for making statements about the state of the universe. In particular, we will make a distinction that is useful as part of developing a conceptual model of choice-making: 1) that there is an observer who typically lacks all relevant information about a choice process (let alone about the state of the universe); and 2) that the rules of decision making are not completely known and/or humans can, for idiosincratic reasons, alter them at whim. 3.2 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Well, hello there, Juan de Dios!&quot;) ## [1] &quot;Well, hello there, Juan de Dios!&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 3.3 Learning objectives In this practice, you will learn about: Choice mechanisms: Utility maximization. Probabilities and integration. How to derive a simple choice model. Other choice mechanisms. 3.4 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapter 3, MIT Press. Hensher, D.A., Rose, J.M., Greene, W.H (2005) Applied Choice Analysis: A Primer, Chapter 3, Cambridge University Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 7, John Wiley and Sons. 3.5 Preliminaries Load the packages used in this section: library(tidyverse) library(evd) 3.6 Utility maximization We will begin by defining a conceptual model of choice based on neo-classical economics, fundamentally consumer choice. The conceptual framework in neo-classical economics is based on the concept of utility. What is utility? In simple terms, utility is a summary indicator of the pleasure, usefulness, enjoyment, or attractiveness associated with making a choice (for instance, buying a new phone). Lets begin by positing a very simple choice situation, in which a decision maker chooses between one of two different alternatives. These alternatives constitute the choice set, and provide the context for the decision-making process. Imagine then that this simple choice example is as follows: Alternative 1: Do nothing (keep using current phone) Alternative 2: Buy new phone (for simplicity, think of a generic model). Further, assume that each alternative can be described by means of a vector of attributes \\(X\\) as follows: \\[ X = [x_1, x_2, \\dots, x_k] \\] The attributes describe each alternative in a way that is relevant to the decision maker. In the present example, two relevant attributes are the cost of each alternative and the characteristics of the current and new phones, for instance, their download speeds. In this way, the two choices can be described by their attributes as follows: \\[ \\begin{array}{cc} \\text{Do-Nothing:} &amp; X_A = [\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{New-Phone}}]\\\\ \\text{New-Phone:} &amp; X_B = [\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}]\\\\ \\end{array} \\] If the decision-maker currently owns a phone that is fully paid, the out-of-pocket cost of doing nothing would be zero. Buying a new phone, on the other hand, would have a positive (and possibly substantial cost). The new phone, on the other hand is faster than the older, currently owned model. The decision-maker can likewise be described by a vector of attributes, say \\(Z\\): \\[ Z = [z_1, z_2, \\dots, z_k] \\] Suppose for example, that decision-maker \\(i\\) can be described in terms of their income, as follows: \\[ Z_i = [\\text{income}_i] \\] The attributes of the decision maker help to capture heterogeneities in behavior: for instance, a decision-maker with a lower income may be more sensitive to cost, since buying a new phone is relatively more expensive. A utility function is a way of summarizing the attributes of the choices and the attributes of the decision-makers in a single quantity, which is what the decision-maker is trying to maximize. We assume that each course of action gives this consumer a level of utility: in other words, he will be more or less happy with each alternative, taking into account their characteristics and his own condition or status: \\[ \\begin{array}{c} U_{i, \\text{Do-Nothing}} = U(\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{Do-Nothing}}, \\text{ income}_i)\\\\ U_{i, \\text{New-Phone}} = U(\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}, \\text{ income}_i)\\\\ \\end{array} \\] Notice that the utility function is specific to a decision maker \\(i\\) and an alternative. Here we define a decision-making rule. The decision-maker considers the utility of the alternatives, and chooses the one that gives the highest utility. In other words decision-maker \\(i\\) will choose to keep the current phone if: \\[ U_{i,\\text{Do-Nothing}} &gt; U_{i,\\text{New-Phone}}, \\] If the reverse is true, then the decision-maker will choose to buy a new phone (in the case of a tie, the decision-maker is indifferent between the two alternatives). We assume that decision-makers are rational and that they do an analysis of the costs and the benefits of each alternative before making the choice. The analyst, however, may fail to observe all aspects of the decision making process. For instance, a decision-maker may need faster speeds because she lacks internet at home. Or a decision-maker just received a large gift from a relative. Or younger people may be more willing to buy new phones than older people. The analyst may observe a decision-maker with a relatively low income buying a new phone. While income alone would have suggested that the decision-maker would be better off keeping the old phone, the analyst has no way of knowing the idiosyncratic factor of the gift. For this reason, it is convenient to decompose the utility into 1) a systematic component, that is, the part that explains the decision-makers’ response to the attributes of the alternative; and 2) a random component, which captures other aspects of the decision making process that the analyst did not observe: \\[ \\begin{array}{c} U_A = V_A + \\epsilon_A\\\\ U_B = V_B + \\epsilon_B\\\\ \\end{array} \\] The random part of the function is called the random utility. If there was no uncertainty at all, if we knew precisely all there is to know about the decision-making process, we would have that \\(\\epsilon_{\\text{Do-Nothing}}\\) and \\(\\epsilon_{\\text{New-Phone}}\\). Accordingly, \\(U_{\\text{Do-Nothing}} = V_{\\text{Do-Nothing}}\\) and \\(U_{\\text{New-Phone}} = V_{\\text{New-Phone}}\\), and we could predict with complete certainty the choice. However, the presence of the random components means that we cannot be certain whether \\(U_A &gt; U_B\\). While this is unfortunate, the presence of the random terms does allow us to make a probabilistic statement, such as: \\[ P_{\\text{Do-Nothing}} = P(U_{\\text{Do-Nothing}} &gt; U_{\\text{New-Phone}}) \\] In other words, the probability of doing nothing equals the probability that the utility of doing nothing is greater than the utility of buying a new phone. After rearranging things, this is equivalent to: \\[ P_{\\text{Do-Nothing}} = P(V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} &lt; \\epsilon_{\\text{New-Phone}} - \\epsilon_{\\text{Do-Nothing}}) \\] The expression above is the foundation of random utility modelling. Before we make more progress, however, we have to answer an important question. 3.7 What about those random terms? library(tidyverse) library(evd) A probabilistic expression is clearly better than being unable to say anything at all regarding choices. To make this expression of practical use, we must assume some distribution for the random terms. Which means that we need to define some probability distribution function. 3.8 Probability distribution functions (PDFs) A candidate for a probability distribution function is any function that satisfies the following two conditions: \\[ \\begin{array}{l} \\text{Condition 1: }f(x)\\ge 0\\text{ for all }x\\\\ \\text{Condition 2: }\\int_{-\\infty}^{\\infty}f(x)dx=1\\\\ \\end{array} \\] These two conditions say that the function must take values of at least zero for the interval of \\(x\\) of interest, and that the area under the curve (that is what the integral means) must equal 1. Lets use an example to illustrate these properties. We will define the following function: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le -L \\\\ \\frac{1}{2L} &amp; \\quad -L&gt; x &gt; L \\\\ 0 &amp; \\quad x \\ge L \\\\ \\end{array} \\right. \\] This function is shown in Figure 3.1 below, with \\(L=2\\): # Define a function uniform &lt;- function(x, L) ifelse( x &lt;= -L, 0, ifelse( x &gt; -L &amp; x &lt;= L, 1/(2 * L), 0 )) # Define parameter L for the distribution L &lt;- 2 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -(L+1), to = L+1, by = 0.01)) %&gt;% mutate(y = uniform(x, L)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 3.1: Uniform distribution It is easy to see that the value of the function is always equal to or greater than zero. You can also verify that the area under the curve in this case is simply the area of the rectangle \\(b \\times h\\), where the base of the rectangle is \\(b = L - (-L)\\) and the height is \\(h=\\frac{1}{2L}\\): (L - (-L)) / (2 * L) ## [1] 1 If you are working with the R Notebook file, try changing the value of the parameter \\(L\\) to see what happens! What is the implication of larger values of L? And of smaller values of L? Since the function above satisfies the two necessary conditions, we conclude that it is a valid probability distribution function. In fact, it turns out to be a form of the uniform probability distribution function, which more generally is defined as: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le b \\\\ \\frac{1}{a - b} &amp; \\quad b&gt; x &gt; a \\\\ 0 &amp; \\quad x \\ge a \\\\ \\end{array} \\right. \\] Given a probability distribution function, we can calculate the probability of a random variable \\(x\\) being contained in a defined interval. For instance, the probability of \\(x &lt; -L\\) is zero, since the area under the curve in that case is zero. The probability of \\(x \\le X\\) is: \\[ \\int_{-\\infty}^{X}f(x)dx \\] In the case of our uniform distribution function, this is simply the area of the rectangle defined by the limits of the integral: # Define L L &lt;- 2 # Define an upper limit for calculating the probability X &lt;- 0 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -(L+1), to = L+1, by = 0.01)) %&gt;% mutate(y = uniform(x, L)) df_p &lt;- data.frame(x =seq(from = -(L+1), to = X, by = 0.01)) %&gt;% mutate(y = uniform(x, L)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + # Plot distribution function geom_area(data = df_p, fill = &quot;orange&quot;, alpha = 1) + # Plot area under the curve ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis What is the probability that \\(x \\le 0\\)? Try changing the upper limit to see what happens. How does the value of the area under the curve change? Associated to a probability distribution function we can define a cumulative distribution function \\(F_X(x) = P(x \\le X)\\), which maps how the probability changes as we change the interval. The cumulative distribution function of our uniform distribution is as follows: \\[ F(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le -L \\\\ \\frac{x + L}{2L} &amp; \\quad -L&gt; x &gt; L \\\\ 1 &amp; \\quad x \\ge L \\\\ \\end{array} \\right. \\] The cumulative distribution function for our uniform distribution appears in Figure 3.2: # Define the cumulative distribution function cuniform &lt;- function(x, L) ifelse( x &lt;= -L, 0, ifelse( x &gt; -L &amp; x &lt;= L, (x + L)/(2 * L), 1 )) # Define L L &lt;- 2 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -(L+1), to = L+1, by = 0.01)) %&gt;% mutate(y = cuniform(x, L)) # Plot ggplot(data = df, aes(x, y)) + geom_step(color = &quot;orange&quot;) + # Plot cumulative distribution function ylim(c(0, 1)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;F(x)&quot;) # Label the y axis Figure 3.2: Uniform cumulative distribution function As you can see, the probability of \\(x \\le -L\\) is zero, the probability of \\(x \\le 0\\) is 0.5, and the probability of \\(x \\le L\\) is one. Lets consider a second example, with a function as follows: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le 0 \\\\ 2x &amp; \\quad 0 &gt; x &gt; 1 \\\\ 0 &amp; \\quad x \\ge 1 \\\\ \\end{array} \\right. \\] This function is shown in Figure 3.3. # Define a function linear &lt;- function(x) ifelse( x &lt;= 0, 0, ifelse( x &gt; 0 &amp; x &lt;= 1, 2 * x, 0 )) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = 0, to = 1, by = 0.01)) %&gt;% mutate(y = linear(x)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + ylim(c(0, 2)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 3.3: Cubic distribution Clearly, \\(f(x) \\ge 0\\) for all values of \\(x\\) in the interval \\(0 \\le x \\le 1\\). We can verify that the area under the curve is 1. In this case the area is that of a triangle, i.e., \\(\\frac{b \\times h}{2}\\). Since the base of the triange is \\(b=1\\) and the height is \\(h=2\\), we see that the area under the curve is 1. Since this is a valid probability distribution function, we can use it to calculate the probability of \\(x \\le X\\) as above. The area under the curve when \\(x \\le X\\) is given by: \\[ F(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le 0 \\\\ \\frac{x \\times 2x}{2} = x^2 &amp; \\quad 0 &gt; x &gt; 1 \\\\ 1 &amp; \\quad x \\ge 1 \\\\ \\end{array} \\right. \\] The plot of the cumulative distribution function in this case is shown in Figure 3.4. # Define a function clinear &lt;- function(x) ifelse( x &lt;= 0, 0, ifelse( x &gt; 0 &amp; x &lt;= 1, x^2, 1 )) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -0.2, to = 1.2, by = 0.001)) %&gt;% mutate(y = clinear(x)) # Plot ggplot(data = df, aes(x, y)) + geom_step(color = &quot;orange&quot;) + # Plot cumulative distribution function ylim(c(0, 1)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 3.4: Linear cumulative distribution function It should be clear from the examples above that calculating probabilities is nothing more than finding the area under the curve of a function. When the function is relatively simple, as the uniform or the linear distributions that we used for the examples, calculating the areas is also straightforward, since the functions describe simple geometric shapes. When the function is more involved, that becomes less straighforward. For example, consider the following function: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le 0 \\\\ 4x^3 &amp; \\quad 0 &gt; x &gt; 1 \\\\ 0 &amp; \\quad x \\ge 1 \\\\ \\end{array} \\right. \\] This function is plotted in Figure 3.5. # Define a function cubic &lt;- function(x) ifelse( x &lt;= 0, 0, ifelse( x &gt; 0 &amp; x &lt;= 1, 4 * x^3, 0 )) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = 0, to = 1, by = 0.01)) %&gt;% mutate(y = cubic(x)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + ylim(c(0, 4)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 3.5: Cubic distribution Unlike the rectangle of the uniform distribution and the triangle of the linear distribution, the area under the curve for this distribution needs to be obtained by integration as follows (do not worry if ): \\[ \\int_{0}^{1}4x^3dx =4\\Big[\\frac{x^4}{4} \\Big]_{0}^{1} = \\Big[x^4 \\Big]_{0}^{1} = 1^4-0 = 1 \\] This shows that the function is a valid probability distribution function. However, the integration makes things more interesting, to say the least! Fortunately, for most applied discrete choice analysis we do not need to solve integrals manually (the monster minds have already done this for us!). The key here is to remember: given a valid probability distribution function the probability that a random variable \\(x \\le X\\) is the area under the curve in the interval \\(-\\infty\\) to \\(X\\). 3.9 A simple random utility discrete choice model We are now ready to deploy a probability distribution function to the probability of choosing an alternative. Returning to our binary choice example, lets assume that the difference in the random utility terms follows the uniform distribution with parameters \\(-L\\) and \\(L\\), that is: \\[ \\epsilon_{\\text{New-Phone}} - \\epsilon_{\\text{Do-Nothing}} \\sim U(-L, L) \\] The probability of choosing the “do-nothing alternative” is: \\[ P_{\\text{Do-Nothing}} = P(V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} &lt; \\epsilon_{\\text{New-Phone}} - \\epsilon_{\\text{Do-Nothing}}) \\] Since we know the probabilities of a random variable being less than a certain value in the uniform distribution, we have that: \\[ P_{\\text{Do-Nothing}} = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} \\le -L \\\\ \\frac{V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} + L}{2L} &amp; \\quad -L&gt; V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} &gt; L \\\\ 1 &amp; \\quad V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} \\ge L \\\\ \\end{array} \\right. \\] Lets unpack this expression. When the systematic utility of a new phone is greater than the systematic utility of doing nothing, the difference between these two terms is negative. The more negative this value is, the lower the probability of doing nothing. When the difference is more negative than \\(-L\\), the probability of doing nothing becomes zero. When the systematic utility of a new phone is identical to the systematic utility of doing nothing, the difference between these two terms is zero, in which case the probability of doing nothing is \\(0.5\\). In other words, there is a 50% chance that the decision maker will do nothing. Finally, when the systematic utility of a new phone is less than the systematic utility of doing nothing, the difference between these two terms is positive. The more the more positive this value is, the higher the probability of doing nothing. When the difference is greater than \\(L\\), the probability of doing nothing becomes one. Now, since the choice set is an exhaustive collection of courses of action, it follows that the probability of the two courses of action must add up to one (the decision-maker does nothing OR buys a new phone): \\[ P_{\\text{Do-Nothing}} + P_{\\text{New-Phone}} = 1 \\] This implies that once we know the probability of doing nothing, the probability of buying a new phone is simply the complement: \\[ P_{\\text{New-Phone}} = 1 - P_{\\text{Do-Nothing}} \\] These probabilities are a discrete choice model. In fact, this is called the linear probability model (see Ben-Akiva and Lerman 1985, 66–68). Other models can be obtained by selecting different probability distribution functions, as we will see in later chapters. The procedure followed here will be the same, even if the probability distribution function selected for the model is different: given a valid probability distribution function, and given the systematic utilities of the alternatives, it is possible to evaluate the probabilistic statement associated with the choice of an alternative. One final note, before discussing other choice mechanisms. The simple example used here was for binary choice, i.e., for a situation with only two alternatives. This was done for convenience of exposition, and we will see how the same ideas generalize for situations with more than two alternatives, that is, for multinomial choice situations. 3.10 Other choice mechanisms Utility maximization is only one of several plausible mechanisms. The utility functions assume that trade-offs among different attributes are possible; for example, the way the utility functions were formulated assumes that a decision-maker is willing to pay more for higher download speeds. While such trade-offs are plausible in many situations, other choice mechanisms could exist in other cases. Ortuzar and Willumsen (2011, Fourth Edition:241–43). For example, a user who is shopping for smartphones may have low tolerance for download speeds below a certain threshold, or may have a budget limit that prevents her from considering certain models. Some alternatives from the choice set may be eliminated or ranked based on some dominant attribute. This kind of choice mechanism is called lexicographic choice, or elimination by attributes. Another plausible choice mechanism is a form of satisficing behavior. Again, a user shopping for a smartphone might find a model that does not maximize her utility, but that is otherwise satisfactory. For example, the decision-maker may consider that the additional time spent finding an even better model is not worth her while, so she stops her search at a suboptimal point. Another choice mechanism seen recently in the literature is regret-minimization (???). In addition to these various mechanisms, it is possible that a decision-maker deploys combinations of them: for example, lexicographic choice to reduce the number of alternatives in a choice set, followed by utility maximization or regret minimization. Despite progress on these models, utility maximization remains the most widely used approach for the analysis of discrete choices. 3.11 Exercise Answer the following questions. 3.11.1 Questions Define utility. Describe in your own words the behavior described by utility maximization. What conditions are necessary for a function to be a valid probability distribution function? Consider the function shown in Figure 3.6. This is called the triangle or tent function. Figure 3.6: Triangle (or tent) function Show that the triangle function in the figure is a valid probability distribution function. Next, consider the following utility functions for two alternatives, namely \\(i\\) and \\(j\\): \\[ \\begin{array}{c} U_i = V_i + \\epsilon_i\\\\ U_j = V_j + \\epsilon_j\\\\ \\end{array} \\] Assume that the difference between the error terms below follows the triangle distribution: \\[ \\epsilon_q = \\epsilon_i - \\epsilon_j \\] Parting from the assumption above, derive a binary choice model for the probability of selecting alternative \\(j\\). References "],
["references.html", "References", " References "]
]
