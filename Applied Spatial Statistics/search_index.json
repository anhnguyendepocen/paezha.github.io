[
["index.html", "Practical Data Analysis for the Spatial Sciences in R Preface", " Practical Data Analysis for the Spatial Sciences in R Antonio Paez 2018-05-07 Preface This book introduces data analysis for applied spatial scientists. These could be geographers, earth scientists, environmental scientists, planners, or others who work with georeferenced datasets. My aim with this book has been to introduce key concepts and techniques in the analysis of spatial data in an intuitive way. While there are more advanced treatments of many of these topics, this book should be appealing to students or others who are approaching this topic for the first time. The book is organized thematically following the cannonical approach seen, for instance, in [BAILEY &amp; GATRELL] and [UNWIN]. This approach is to conceptualize data by their unit of support. Each chapter covers a topic that builds on previous material. In this way, this is not necessarily meant as a reference (there are better books for that). The chapters are followed by an activity. I have used these materials for teaching spatial data analysis in different settings. In a flipped classroom setting, the chapters are used as practice material by the students before class. The activity is then completed in class, with the instructor providing support and motivating some discussion. Used in a traditional lecture style, the materials provide structure and visual aids. The activities can be completed by the students as homework or during lab time. "],
["introduction-to-r.html", "1 Introduction to R 1.1 Learning objectives 1.2 RStudio Window 1.3 Some basic operations 1.4 Data Classes in R 1.5 Data Types in R 1.6 Indexing and Data Transformations 1.7 Visualization 1.8 Creating a simple map", " 1 Introduction to R NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. Now that you have installed R and RStudio we can begin with an overview of basic operations and data structures in this computing language. Please note that this document you are reading, called an R Notebook, is an example of what is called “literate programming”, a style of document that uses code to illustrate a discussion, as opposed to the traditional programming style that uses natural language to discuss/document the code. It flips around the usual technical writing approach to make it more intuitive and accessible. Whenever you see a chunk of code as follows, you can run it (by clicking the ‘play’ icon on the top right corner) to see the results. Try it! print(&quot;Hello, Geography 4GA3&quot;) ## [1] &quot;Hello, Geography 4GA3&quot; The chunk of code above instructed R (and trough R the computer) to print (or display on the screen) some text. 1.1 Learning objectives ADD LEARNING OBJECTIVES 1.2 RStudio Window If you are reading this, you probably already read the document ‘00 Installation of R’. We can now proceed to discuss some basic concepts of operations and data types. 1.3 Some basic operations R can perform many types of operations. Some simple operations are arithmetic. Other are logical. And so on. For instance, R can be instructed to conduct sums, as follows: 2 + 2 ## [1] 4 R can be instructed to do multiplications: 2 * 3 ## [1] 6 And sequences of operations, using brackets to indicate their order. Compare the following two expressions: 2 * 3 + 5 ## [1] 11 2 * (3 + 5) ## [1] 16 Other operations produce logical results (values of true and false): 3 &gt; 2 ## [1] TRUE 3 &lt; 2 ## [1] FALSE And of course, you can combine operations in an expression: 2 * 3 + 5 &lt; 2 * (3 + 5) ## [1] TRUE As you can see, R can be used as a calculator, but it is much more powerful than that. We can also create variables. You can think of a variable as a box with a name, whose contents can change. Variables are used to keep track of important stuff in your calculations, and to automate operations. To create a variable, a value is assigned to a name, using this notation &lt;-. You can read this x &lt;- 2 as “assign value of 2 to a variable called x”. For instance: x &lt;- 2 y &lt;- 3 z &lt;- 5 Check your “Global Environment”, the tab where the contents of your “Workspace” are displayed for you. You can also simply type the name of the variable in the Console to see its contents. Now that we have some variables with values, we can express operations as follows (same as above) x * y + z ## [1] 11 x * (y + z) ## [1] 16 However, if we wanted, we could change the values of any of x, y, and/or z and repeat the operations. This allows to automate some instructions: x &lt;- 4 x * y + z ## [1] 17 1.4 Data Classes in R R can work with different data classes, including: Numerical Character Logical Factor This allows you to store information in different forms, which can be useful. For instance, you may want to save some text: name &lt;- &quot;Hamilton&quot; Or numerical information: population &lt;- 551751 If you wish to check what class an object is, you can use the function class: class(name) ## [1] &quot;character&quot; class(population) ## [1] &quot;numeric&quot; 1.5 Data Types in R R can work with different data types, including scalars (essentially matrices with only one element), vectors (matrices with one dimension of size 1) and matrices (more generally. print(&#39;This is a scalar&#39;) ## [1] &quot;This is a scalar&quot; 1 ## [1] 1 print(&#39;This is a vector&#39;) ## [1] &quot;This is a vector&quot; c(1,2,3,4) ## [1] 1 2 3 4 print(&#39;This is a matrix&#39;) ## [1] &quot;This is a matrix&quot; matrix(c(1,2,3,4),nrow = 2, ncol=2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 The command c() is used to concatenate the arguments. The command matrix() creates a matrix with the specified number of rows and columns. An important data type in R is a data frame. A data frame is a table consisting of rows and columns - commonly a set of vectors that have been collected for convenience. A data frame is used to store data in digital format. (If you have used Excel or another spreadsheet software before, data frames will be familiar to you: they look a lot like a sheet in a spreadsheet.) A data frame can accommodate large amounts of information (several billion individual items). The data can be numeric, character, logical, and so on. Each grid cell in a data frame has an address that can be identified based on the row and column it belongs to. R can use these addresses to perform mathematical operations. R labels columns alphabetically and rows numerically (or less commonly alphabetically). To illustrate a data frame, let us first create the following vectors, that include names, populations, average salaries, and coordinates of some cities: Name &lt;- c(&#39;Hamilton&#39;,&#39;Waterloo&#39;,&#39;Toronto&#39;) Population &lt;- c(551751, 219153, 2731571) AvgSalary &lt;- c(45692, 57625, 48920) Latitude &lt;- c(43.255203, 43.4668, 43.6532) Longitude &lt;- c(-79.843826, -80.51639, -79.3832) Again, note that &lt;- is an assignment. In other words, it assigns the item on the right to the name on the left. After you execute the chunk of code above, you will notice that new values appear in your Environment. These are five vectors of size 1:3, one that is composed of alphanumeric information (or chr, for ‘character’) and four columns that are numeric (num). These vectors can be collected in a dataframe. This is done for convenience, so we know that all these data belong together in some way. Please note that to create a data frame, the vectors must have the same length. In other words, you cannot create a table with elements that have different numbers of rows (other data types allow you to do this, but not data frames). We will now create a data frame. We will call it “Cities”. There are rules for names, but in most cases it helps if the names are intuitive and easy to remember. The function used to create a data frame is data.frame() and the arguments are the vectors that we wish to collect there. Cities &lt;- data.frame(Name, Population, AvgSalary, Latitude, Longitude) After running the chunk above, now you have a new object in your environment, namely a data frame called Cities. If you double clic on Cities in the Environment tab, you will see that this data frame has five columns (labeled Name, Population, AvgSalary, Latitude, and Longitude), and three rows. You can enter data into a data frame and then use the many built-in functions of R to perform various types of analysis. Please note that Name, which was an alphanumeric vector, was converted to a factor in the data frame. A factor is a way to store nominal variables that may have two or more levels. In the present case, the factor variable has three levels, corresponding to three cities. If we had information for multiple years, each city might appear more than once, for each year that information was available. 1.6 Indexing and Data Transformations Data frames store information that is related in a compact way. To perform operations effectively, it is useful to understand the way R locates information in a data frame. As noted before, each grid cell has an address, or in other words an index, that can be referenced in several convenient ways. For instance, assume that you wish to reference the first value of the data frame, in other words, row 1 of column Name. To do this, you would go use the following instruction: Cities[1,1] ## [1] Hamilton ## Levels: Hamilton Toronto Waterloo This will recall the element in the first row and first column of Cities. As an alternative, you could type: Cities$Name[1] ## [1] Hamilton ## Levels: Hamilton Toronto Waterloo As you see, this has the same effect. The string sign $ is used to reference columns in a data frame. Therefore, R will call the first element of Name in data frame Cities. Cities[1,2] is identical to Cities$Name[2]. Try changing the code in the chunk and executing. If you type Cities$Name, R will recall the full column. Indexing is useful to conduct operations. Suppose for instance, that you wished to calculate the total population of two cities, say Hamilton and Waterloo. You can execute the following instructions: Cities$Population[1] + Cities$Population[2] ## [1] 770904 (More involved indexing is also possible, for example, if we use logical operators. Do not worry too much about the details, but you can verify that the results are identical) Cities$Population[Cities$Name==&#39;Hamilton&#39;] + Cities$Population[Cities$Name==&#39;Waterloo&#39;] ## [1] 770904 Suppose that you wanted to calculate the total population of the cities in your data frame. To do this, you would use the instruction sum: sum(Cities$Population) ## [1] 3502475 You have already seen how it allows you to store in memory the results of some instruction, by means of an assignment &lt;-. You can also perform many other useful operations. For instance, calculate the maximum value for a set of values: max(Cities$Population) ## [1] 2731571 And, if you wanted to find which city is the one with the largest population, you would use a logical statement as an index: Cities$Name[Cities$Population==max(Cities$Population)] ## [1] Toronto ## Levels: Hamilton Toronto Waterloo As you see, Toronto is the largest city (by population) in this dataset. Using indexing in imaginative ways provides a way to do fairly sophisticated data analysis. Likewise, the function for finding the minimum value for a set of values is min: min(Cities$Population) ## [1] 219153 Try calculating the average of the population of the cities, using the command mean. Use the empty chunk below for this (the result should be 1167492): Finding the maximum and minimum, aggregating (calculating the sum of a series of values), and finding the average are examples of transformations applied to the data. They give insights into aspects of the dataset that are not evident from the raw data. 1.7 Visualization The data frame, in essence a table, informative as it is, may not be the best way to learn from the data. Visualization is often a valuable complement to data analysis. Say, we might be interested in finding which city has the largest population and which city has the smallest population. We could achieve this by using similar instructions as before, for example: paste(&#39;The city with the largest population is&#39;,Cities$Name[Cities$Population==max(Cities$Population)]) ## [1] &quot;The city with the largest population is Toronto&quot; paste(&#39;The city with the smallest population is&#39;, Cities$Name[Cities$Population==min(Cities$Population)]) ## [1] &quot;The city with the smallest population is Waterloo&quot; (Note that paste is similar to print, except that it converts everything to characters before printing. We use this command because the contents of Name in data frame Cities are not characters, but levels.) A more convenient way of understanding these data is by visualizing them, using for instance a bar chart. We will proceed to create a bar chart, using a package called ggplot2. This package implements a grammar of graphics, and is a very flexible way of creating plots in R. Since ggplot2 is a package, we first must ensure that it is installed. You can install it using the command install as follows: #install.packages(&quot;ggplot2&quot;) As an alternative, you can use the Packages tab in RStudio. Simply navigate to the tab, click install, and select ggplot2 from there. Note that you need to install the package only once! Essentially install adds it to your library of packages, where it will remain available. Once the package is installed, it becomes available, but to use it you must load it in memory. For this, we use the command library(), which is used to load a package, that is, to activate it for use. Assuming that you already have installed ggplot2, we proceed to load it: library(ggplot2) Now all commands from the ggplot2 package are available to you. This package works by layering a series of objects, beginning with a blank plot, to which we can add things. The command to create a plot is ggplot(). This command accepts different arguments. For instance, we can pass data to it in the form of a data frame. We can also indicate different aesthetic values, that is, the things that we wish to plot. None of this is plotted, though, until we indicate which kind of geom or geometric object we wish to plot. For a bar chart, we would use the following instructions: ggplot(data = Cities, aes(x = Name, y = Population)) + geom_bar(stat = &#39;identity&#39;) Let us break down these instructions. We are asking ggplot2 to create a plot that will use the data frame Cities. Furthermore, we tell it to use the values of Names in the x-axis, and the values of Population in the y-axis. Run the following chunk: ggplot(data = Cities, aes(x = Name, y = Population)) Notice how ggplot2 creates a blank plot, but it has yet to actually render any of the population information in there. We layer elements on a plot by using the + sign. It is only when we tell the package to add some geometric element that it renders something on the plot. In the previous case, we told ggplot2 to draw bars (by using the geom_bar command). The argument of geom_bar was stat = 'identity', to indicate that the data for the y-axis was to be used ‘as-is’ without further statistical transformations. There are many different geoms that can be used in ggplot2. You can always consult the help/tutorial files by typing ??ggplot2 in the console. See: ??ggplot2 1.8 Creating a simple map We will see how maps are used in spatial statistical analysis. The simplest one that can be created is a so-called dot map that displays the location of an event of interest. A dot map is, in fact, simply a scatterplot of the coordinates of events. We can use ggplot2 to create a simple dot map of the cities in your simple dataset. For this, we create a ggplot object, and for the x and y aesthetics we use the coordinates. The geometric element that we want to render is a point: ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point() This is a simple dot map that simply shows the locations of the cities. We can add labels by means of the geometric element text: ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point() + geom_text(aes(label = Name)) A proportional symbol map changes the size of the symbols to add information to the plot. To create a proportional symbol map, we add to the aesthetics the instruction to use some variable for the size of the symbols: ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point(aes(size = Population)) + geom_text(aes(label = Name)) And fix the position of the labels by adding a vertical justification to the text (vjust) and expanding the limits of the plot (expand_limits): ggplot(data = Cities, aes(x = Longitude, y = Latitude)) + geom_point(aes(size = Population)) + geom_text(aes(label = Name), vjust = 2) + expand_limits(x = c(-80.7, -79.2), y = c(43.2, 43.7)) You have now created a relatively simple proportional symbols map! You can see that creating a plot is simply a matter of instructing R (through ggplot2) to complete a series of instructions: Create a ggplot2 object using a dataset, which will render stuff at locations given by variable1 and variable 2: ggplot(data = dataset, aes(x = variable1, y = variable2)) Add stuff to the plot. For instance, to add points use geom_point, to add lines use geom_line, and so on. Check the ggplot2 Cheat Sheet for more information on how to use this package. A last note. Many other visualization alternatives (for instance, Excel) provide point-and-click functions for creating plots. In contrast, ggplot2 in R requires that the plot be created by meticulously instructing the package what to do. While this is more laborious, it also means that you have complete control over the creation of plots, which in turn allows you to create more flexible and inventive visuals. This concludes your basic overview of basic operations and data structures in R. You will have an opportunity to learn more about creating maps in R with your reading. "],
["introduction-to-mapping-in-r.html", "2 Introduction to Mapping in R 2.1 Learning objectives 2.2 Preliminaries 2.3 Improving on the proportional symbols map", " 2 Introduction to Mapping in R NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. Spatial statistics is a sub-field of spatial analysis that has grown in relevance in recent years as a result of 1) the availability of information that is geo-coded, in other words, that has geographical references; and 2) the availability of software to analyze such information. A key technology fuelling this trend is that of Geographical Information Systems (GIS). GIS are, at their simplest, digital mapping for the 21st century. In most cases, however, GIS go beyond cartographic functions to also enable and enhance our ability to analyze data. There are many available packages for geographical information analysis. Some are very user friendly, and widely available in many institutional contexts, such as ESRI’s Arc software. Others are fairly specialized, such as Caliper’s TransCAD, which implements many operations of interest for transportation engineering and planning. Others packages have the advantage of being more flexible and/or free. Such is the case of the R statistial computing language. R has been adopted by many in the spatial analysis community, and a number of specialized libraries have been developed to support mapping and spatial data analysis functions. The objective of this note is to provide an introduction to mapping in R. Maps are one of the fundamental tools of spatial statistics and spatial analysis, and R allows for many GIS-like functions. To use this note you will need the following: This R markdown notebook. A dataset called Snow.RData In the previous reading/practice you created a simple proportional symbols map. In this reading/practice you will learn how to create more sophisticated maps. 2.1 Learning objectives ADD LEARNING OBJECTIVES 2.2 Preliminaries It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Now that your workspace is clear, you can proceed to load a sample dataset. It is very important that R knows where to look for it. This means that the path to the file is explicit, or alternatively that you set the working directory to the directory where the file is. For instance, in my computer I would type: load(&quot;Snow.RData&quot;) As an alternative, I could set the working directory to the directory where the file is, and then I could load it. It is good practice to set the working directory, so that you know where your files are when reading and saving. 2.3 Improving on the proportional symbols map If you correctly set the path to your file, you will now have two dataframes in your Global Environment, namely df_pumps and df_deaths. You can examine the contents of these dataframes by means of the command head. This command displays the first few rows of the dataframe. Try it: head(df_deaths) ## long lat Id Count type ## 0 -0.1379301 51.51342 1 3 death ## 1 -0.1378831 51.51336 2 2 death ## 2 -0.1378529 51.51332 3 1 death ## 3 -0.1378120 51.51326 4 1 death ## 4 -0.1377668 51.51320 5 4 death ## 5 -0.1375369 51.51318 6 2 death These data are from the famous London cholera example. This is the study by John Snow (not the one from Game of Thrones, but the British physician) into the cholera outbreak of Soho, London, in 1854. John Snow is considered the father of spatial epidemiology, and his study mapping the outbreak is credited with helping find its cause. The dataframe df_deaths includes the geocoded addresses of cholera cases in long and lat, and the number of cases the Count recorded at each address, as well as unique identifiers for the addresses (Id). The dataframe df_pumps includes the geocoded locations of water pumps in Soho. As in your previous reading, it is possible to map the cases using ggplot2. Begin by loading the libraries needed: library(tidyverse) ## -- Attaching packages ---------------------------------- tidyverse 1.2.1 -- ## v tibble 1.4.2 v purrr 0.2.4 ## v tidyr 0.8.0 v dplyr 0.7.4 ## v readr 1.1.1 v stringr 1.3.0 ## v tibble 1.4.2 v forcats 0.3.0 ## Warning: package &#39;stringr&#39; was built under R version 3.4.4 ## Warning: package &#39;forcats&#39; was built under R version 3.4.4 ## -- Conflicts ------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Now, you can create a blank ggplot2 object on which you can render the points for deaths and the pumps. ggplot() + geom_point(data = df_deaths, aes(x = long, y = lat), color = &quot;blue&quot;, shape = 16) + geom_point(data = df_pumps, aes(x = long, y = lat), color = &quot;black&quot;, shape = 17) This does a decent job of displaying the information, now using different colors and shapes for different types of events (deaths and pumps). However, it is not a very good quality map. A package that extends the functionality of ggplot2 for mapping is ggmap. We will see next how to enhance our proportional symbol map using this package. First you need to load the package (you need to install it first if you have not already): library(ggmap) Now, you can get a base map, to provide context to the information that you are planning to display. A base map can be obtained by means of the get_map function of the ggmap package. In this case, the map is retrieved based on the coordinates of Soho in London. The map can be sourced from Google Maps, OpenStreetMap, Stamen Maps, or CloudMade maps. The zoom is an integer that varies from 3 (a continent) to 21 (a building). In this case, a zoom of 16 captures a neighborhood. The maptype varies by source. In the case of “stamen”, the types can be “terrain”, “watercolor”, or “toner” london_main &lt;- get_map(c(-.137,51.513), zoom = 16, source = &quot;stamen&quot;, maptype = &quot;watercolor&quot;) You can render the map that you retrieved by means of the ggmap command: ggmap(london_main) A convenient aspect of ggmap is that it retains the functionality of ggplot2. Since we have already used ggplot2 to create a simple map, we can do the same thing, but now layered on a ggmap object. This will preserve the cartographic properties of the map (including the aspect, and so on). For instance, we will replace the blank ggplot object that we used before with the base map: ggmap(london_main) + geom_point(data = df_deaths, aes(x = long, y = lat), color = &quot;blue&quot;, shape = 16) + geom_point(data = df_pumps, aes(x = long, y = lat), color = &quot;black&quot;, shape = 17) The above results in a much nicer map. From here, we can further enhance the map by changing the size of the symbols, in the case of the deaths by making the symbols proportional to the count of deaths: ggmap(london_main) + geom_point(data = df_deaths, aes(x = long, y = lat, size = Count * 20), color = &quot;blue&quot;) + geom_point(data = df_pumps, aes(x = long, y = lat), color = &quot;black&quot;, size = 5, shape = 17) We could even begin to do some spatial analysis on this map! For instance, we could calculate the density (we will cover this technique in more detail later on; it is called kernel analysis). For this, we use the stat_density2d function of the ggplot2 package (for this to work, the dataframe df_deaths is expanded, so that every row represents a single case; this function is part of the splitstackshape package): library(splitstackshape) ## Warning: package &#39;splitstackshape&#39; was built under R version 3.4.4 ggmap(london_main) + stat_density2d(data = expandRows(df_deaths, &quot;Count&quot;), aes(x = long, y = lat, fill = ..level.., alpha = ..level..), size = 20, bins = 10, geom = &quot;polygon&quot;) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;blue&quot;) + scale_alpha(range = c(0.5, 0.75), guide = FALSE) + geom_point(data = df_pumps, aes(x = long, y = lat, shape = type), color = &quot;black&quot;, size = 5, shape = 17) A map showing the kernel density of deaths makes it very clear that most cases of cholera happend in the neighborhood of one (possibly contaminated) water pump! Snow noted that: “It will be observed that the deaths either very much diminished, or ceased altogether, at every point where it becomes decidedly nearer to send to another pump than to the one in Broad street. It may also be noticed that the deaths are most numerous near to the pump where the water could be more readily obtained.” Snow’s analysis led to the closure of the pump, after which the cholera outbreak subsided. This illustrates how some relatively simple spatial analysis can help to save lives. You can read more about this case here. In this practice you have learned how to implement some simple mapping and spatial statistical analysis using R. In future readings we will further explore the potential of R for both. For more information on the functionality of ggmap, check ggmap: Spatial Visualization with ggplot2 "],
["introduction-to-mapping-in-r-activity.html", "3 Introduction to Mapping in R - Activity 3.1 Preliminaries 3.2 Activity", " 3 Introduction to Mapping in R - Activity 3.1 Preliminaries In the practice that preceded this activity, you used ggmap to create a proportional symbol map, a mapping technique used in spatial statistics for visualization of geocoded event information. As well, you implemented a simple technique called kernel analysis to the map to explore the distribution of events in the case of the cholera outbreak of Soho in London in 1854. Geocoded events are often called point patterns, so with the cholera data you were working with a point pattern. In this activity, we will map another type of spatial data, called areal data. Areas are often administrative or political jurisdictions. For this activity you will need the following: This R markdown notebook. A dataset called HamiltonDAs.RData It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(ggmap) Now that your workspace is clear, you can proceed to load a sample dataset. It is very important that R knows where to look for it. This means that the path to the file is explicit, or alternatively that you set the working directory to the directory where the file is. load(&quot;HamiltonDAs.RData&quot;) The dataframe you just loaded is of Dissemination Areas or DAs, a type of geography used by the Census of Canada. We will first use the ggmap::get_map function (this is read get_map function from the ggmap package). This function is used to obtain a base map. For this, the argument must specify the location for the capture, which can be a string of text. The outcome can be saved as an object that can later be used for mapping: hamilton &lt;- get_map(&quot;Hamilton, Ontario&quot;) ## Map from URL : http://maps.googleapis.com/maps/api/staticmap?center=Hamilton,+Ontario&amp;zoom=10&amp;size=640x640&amp;scale=2&amp;maptype=terrain&amp;language=en-EN&amp;sensor=false ## Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=Hamilton,%20Ontario&amp;sensor=false Since ggmap is built on top of ggplot2, all functions of that package are available, including other geoms. Previously you mapped points. Now, the geom for polygons can be used for areas. To create such a map, we layer other geoms objects as layers on top of the base map. For instance, to plot the DAs on your base map: head(HamiltonDAs) ## long lat order hole piece group GTA06 VAR1 VAR2 ## 1 -79.86997 43.29277 1 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 2 -79.86904 43.29339 2 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 3 -79.86803 43.29426 3 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 4 -79.86699 43.29532 4 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 5 -79.86666 43.29565 5 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 6 -79.86586 43.29637 6 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## VAR3 VAR4 VAR5 ## 1 0.3450731 0.3057122 0.3622016 ## 2 0.3450731 0.3057122 0.3622016 ## 3 0.3450731 0.3057122 0.3622016 ## 4 0.3450731 0.3057122 0.3622016 ## 5 0.3450731 0.3057122 0.3622016 ## 6 0.3450731 0.3057122 0.3622016 ggmap(hamilton) + geom_polygon(data = HamiltonDAs, aes(x = long, y = lat, group = group), color = &#39;black&#39;, alpha = .3, size = .3) We selected color “black” for the polygons, with a transparency alpha = 0.3 (alpha = 0 is completely transparent, alpha = 1 is completely opaque, try it!), and line size 0.3. This map only shows the DAs, which is nice. However, if you examine the dataframe, you will notice that it includes, in additional to the geometric information, other variables (called VAR1, VAR2,…): head(HamiltonDAs) ## long lat order hole piece group GTA06 VAR1 VAR2 ## 1 -79.86997 43.29277 1 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 2 -79.86904 43.29339 2 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 3 -79.86803 43.29426 3 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 4 -79.86699 43.29532 4 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 5 -79.86666 43.29565 5 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## 6 -79.86586 43.29637 6 FALSE 1 4050.1 4050 0.3788377 0.3418337 ## VAR3 VAR4 VAR5 ## 1 0.3450731 0.3057122 0.3622016 ## 2 0.3450731 0.3057122 0.3622016 ## 3 0.3450731 0.3057122 0.3622016 ## 4 0.3450731 0.3057122 0.3622016 ## 5 0.3450731 0.3057122 0.3622016 ## 6 0.3450731 0.3057122 0.3622016 Thematic maps can also be created with those variables. The next chunk of code plots the DAs and adds info. The fill argument is used to select a variable to color the polygons. The function cut_number is used to classify the values of the variable in 5 groups of equal size (notice that the lines of the polygons are still black). The scale_fill_brewer function can be used to select different palettes or coloring schemes): ggmap(hamilton) + geom_polygon(data = HamiltonDAs, aes(x = long, y = lat, group = group, fill = cut_number(VAR5, 7)), colour = &#39;black&#39;, alpha = 1, size = .3) + scale_fill_brewer(palette = &quot;Reds&quot;) 3.2 Activity Now you know how to create a thematic map with polygons (areal data). Create thematic maps for variables VAR1 through VAR5 in the dataframe HamiltonDAs. Remember that you can introduce new chunks of code. Imagine that these maps were found, and for some reason the variables were not labeled. They may represent income, or population density, or something else. Which of the five maps you just created is more interesting? Rank the five maps from most to least interesting. Explain the reasons for your ranking. "],
["statistical-maps-ii.html", "4 Statistical Maps II 4.1 Learning objectives 4.2 Suggested reading 4.3 Preliminaries 4.4 Summarizing a dataframe 4.5 Factors 4.6 Subsetting data 4.7 Pipe operator 4.8 More on mapping", " 4 Statistical Maps II NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In Practices 1 and 2, you were introduced to the following concepts: Basic operations in R. These include arithmetic and logical operations, among others. Data classes in R. Data can be numeric, characters, logical values, etc. Data types in R. Ways to store data, for instance as vector, matrix, dataframes, etc. Indexing. Ways to retrieve information from a data frame by referring to its location therein. Creating simple maps in R. You used ggmap, a package that builds on ggplot2. Please review the previous practices if you need a refresher on these concepts. For this practice you will need the following: This R markdown notebook. The following dataset: Data1.RData 4.1 Learning objectives In this practice you will learn to quickly summarize the descriptive statistics of a dataframe. You will learn more about factors. Factors are a class of data that is used for categorical data. For instance, a parcel may be categorizes as developed or undeveloped; a plot of land may be zoned for commercial, residential, or industrial use; a sample may be mineral x or y. These are not quantities but rather reflect a quality of the entity that is being described. You will learn how to subset a dataset. Sometimes you want to work with only a subset of a dataset. This can be done using indexing with logical values, or using specialized functions. You will also learn how to use pipe operators. A pipe operator allows you to pass the results of a function to another function. It makes writing instructions more intuitive and simple. You will add layers to a ggmap object to improve a map. 4.2 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapters 1-3. John Wiley &amp; Sons: New Jersey. 4.3 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) Now that your workspace is clear, you can proceed to load the sample dataset. It is very important that R knows where to look for it. This means that the path to the file must be explicit, or alternatively that you set the working directory to the directory where the file is. load(&quot;Data1.RData&quot;) The datasets include the following dataframe: Data The dataframe Data includes \\(n = 65\\) observations (Note: text between $ characters is mathematical notation in LaTeX). These observations are geocoded using a false origin and coordinates normalized to the unit-square (the extent of their values is between zero and one). The coordinates are x and y. In addition, there are three variables associated with the locations (VAR1, VAR2, VAR3). The variables are generic. Feel free to think of them as if they were housing prices or concentrations in ppb of some contaminant. Finally, a factor variable states whether the variables were measured for a location: if the status is “FALSE”, the values of the variables are missing. 4.4 Summarizing a dataframe Obtaining the a set of descriptive statistics for a dataframe is very simple thanks to the function summary. For instance, the summary of Data is: summary(Data) ## x y VAR1 VAR2 ## Min. :0.01699 Min. :0.01004 Min. : 50.0 Min. : 50.0 ## 1st Qu.:0.22899 1st Qu.:0.19650 1st Qu.: 453.3 1st Qu.: 570.1 ## Median :0.41808 Median :0.50822 Median : 459.1 Median : 574.4 ## Mean :0.49295 Mean :0.46645 Mean : 458.8 Mean : 562.1 ## 3rd Qu.:0.78580 3rd Qu.:0.74981 3rd Qu.: 465.4 3rd Qu.: 594.2 ## Max. :0.95719 Max. :0.98715 Max. :1050.0 Max. :1050.0 ## NA&#39;s :5 NA&#39;s :5 ## VAR3 Observed ## Min. : 50.0 FALSE: 5 ## 1st Qu.: 630.3 TRUE :60 ## Median : 640.0 ## Mean : 638.1 ## 3rd Qu.: 646.0 ## Max. :1050.0 ## NA&#39;s :5 This function reports the minimum, maximum, mean, median, and quantile values of a numeric variable. When variables are characters or factors, their frequency is reported. For instance, in Data, there are five instances of FALSE and sixty instances of TRUE. 4.5 Factors A factor describes a category. You can examine the class of a variable by means of the function class. From the summary, it is clear that several variables are numeric. However, for Observed, it is not evident if the variable is character or factor. Use of class reveals that it is indeed a factor: class(Data$Observed) ## [1] &quot;factor&quot; Factors are an important data type because they allow us to store information that is not measured as a quantity. For example, the quality of the cut of a diamond is categorized as Fair &lt; Good &lt; Very Good &lt; Premium &lt; Ideal. Sure, we could store this information as numbers from 1 to 5. However, the quality of the cut is not a quantity, and should not be treated like one. In the dataframe Data, the variable Observed could have been coded as 1’s (for missing) and 2’s (for observed), but this does not mean that one observed is twice the amount of one missing! In this case, the numbers would not be quantities but labels. Factors in R allow us to work directly with the labels. Now, you may be wondering what does it mean when the status of a datum’s Observed variable is coded as FALSE. If you check again the summary, there are five cases of NA in the variables VAR1 through VAR3. NA essentially means that the value is missing. Likely, the five NA values correspond to the five missing observations. We can check this by subsetting the data. 4.6 Subsetting data We subset data when we wish to work only with parts of a dataset. We can do this by indexing. For example, we could retrieve the part of the dataframe that corresponds to the FALSE values in the Observed variable: Data[Data$Observed == FALSE,] ## x y VAR1 VAR2 VAR3 Observed ## 61 0.34 0.83 NA NA NA FALSE ## 62 0.29 0.52 NA NA NA FALSE ## 63 0.13 0.32 NA NA NA FALSE ## 64 0.62 0.10 NA NA NA FALSE ## 65 0.88 0.85 NA NA NA FALSE Data are indexed by means of the square brackets [ and ]. The indices correspond to the rows and columns. The logical statement Data$Observed == False selects the rows that meet the condition, whereas leaving a blank for the columns simply means “all columns”. As you can see, the five NA values correspond, as anticipated, to the locations where Observed is FALSE. Using indices is only one of different ways of subsetting data. Base R also has a subset command, that is implemented as follows: subset(Data, Observed == FALSE) ## x y VAR1 VAR2 VAR3 Observed ## 61 0.34 0.83 NA NA NA FALSE ## 62 0.29 0.52 NA NA NA FALSE ## 63 0.13 0.32 NA NA NA FALSE ## 64 0.62 0.10 NA NA NA FALSE ## 65 0.88 0.85 NA NA NA FALSE And the package dplyr (part of the tidyverse) has a function called filter: filter(Data, Observed == FALSE) ## Warning: package &#39;bindrcpp&#39; was built under R version 3.4.4 ## x y VAR1 VAR2 VAR3 Observed ## 1 0.34 0.83 NA NA NA FALSE ## 2 0.29 0.52 NA NA NA FALSE ## 3 0.13 0.32 NA NA NA FALSE ## 4 0.62 0.10 NA NA NA FALSE ## 5 0.88 0.85 NA NA NA FALSE The three approaches give the same result, but subset and filter are somewhat easier to write. You could nest any of the above approaches as part of another function. For instance, if you wanted to do a summary of the selected subset of the data, you would: summary(filter(Data, Observed == FALSE)) ## x y VAR1 VAR2 VAR3 ## Min. :0.130 Min. :0.100 Min. : NA Min. : NA Min. : NA ## 1st Qu.:0.290 1st Qu.:0.320 1st Qu.: NA 1st Qu.: NA 1st Qu.: NA ## Median :0.340 Median :0.520 Median : NA Median : NA Median : NA ## Mean :0.452 Mean :0.524 Mean :NaN Mean :NaN Mean :NaN ## 3rd Qu.:0.620 3rd Qu.:0.830 3rd Qu.: NA 3rd Qu.: NA 3rd Qu.: NA ## Max. :0.880 Max. :0.850 Max. : NA Max. : NA Max. : NA ## NA&#39;s :5 NA&#39;s :5 NA&#39;s :5 ## Observed ## FALSE:5 ## TRUE :0 ## ## ## ## ## Or: summary(Data[Data$Observed == FALSE,]) ## x y VAR1 VAR2 VAR3 ## Min. :0.130 Min. :0.100 Min. : NA Min. : NA Min. : NA ## 1st Qu.:0.290 1st Qu.:0.320 1st Qu.: NA 1st Qu.: NA 1st Qu.: NA ## Median :0.340 Median :0.520 Median : NA Median : NA Median : NA ## Mean :0.452 Mean :0.524 Mean :NaN Mean :NaN Mean :NaN ## 3rd Qu.:0.620 3rd Qu.:0.830 3rd Qu.: NA 3rd Qu.: NA 3rd Qu.: NA ## Max. :0.880 Max. :0.850 Max. : NA Max. : NA Max. : NA ## NA&#39;s :5 NA&#39;s :5 NA&#39;s :5 ## Observed ## FALSE:5 ## TRUE :0 ## ## ## ## ## Nesting functions makes it difficult to read the code, since functions are evaluated from the innermost to the outermost function, whereas we are used to read from left to right. Fortunately, R implements (as part of package magrittr which is required by tidyverse) a so-called pipe operator that simplifies things and allows for code that is more intuitive to read. 4.7 Pipe operator A pipe operator is written this way: %&gt;%. Its objective is to pass forward the output of a function to a second function, and they can be chained to create complex instructions. For instance, instead of nesting the subsetting instructions in the summary function, you could do the subsetting first, and pass the results of that to the summary for further processing. This would look like this: subset(Data, Observed == FALSE) %&gt;% summary() ## x y VAR1 VAR2 VAR3 ## Min. :0.130 Min. :0.100 Min. : NA Min. : NA Min. : NA ## 1st Qu.:0.290 1st Qu.:0.320 1st Qu.: NA 1st Qu.: NA 1st Qu.: NA ## Median :0.340 Median :0.520 Median : NA Median : NA Median : NA ## Mean :0.452 Mean :0.524 Mean :NaN Mean :NaN Mean :NaN ## 3rd Qu.:0.620 3rd Qu.:0.830 3rd Qu.: NA 3rd Qu.: NA 3rd Qu.: NA ## Max. :0.880 Max. :0.850 Max. : NA Max. : NA Max. : NA ## NA&#39;s :5 NA&#39;s :5 NA&#39;s :5 ## Observed ## FALSE:5 ## TRUE :0 ## ## ## ## ## The code above is read as “subset Data and pass the results to summary”. Pipe operators make writting and reading code somewhat more natural. 4.8 More on mapping Observations in the sample dataset are georeferenced, and so they can be plotted. Since they are based on false origins and are normalized, we cannot map them to the surface of the Earth. However, we can still visualize their spatial distribution. This can be done by using ggplot2. For instance, for Data: ggplot() + geom_point(data = Data, aes(x = x, y = y), shape = 17, size = 3) + coord_fixed() The above simply plots the coordinates, so that we can see the spatial distribution of the observations. (Notice the use of coord_fixed to maintain the aspect ratio of the plot to 1, the relationship between width and height). You have control of the shape of the markers, as well as their size. You can consult the shapes available here. Experiment with different shapes and sizes if you wish. The dataframe Data includes more attributes that could be used in the plot. For instance, if you wished to create a thematic map showing VAR1 you would do the following: ggplot() + geom_point(data = Data, aes(x = x, y = y, color = VAR1), shape = 17, size = 3) + coord_fixed() The shape and size assignments happen outside of aes, and so are applied equally to all observations. In some cases, you might want to let other aesthetic attributes vary with the values of a variable in the dataframe. For instance, if we let the sizes change with the value of the variable: ggplot() + geom_point(data = Data, aes(x = x, y = y, color = VAR1, size = VAR1), shape = 17) + coord_fixed() ## Warning: Removed 5 rows containing missing values (geom_point). Note how there is a warning, saying that five observations were removed because data were missing! These are likely the five locations where Observed == FALSE! To make it more clear which observations are these, you could set the shape to vary according to the value of Observed, as follows: ggplot() + geom_point(data = Data, aes(x = x, y = y, color = VAR1, shape = Observed), size = 3) + coord_fixed() You can change the coloring scheme by means of scale_color_distiller (you can can check the different color palettes available here): ggplot() + geom_point(data = Data, aes(x = x, y = y, color = VAR1, shape = Observed), size = 3) + scale_color_distiller(palette = &quot;RdBu&quot;) + coord_fixed() You will notice maybe that with this coloring scheme some observations become very light and difficult to distinguish from the background. This can be solved in many different ways (for instance, by changing the color of the background!). A simple fix is to add a layer with hollow symbols, as follows: ggplot() + geom_point(data = Data, aes(x = x, y = y, color = VAR1), shape = 17, size = 3) + geom_point(data = Data, aes(x = x, y = y), shape = 2, size = 3) + scale_color_distiller(palette = &quot;RdBu&quot;) + coord_fixed() Finally, you could try subsetting the data to have greater control of the appareance of your plot, for instance: ggplot() + geom_point(data = subset(Data, Observed == TRUE), aes(x = x, y= y, color = VAR1), shape = 17, size = 3) + geom_point(data = subset(Data, Observed == TRUE), aes(x = x, y= y), shape = 2, size = 3) + geom_point(data = subset(Data, Observed == FALSE), aes(x = x, y= y), shape = 18, size = 4) + scale_color_distiller(palette = &quot;RdBu&quot;) + coord_fixed() "],
["statistical-maps-ii-activity.html", "5 Statistical Maps II - Activity 5.1 Housekeeping Questions 5.2 Learning objectives 5.3 Suggested reading 5.4 Preliminaries 5.5 Activity", " 5 Statistical Maps II - Activity 5.1 Housekeeping Questions Answer the following questions: How many examinations are there in this course? What is the date of the first examination? Where is Mr. De Luca’s office? 5.2 Learning objectives The objective of this activity is to learn about patterns and processes, including random patterns. You should understand the general approach to retrieve a process from a pattern. You should be able to discuss the importance of discriminating random patterns. 5.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapters 1-3. John Wiley &amp; Sons: New Jersey. 5.4 Preliminaries For this activity you will need the following: This R markdown notebook. The following datasets: Data1.RData Data2.RData It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) Now that your workspace is clear, you can proceed to load the required dataset. It is very important that R knows where to look for it. This means that the path to the file is explicit, or alternatively that you set the working directory to the directory where the file is. load(&quot;Data1.RData&quot;) load(&quot;Data2.RData&quot;) The datasets include the following dataframe which will be used in the first part of the activity: Data And the following related dataframes, which will be used in the second part of the activity: PointPattern1 PointPattern2 PointPattern3 The dataframe Data includes \\(n = 65\\) observations (Note: text between $ characters is mathematical notation in LaTeX). These observations are geocoded using a false origin and coordinates normalized to the unit-square (the extent of their values is between zero and one). The coordinates are x and y. In addition, there are three variables associated with the locations (VAR1, VAR2, VAR3). The variables are generic. Feel free to think of them as if they were housing prices or concentrations in ppb of some contaminant. Finally, a factor variable states whether the variables were measured for a location: if the status is “FALSE”, the values of the variables are missing. The dataframes PointPattern* are locations of some generic event. The coordinates x and y are also based on a false origin and are normalized to the unit-square. Feel free to think of these events as cases of flu, the location of trees of a certain species, or the location of fires. 5.5 Activity Create thematic maps for variables VAR1 through VAR3 in the dataframe Data. Suppose that you were tasked with estimating the value of a variable for the locations where those were not measured. For instance, you could be a realtor, and you need to assess the value of a property, and the only information available is the published values of other properties in the region. As an alternative, you could be an environmental scientist, and you need to estimate what the concentration of a contaminant at a site, based on previous measurements at other sites in the region. Propose one or more ways to guess those missing values, and explain your reasoning. The approach does not need to be the same for all variables! Plot all three point patterns. Imagine that you are a public health official and you need to plan services to the public. If you were asked to guess where the next event would emerge, where would be your guess in each map? Explain your answer. "],
["maps-as-processes-null-landscapes-and-spatial-processes.html", "6 Maps as Processes: Null Landscapes and Spatial Processes 6.1 Introduction 6.2 Learning objectives 6.3 Suggested reading 6.4 Preliminaries 6.5 Random numbers 6.6 Null landscapes 6.7 Simulating spatial processes", " 6 Maps as Processes: Null Landscapes and Spatial Processes 6.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In last practice your learning objectives were: How to obtain a descriptive summary of a dataframe. Factors and how to use them. How to subset a dataframe. Pipe operators and how to use them. How to improve your maps. Please review the previous practices if you need a refresher on these concepts. For this practice you will need the following: This R markdown notebook. 6.2 Learning objectives In this practice, you will learn: How to generate random numbers with different properties. About Null Landscapes. How to create new columns in a dataframe using a formula. How to simulate a spatial process. 6.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapters 4. John Wiley &amp; Sons: New Jersey. 6.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) 6.5 Random numbers Colloquially, we understand random as something that happens in an unpredictable way. The same word in statistics has a precise meaning, as the outcome of a process that cannot be predicted with certainty. The question whether random processes exist is philosophically interesting. In the early stages of the invention of science, there was much optimism that humans could one day understand every aspect of the universe. This notion is well illustrated by Laplace’s Demon, a hypothetical entity that could predict the state of the universe in the future based on an all-encompassing knowledge of the state of the universe at any past point in time (see here). There are two important limitations to this perspective. First, there is the assumption that the mechanisms of operation of phenomena are well understood (in the case of Laplace’s Demon, it was somewhat naively assumed that classical Newtonian mechanics were sufficient). And secondly, the assumption that all relevant information is available to the observer. In reality, there are many processes that are not fully understood. Furthermore, there are often constraints in terms of how much information (and how accurately) can be collected with respect to any given phenomenon. A process can be deterministic. However, When limited knowledge/limited information prevent us from being able to make certain predictions, we assume that the process is random. It is important to note that “random” does not mean that just any outcome is possible. For instance, if you flip a coin, there are only two possible outcomes. If you roll a dice, there are only six possible outcomes. The concentration of a pollutant cannot be negative. The height of a human adult cannot be zero or 10 meters. And so on. Over time, many formulas have been devised to describe different types of random processes. A random probability distribution function describes the probability of observing different outcomes. For instance, a formula for processes similar to coin flips was discovered by Bernoulli in 1713 (see here). The following function reports a random binomial variable. The number of observations n is how many random numbers we require. The size is the number of trials. For instance, if the experiment was flipping a coin, it would be how many times we get heads in size flips. The probability of success prob is the probability of getting heads in any given toss. Execute the chunk repeatedly to see what happens. rbinom(n = 1, size = 1, prob = 0.5) ## [1] 1 If you tried this “experiment” repeatedly, you would find that “heads” (1s) and “tails” (0s) appear each about 50% of the time. A way to implement this is to increase n- think of this as recruiting more people to do coin flips at the same time: n &lt;- 1000 # Number of people tossing the coin one time. coin_flips &lt;- rbinom(n = n, size = 1, prob = 0.5) sum(coin_flips)/n ## [1] 0.5 What happens if you change the size to 0, and why? The binomial function is an example of a discrete probability distribution function, because it can take only one of a discrete (limited) number of values (0 and 1). Other random probability distribution functions are for continuous variables, variables that can take any value within a predefined range. The most famous of this distributions is the normal distribution, which you may know also as the bell curve, which is attributed to Gauss (see here). This distribution is defined by a centering paramater (its mean) and a spread parameter (its standard deviation). In the normal distribution, 68% of values are within one standard deviation from the mean, 95% of values are within two standard deviations from the mean, and 99.7% of values are within three standard deviations from the mean. The following function reports a value taken at random from a normal distribution with mean zero and standard deviation sd of one. Execute this chunk repeatedly to see what happens: rnorm(1, mean = 0, sd = 1) ## [1] 0.7046975 Let’s say that the average height of Canadian men is 170.7 cm and the standard deviation is 7 cm. The heigh of a random person in this population would be: rnorm(1, mean = 170.7, sd = 7) ## [1] 157.0811 And the distribution of heights of n men in this population would be: n &lt;- 1000 height &lt;- rnorm(n, mean = 170.7, sd = 7) height &lt;- data.frame(height) ggplot(data = height, aes(x = height)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Men shorter than 150 cm would be extremely rare, as well as men taller than 190 cm. 6.6 Null landscapes So what have random variables have to do with maps? Random variables can be used to generate purely random maps. These are called null landscapes or neutral landscapes in spatial ecology (see this). The concept of null landscapes is quite useful, because they provide a benchmark to compare statistical maps. Let’s see how to generate a null landscape of events. Suppose that there is a landscape with coordinates in the unit square, that is divided in very small discrete units of land. Each of these units of land can be the location of an event. For example, a tree might be present; or a case of a disease. Let’s first create a landscape. For this, we will use the expand.grid function to find all combinations of two sets of coordinates in the unit interval, using small partitions: coords &lt;- expand.grid(x = seq(from = 0, to = 1, by = 0.05), y = seq(from = 0, to = 1, by = 0.05)) Now, let’s generate a binomial random variable to go with these coordinates. events &lt;- rbinom(n = nrow(coords), size = 1, prob = 0.5) We will collect the coordinates and the random variable in a dataframe for plotting: null_pattern &lt;- data.frame(coords, events) This is our null landscape: ggplot() + geom_point(data = subset(null_pattern, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() By changing the probability prob in the function rbinom you can make the event more or less frequent. Try it! A continuous random variable can be used to generate a landscape with a null trend. For instance, imagine that a group of individuals are asked to stand in formation, and that they arrange themselves purely at random. What would a map of their heights look like? heights &lt;- rnorm(n = nrow(coords), mean = 170.7, sd = 7) Collecting in a dataframe for plotting: null_trend &lt;- data.frame(coords, heights) One possible map of heights when the individuals stand in formation at random would look like: ggplot() + geom_point(data = null_trend, aes(x = x, y = y, color = heights), shape = 15) + scale_color_distiller(palette = &quot;Spectral&quot;) + coord_fixed() These are only two of many possible techniques to generate null landscapes. We will discuss other later in the course. 6.7 Simulating spatial processes Null landscapes are interesting as a benchmark. More interesting are landscapes that emerge as the output of a non-random process. Here we will see how to introduce a systematic element into a null landscape to simulate spatial processes. Let’s begin with the point pattern, using the same landscape that we used above. We will first copy the coordinates of the landscape to a new dataframe, that we will call pattern1: pattern1 &lt;- coords Next, we will use the function mutate from the dplyr package that is part of the tidyverse. This function adds a column to a data frame that could be calculated using a formula. For instance, we will now make the probability prob of the random binomial number generator a function of the coordinates: pattern1 &lt;- mutate(pattern1, events = rbinom(n = nrow(pattern1), size = 1, prob = (x))) Plot this pattern: ggplot() + geom_point(data = subset(pattern1, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() Since the probability of a “success” in the binomial experiment is proportional to the value of x (the coordinate of the event), now the events are clustered to the right of the plot. The underlying process in this case can be described in simple terms as “the probability of an event increases in the east direction”, possibly as a result of wind conditions, soil fertility, or other environmental factors that follow a trend. Let’s see what happens when we make this probability a function of the y coordinate: pattern1 &lt;- mutate(pattern1, events = rbinom(n = nrow(pattern1), size = 1, prob = (y))) ggplot() + geom_point(data = subset(pattern1, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() Since the probability of a “success” in the binomial experiment is proportional to the value of y (the coordinate of the event), now the events are clustered to the top. The probability could be the interaction of the two coordinates: pattern1 &lt;- mutate(pattern1, events = rbinom(n = nrow(pattern1), size = 1, prob = (x * y))) ggplot() + geom_point(data = subset(pattern1, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() Which of course means that the events cluster on the top-right corner. A somewhat more sophisticated example could make the probability a function of distance from the center of the region: pattern1 &lt;- coords pattern1 &lt;- mutate(pattern1, distance = sqrt((0.5 - x)^2 + (0.5 - y)^2), events = rbinom(n = nrow(pattern1), size = 1, prob = 1 - exp(-0.5 * distance))) Don’t worry too much about the formula that I selected to generate this process; we will see different tools to describe a spatial process. In this particular example, I selected a function that makes the probability increase with distance from the center of the region. Plot this pattern: ggplot() + geom_point(data = subset(pattern1, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() As you would expect, there are few events near the center, and the number of events tends to increase away from the center. To conclude this practice, let’s revisit the example of the people standing in formation, but now their sorting is not random, since taller people tend to stand towards the back. We can simulate this by making the height a function of position. First, we copy the coordinates to a new dataframe for our trend experiment: trend1 &lt;- coords Again we use mutate to add a column to a data frame that could be calculated using a formula. For instance, we will now make the probability prob of the random binomial number generator a function of the coordinates: trend1 &lt;- mutate(trend1, heights = 160 + 20 * y + rnorm(n = nrow(pattern1), mean = 0, sd = 7)) If people have a preference for standing next to people about their same height, and shorter people have a preference for standing near the front, this is a possible map of heights in the formation: ggplot() + geom_point(data = trend1, aes(x = x, y = y, color = heights), shape = 15) + scale_color_distiller(palette = &quot;Spectral&quot;) + coord_fixed() As expected, shorter people are towards the “front” (bottom of the plot) and taller people towards the back. It is not a uniform process, since there is still some randomness, but a trend can be clearly appreciated. In the vast majority of cases, we do not know the process; that is precisely what we wish to infer. Understanding process generation in a statistical sense, as well as null landscapes, is a useful tool that can help to infer processes in applications with empirical (as opposed to simulated) data. This concludes Practice 3. "],
["statistical-maps-maps-as-processes-activity.html", "7 Statistical Maps: Maps as Processes - Activity 7.1 Practice questions 7.2 Learning objectives 7.3 Suggested reading 7.4 Preliminaries 7.5 Activity", " 7 Statistical Maps: Maps as Processes - Activity 7.1 Practice questions Answer the following questions: What is a Geographic Information System? What distinguishes a statistical map from other types of mapping techniques? What is a null landscape? 7.2 Learning objectives In this activity, you will: Learn about stochastic processes. Simulate landscapes using various types of processes. Discuss the difference between random and non-random landscapes. Think about ways to decide whether a landscape is random. 7.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 4. John Wiley &amp; Sons: New Jersey. 7.4 Preliminaries For this activity you will need the following: This R markdown notebook. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) In the practice that preceded this activity, you learned how to simulate null landscapes and spatial processes. In this activity, the concept of a stochastic process is introduced. Some processes are random, such as the ones used in the practice to create null landscapes. These processes take values with some probability, but cannot be predicted with any certainty. Let’s illustrate using again a unit square: coords &lt;- expand.grid(x = seq(from = 0, to = 1, by = 0.05), y = seq(from = 0, to = 1, by = 0.05)) Here is an example of a random pattern of events: events &lt;- rbinom(n = nrow(coords), size = 1, prob = 0.5) null_pattern &lt;- data.frame(coords, events) ggplot() + geom_point(data = subset(null_pattern, events == 1), aes(x = x, y = y), shape = 15) + coord_fixed() A systematic or deterministic process is one that contains no elements of randomness, and can therefore be predicted with complete certainty. For instance (note the use of xlim to set the extent of x axis in the plot): deterministic_point_pattern &lt;- coords deterministic_point_pattern &lt;- mutate(deterministic_point_pattern, events = round(x)) ggplot() + geom_point(data = subset(deterministic_point_pattern, events == 1), aes(x = x, y = y), shape = 15) + xlim(0, 1) + coord_fixed() In the process above, I used the function round() and the coordinate x. The function gives a value of one for all points with x &gt; 0.5, and a value of zero to all points with x &lt;= 0.5. The pattern is fully deterministic: if I know the value of the x coordinate I can predict whether an event will be present. A stochastic process, on the other hand, is a process that is neither fully random or deterministic, but rather a combination of the two. Let’s illustrate: stochastic_point_pattern &lt;- coords stochastic_point_pattern &lt;- mutate(stochastic_point_pattern, events = round(x) - round(x) * rbinom(n = nrow(coords), size = 1, prob = 0.5)) ggplot() + geom_point(data = subset(stochastic_point_pattern, events == 1), aes(x = x, y = y), shape = 15) + xlim(0, 1) + coord_fixed() The process above has a deterministic component (the probability of an event is zero if x &lt;= 0.5), and a random component (the probability of a coordinate being an event is 0.5 when x &gt; 0.5). 7.5 Activity Simulate and plot a landscape using a random, stochastic, or deterministic process. It is your choice whether to simulate a point pattern or a continuous variable. Share the map with a fellow student, and ask them to guess whether the map is random or non-random. Repeat 1 and 2 a few times. Propose one or more ways to decide whether a landscape is random, and explain your reasoning. The approach does not need to be the same for point patterns and continuous variables! "],
["point-pattern-analysis-i.html", "8 Point Pattern Analysis I 8.1 Introduction 8.2 Learning objectives 8.3 Suggested reading 8.4 Preliminaries 8.5 Point patterns 8.6 Processes and point patterns 8.7 Intensity and density 8.8 Quadrats and density maps", " 8 Point Pattern Analysis I 8.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In last practice your learning objectives were: How to generate random numbers with different properties. About Null Landscapes. How to create new columns in a dataframe using a formula. How to simulate a spatial process. Please review the previous practices if you need a refresher on these concepts. For this practice you will need the following: This R markdown notebook. A dataset called Data3.RData This dataset includes a dataframe with four sets of spatial events, labeled as “Pattern 1”, “Pattern 2”, “Pattern 3”, “PointPattern4”, with n = 60 events in each set. 8.2 Learning objectives In this practice, you will learn: A formal definition of point pattern. Processes and point patterns. The concepts of intensity and density. The concept of quadrats and how to create density maps. More ways to control the look of your plots, in particular faceting and adding lines. 8.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 8.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(spatstat) ## Warning: package &#39;spatstat&#39; was built under R version 3.4.4 ## Loading required package: spatstat.data ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse ## Loading required package: rpart ## ## spatstat 1.55-1 (nickname: &#39;Gamble Responsibly&#39;) ## For an introduction to spatstat, type &#39;beginner&#39; Load the data that you will use for this practice: load(&quot;Data3.RData&quot;) 8.5 Point patterns Previously you created different types of maps and learned about different kinds of processes (i.e., random, stochastic, deterministic). A map that you have seen in several occasions is one where the coordinates of an even of interest are available. The simplest kind of data of this type is when only the coordinates are available: we call this a point pattern. A point pattern is given by a set of events of interest that are observed in a region \\(R\\). A region has an infinite number of points, essentially coordinates \\((x_i, y_i)\\) on the plane. The number of points is infinite, because there is a point defined by, say, coordinates (1,1), and also a point for coordinates (1.1,1), and for coordinates for (1.01,1), and so on. Any location that can be described by a set of coordinates contained in the region is a point. Not all points are events, however. An event is defined as a point where something of interest happened. This could be the location where a tree exists, or a crime happened, the epicenter of an earthquake, a case of a disease was reported, and so on. There might be one such occurrence, or more. Each event is be denoted by: \\[ \\textbf{s}_i \\] with coordinates: \\[ (x_i,y_i). \\] Sometimes other attributes of the events have been measured as well. For example, the event could be an address where cholera was reported (as in John Snow’s famous map). In addition to the address (which can be converted into the coordinates of the event), the number of cases could be recorded. Other examples could be the height and diameter of trees, the magnitude of an earthquake, and so on. It is important, for reasons that will be discussed later, that the point pattern is a complete enumeration. What this means is that every event that happened has been recorded! Interpretation of most analysis becomes dubious if the events are only sampled, that is, if only a few of them have been recorded. 8.6 Processes and point patterns Point patterns are interesting in many applications. In these application, a key question of interest is whether the pattern is random. Imagine, for instance, a point pattern that records crimes in a region. The pattern might be random, in which case there is no way to anticipate where the next occurrence will be. Non-random patterns, on the other hand, are likely the outcome of some meaningful process. For instance, crimes might cluster as a consequence of some common environmental variable (e.g., concentration of wealth). On the contrary, they might repeal each other (e.g., the location of a crime draws attention of law enforcement, and therefore the next occurrence of a crime tends to happen away from it). Deciding whether the pattern is random or not is the initial step towards developing hypotheses about the underlying process. Consider for example the following patterns. To create the following figure, you can use faceting by means of ggplot2::facet_grid: ggplot() + geom_point(data = PointPatterns, aes(x = x, y = y)) + facet_grid(.~ Pattern) + coord_fixed() As you can see, faceting is a convenient way to simultaneously plot different parts of a dataframe (in the present case, the different Patterns). In a previous discussion, a few ideas were suggested as possible ways of deciding whether a map of events (i.e., a point pattern) is random. One idea was to consider the intensity of the process. This is discussed in more detail next. 8.7 Intensity and density The intensity of a spatial point process is the expected number of events per unit area. This is conventionally denoted by the greek letter \\(\\lambda\\). In most cases the process is not know, so its intensity cannot be directly measured. In its place, the density of the point pattern is taken as the empirical estimate of the intensity of the underlying process. The density of the point pattern is calculated very simply as the number of events divided by the area of the region, that is: \\[ \\hat{\\lambda} = \\frac{(S \\in R)}{a} = \\frac{n}{a}. \\] Notice the use of the “hat” symbol on top of the Greek lambda. This symbol is called “caret”. The hat notation is used to indicate an estimated value of an unobserved parameter of a process, in the present case the intensity of the spatial point process. Consider one of the point patterns in your sample dataset, say “Pattern 1”. Let’s summarize it: summary(subset(PointPatterns, Pattern == &quot;Pattern 1&quot;)) ## x y Pattern ## Min. :0.0285 Min. :0.005306 Pattern 1:60 ## 1st Qu.:0.3344 1st Qu.:0.236509 Pattern 2: 0 ## Median :0.5247 Median :0.500262 Pattern 3: 0 ## Mean :0.5531 Mean :0.500248 Pattern 4: 0 ## 3rd Qu.:0.8417 3rd Qu.:0.761218 ## Max. :0.9888 Max. :0.999808 We see that there are \\(n = 60\\) points in this dataset. Since the region is the unit square (check how the values of the coordinates range from approximately zero to approximately 1), the area of the region is 1. This means that for “Pattern 1”: \\[ \\hat{\\lambda} = \\frac{60}{1} = 60 \\] This is the overall density of the point pattern. 8.8 Quadrats and density maps The overall density of a point process (calculated above) can be mapped by means of the geom_bin2d function of the ggplot2 package. This function divides two dimensional space into bins and reports the number of events or the density of the events in the bins. Let’s give this a try: ggplot() + geom_bin2d(data = subset(PointPatterns, Pattern == &quot;Pattern 1&quot;), aes(x = x, y = y), binwidth = c(1, 1)) + coord_fixed() Let’s see step-by-step how this plot is made. ggplot() creates a plot object. geom_bin2d is called to plot a map of counts of events in the space defined by the bins. The dataframe used for plotting the bins is PointPatterns, subset so that only the points in “Pattern 1” are used. The coordinates x and y are used to plot (in aes(), we indicate that x in the dataframe corresponds to the x axis in the plot, and y in the dataframe corresponds to y axis in the plot) The size of the bin is defined as 1-by-1 (binwidth = c(1, 1)) coord_fixed is applied to ensure that the aspect ratio of the plot is one (one unit of x is the same length as one unit of y in the plot). The map of the overall density of the process above is not terribly interesting. It only reports what we already knew, that globally the density of the point pattern is 60. It would be more interesting to see how the density varies across the region. We do this by means of the concept of quadrats. Imagine that instead of calculating the overall (or global) intensity of the point pattern, we subdivided the region into a set of smaller subregions. For instance, we could draw horizontal and vertical lines to create smaller squares: ggplot() + geom_vline(xintercept = seq(from = 0, to = 1, by = 0.25)) + geom_hline(yintercept = seq(from = 0, to = 1, by = 0.25)) + geom_point(data = subset(PointPatterns, Pattern == &quot;Pattern 1&quot;), aes(x = x, y = y)) + coord_fixed() Notice the following functions used to create the vertical lines (geom_vline) and horizontal lines (geom_hline), from 0 to 1 every 0.25 units of distance respectively. Each of the smaller squares used to subdivide the region is called a quadrat. To make things more interesting, instead of calculating the overall density, we can calculate the density for each quadrat. Let’s visualize a density map using quadrats: ggplot() + geom_bin2d(data = subset(PointPatterns, Pattern == &quot;Pattern 1&quot;), aes(x = x, y = y), binwidth = c(0.50, 0.50)) + geom_point(data = subset(PointPatterns, Pattern == &quot;Pattern 1&quot;), aes(x = x, y = y)) + scale_fill_distiller(palette = &quot;RdBu&quot;) + coord_fixed() You can, of course, change the size of the quadrats. Let’s take a look at the four point patterns (by means of faceting), after creating a variable to easily control the size of the quadrat. Let’s call this variable q_size: q_size &lt;- 0.25 ggplot() + geom_bin2d(data = PointPatterns, aes(x = x, y = y), binwidth = c(q_size, q_size)) + geom_point(data = PointPatterns, aes(x = x, y = y)) + facet_grid(.~ Pattern) + scale_fill_distiller(palette = &quot;RdBu&quot;) + coord_fixed() Notice the differences in the density maps? Try changing the size of the quadrat to 1. What happens, and why? Next, try a smaller quadrat size, say 0.25. What happens, and why? Try even smaller quadrat sizes, but greater than zero. What happens now? The package spatstat includes numerous functions for the analysis of point patterns. A relevant function at this point, is quadratcount, which returns the number of events per quadrat. To use this function, we need to convert the point patterns to a type of data used by spatstat denominated ppp (for plannar point pattern). This is simple, thanks to a utility function in spatstat called as.ppp. This function takes as arguments (inputs) a set of coordinates, and data to define a window. Let’s convert the spatial patterns to ppp objects. First, define the window by means of the owin function, and using the 0 to 1 interval for our region: Wnd &lt;- owin(c(0,1), c(0,1)) Now, a ppp object can be created: ppp1 &lt;- as.ppp(PointPatterns, Wnd) If you examine these new ppp objects, you will see that they pack the same basic information (i.e., the coordinates), but also the range of the region and so on: summary(ppp1) ## Marked planar point pattern: 240 points ## Average intensity 240 points per square unit ## ## Coordinates are given to 8 decimal places ## ## Multitype: ## frequency proportion intensity ## Pattern 1 60 0.25 60 ## Pattern 2 60 0.25 60 ## Pattern 3 60 0.25 60 ## Pattern 4 60 0.25 60 ## ## Window: rectangle = [0, 1] x [0, 1] units ## Window area = 1 square unit As you can see, the ppp object includes the four patterns, calculates the frequency of each (the number of events), and their respective overall intensities. Once the patterns are in ppp form, quadratcount can be used to compute the counts of events. To calculate the count separately for each pattern, you need to use split.ppp(). The other two arguments are the number of quadrats in the horizontal (nx) and the vertical (ny) directions: quadratcount(split.ppp(ppp1), nx = 4, ny = 4) ## List of spatial objects ## ## Pattern 1: ## x ## y [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1] ## [0.75,1] 3 5 1 6 ## [0.5,0.75) 2 3 4 6 ## [0.25,0.5) 5 4 2 3 ## [0,0.25) 2 4 4 6 ## ## Pattern 2: ## x ## y [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1] ## [0.75,1] 14 2 2 6 ## [0.5,0.75) 0 0 4 6 ## [0.25,0.5) 6 3 1 2 ## [0,0.25) 4 6 2 2 ## ## Pattern 3: ## x ## y [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1] ## [0.75,1] 2 11 5 7 ## [0.5,0.75) 1 1 6 4 ## [0.25,0.5) 1 10 3 2 ## [0,0.25) 2 1 2 2 ## ## Pattern 4: ## x ## y [0,0.25) [0.25,0.5) [0.5,0.75) [0.75,1] ## [0.75,1] 4 5 6 3 ## [0.5,0.75) 3 3 4 2 ## [0.25,0.5) 3 3 4 2 ## [0,0.25) 5 4 6 3 Compare the counts of the quadrats for each pattern. They should replicate what you observed in the density plots before. This concludes Practice 4. The next in-class activity will illustrate how quadrats are a useful tool to explore the question whether a map is random. "],
["point-pattern-analysis-i-activity.html", "9 Point Pattern Analysis I-Activity 9.1 Practice questions 9.2 Learning objectives 9.3 Suggested reading 9.4 Preliminaries 9.5 Activity", " 9 Point Pattern Analysis I-Activity 9.1 Practice questions Answer the following questions: What is a random process? What is a deterministic process? What is a stochastic process? What is the usefulness of a null landscape? 9.2 Learning objectives In this activity, you will: Use the concept of quadrats to analyze a real dataset. Learn about a quadrat-based test for randomness in point patterns. Learn how to use the p-value of a statistical test to make a decision. Think about the distribution of events in a null landscape. Think about ways to decide whether a landscape is random. 9.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 9.4 Preliminaries For this activity you will need the following: This R markdown notebook. A dataset called Toronto Business Points.RData. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(spatstat) In the practice that preceded this activity, you learned about the concepts of intensity and density, about quadrats, and also how to create density maps. Begin by loading the data that you will use in this activity: load(&quot;Toronto Business Points.RData&quot;) If you inspect your workspace, you will see that the following dataframes are there: Fast_Food Gas_Stands Paez_Mart Toronto These are locations of fast food restaurants and gas stands in Toronto (data are from 2008). Paez Mart on the other hand is my project to cover Toronto with convenience stores. The points are the planned locations of the stores. Toronto is the city boundary. Try plotting the following: ggplot() + geom_point(data = Fast_Food, aes(x = x, y = y)) + geom_polygon(data = Toronto, aes(x = long, y = lat, group = group), color = &quot;black&quot;, fill = NA, alpha = 1, size = .3) In addition to the dataframes, the following ppp objects are available: Fast_Food.ppp Gas_Stands.ppp Paez_Mart.ppp These include the same information, but for your convenience I have converted them to ppp for use with spatstat functions. For instance, you can calculate the counts of events by quadrat by means of quadrat.count. The input must be a ppp object, and the number of quadrats on the horizontal (nx) and vertical (ny) direction (notice how I use the function table to present the frequency of quadrats with number of events): q_count &lt;- quadratcount(Fast_Food.ppp, nx = 3, ny = 3) table(q_count) ## q_count ## 0 9 43 48 60 67 83 147 157 ## 1 1 1 1 1 1 1 1 1 As you see from the table, there is one quadrat with zero events, one quadrat with nine events, one quadrat with forty-three events, and so on. You can also plot the results of the quadratcount! plot(q_count) A useful function in the spatstat package is quadrat.test. This function implements a statistical test that compares the empirical distribution of events by quadrats to the distribution of events as expected under the hypothesis that the underlying process is random. This is implemented as follows: q_test &lt;- quadrat.test(Fast_Food.ppp, nx = 3, ny = 3) ## Warning: Some expected counts are small; chi^2 approximation may be ## inaccurate q_test ## ## Chi-squared test of CSR using quadrat counts ## Pearson X2 statistic ## ## data: Fast_Food.ppp ## X2 = 225.72, df = 8, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## ## Quadrats: 9 tiles (irregular windows) The quadrat test reports a p-value which can be used to make a decision. The p-value is the probability that you will be mistaken if you reject the null hypothesis. To make a decision, you need to know what is the null hypothesis, and your own tolerance for making a mistake. In the case above, the p-value is very, very small (2.2e-16 = 0.00000000000000022). Since the null hypothesis is spatial randomness, you can reject this hypothesis and the probability that you are making a mistake is vanishingly small. Try plotting the results of quadrat.test: plot(q_test) 9.5 Activity Use Fast_Food, Gas_Stands, Paez_Mart, and Toronto to create density maps for the three point patterns. Select a quadrat size that you think is appropriate. Show your maps to a fellow student. Did they select the same quadrat size? If not, what was their rationale for their size? Use Fast_Food.ppp, Gas_Stands, and Paez_Mart, and the function quadratcount to calculate the number of events per quadrat. Remember that you need to select the number of quadrats in the horizontal and vertical directions! Use the function table() to examine the frequency of events per quadrat for each of the point patterns. What are the differences among these point patterns? What would you expect the frequency of events per quadrat to be in a null landscape? Use Fast_Food.ppp, Gas_Stands, and Paez_Mart, and the function quadrat.test to calculate the test of spatial independence for these point patterns. What is your decision in each case? "],
["point-pattern-analysis-ii.html", "10 Point Pattern Analysis II 10.1 Introduction 10.2 Learning objectives 10.3 Suggested reading 10.4 Preliminaries 10.5 A quadrat-based test for spatial independence 10.6 Limitations of quadrat analysis: size and number of quadrats 10.7 Limitations of quadrat analysis: relative position of events 10.8 Kernel density", " 10 Point Pattern Analysis II 10.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In the last practice/session your learning objectives included: A formal definition of point pattern. Processes and point patterns. The concepts of intensity and density. The concept of quadrats and how to create density maps. More ways to control the look of your plots, in particular faceting and adding lines. Please review the previous practices if you need a refresher on these concepts. For this practice you will need the following: This R markdown notebook. Datasets called Data3.RData and Data4.RData. Data3.RData includes a dataframe with four sets of spatial events, labeled as “Pattern 1”, “Pattern 2”, “Pattern 3”, “PointPattern4”, with n = 60 events in each set. Data4.RData includes a spatstat ppp-class object with two sets of spatial events, labeled as “Pattern 1” and “Pattern 2” 10.2 Learning objectives In this practice, you will learn: The intuition behind the quadrat-based test of independence. About the limitations of quadrat-based analysis. The concept of kernel density. More ways to manipulate objects to do point pattern analysis using spatstat. 10.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 10.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(spatstat) Load the datasets that you will use for this practice: load(&quot;Data3.RData&quot;) load(&quot;Data4.RData&quot;) There is a dataframe called PointPatterns. You can check this by means of the function class class(). class(PointPatterns) ## [1] &quot;data.frame&quot; In addition, there is a ppp object with two point patterns: class(pp1) ## [1] &quot;ppp&quot; The summary for PointPatterns shows that these point patterns are located in a square-unit window (check the max and min values of x and y): summary(PointPatterns) ## x y Pattern ## Min. :0.0169 Min. :0.005306 Pattern 1:60 ## 1st Qu.:0.2731 1st Qu.:0.289020 Pattern 2:60 ## Median :0.4854 Median :0.550000 Pattern 3:60 ## Mean :0.5074 Mean :0.538733 Pattern 4:60 ## 3rd Qu.:0.7616 3rd Qu.:0.797850 ## Max. :0.9990 Max. :0.999808 As seen in the previous practice and activity, the package spatstat employs a type of object called ppp (for planar point pattern). Fortunately, it is relatively simple to convert a dataframe into a ppp object by means of as.ppp(). This function requires that you define a window for the point pattern, something we can do by means of the owin function: W &lt;- owin(xrange = c(0, 1), yrange = c(0, 1)) Now you can convert the dataframe into a ppp object: PointPatterns.ppp &lt;- as.ppp(PointPatterns, W) 10.5 A quadrat-based test for spatial independence In the preceding activity, you used a quadrat-based spatial independence test to help you decide whether a pattern was random (the function was quadrat.test). We will now review the intuition of the test. Let’s begin by plotting the patterns. You can use split to do plots for each pattern separately, instead of putting all of them in a single plot (these plots are not as responsive as ggplot2 but are quick): plot(split(PointPatterns.ppp)) Notice that you can also plot individual patterns by using $ followed by the factor that identifies the desired pattern: plot(split(PointPatterns.ppp)$&quot;Pattern 4&quot;) Now calculate the quadrat-based test of independence: q_test &lt;- quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 3, ny = 3) q_test ## ## Chi-squared test of CSR using quadrat counts ## Pearson X2 statistic ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 48, df = 8, p-value = 1.976e-07 ## alternative hypothesis: two.sided ## ## Quadrats: 3 by 3 grid of tiles Plot the results of the quadrat test: plot(q_test) As discussed in the previous session, the expected distribution of events on quadrats under the null landscape tends to be quite even. This is because each quadrat has equal probability of having the same number of events (depending on size, when the quadrats are not all the same size). If you check the plot of the quadrat test above, you will notice that the first number (top left corner) is the number of events in the quadrat. The second number (top right corner) is the expected number of events for a null landscape. The third number is a residual, based on the difference between the observed and expected number of events. More specifically, the residual is a Pearson residual, defined as follows: \\[ r_i=\\frac{O_i - E_i}{\\sqrt{E_i}}, \\] where \\(O_i\\) is the number of observed events in quadrat \\(i\\) and \\(E_i\\) is the number of expected events in quadrat \\(i\\). When the number of observed events is similar to the number of expected events, \\(r_i\\) will tend to be a small value. As the difference grows, the residual will also grow. The independence test is calculated from the residuals as: \\[ X2=\\sum_{i=1}^{Q}r_i^2, \\] where \\(Q\\) is the number of quadrats. In other words, the test is based on the squared sum of the Pearson residuals. The smaller this number is, the more likely that it is not different from the null landscape (i.e., a random process), and the larger it is, the more likely that it is different from the null landscape. This is reflected by the p-value of the test (technically, the p-value is obtained from comparing the test to the Chi-square distribution). Consider for instance the first pattern in the examples: plot(quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 1&quot;, nx = 3, ny = 3)) You can see that the Pearson residual of the top left quadrat is indeed -0.6567673, the next to its right is -0.2704336, and so on. The value of the test statistic should be then: paste(&quot;X2 = &quot;, (-0.65)^2 + (-0.26)^2 + (0.52)^2 + (-0.26)^2 + (0.9)^2 + (0.52)^2 + (-1)^2 + (0.13)^2 + (0.13)^2) ## [1] &quot;X2 = 2.9423&quot; Which you can confirm by examining the results of the test (the small difference is due to rounding errors): quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 1&quot;, nx = 3, ny = 3) ## ## Chi-squared test of CSR using quadrat counts ## Pearson X2 statistic ## ## data: split(PointPatterns.ppp)$&quot;Pattern 1&quot; ## X2 = 3, df = 8, p-value = 0.1313 ## alternative hypothesis: two.sided ## ## Quadrats: 3 by 3 grid of tiles Explore the remaining patterns. You will notice that the residuals and test statistic tend to grow as more events are concentrated in space. In this way, the test is a test of density of the quadrats: is their density similar to what would be expected from a null landscape? 10.6 Limitations of quadrat analysis: size and number of quadrats As hinted by the previous activity, one issue with quadrat analysis is the selection of the size for the quadrats. Changing the size of the quadrats has an impact on the counts, and in turn on the aspect of density plots and even the results of the test of independence. For example, the results of the test for “Pattern 2” in the dataset change when the number of quadrats is modified. For instance, with a small number of quadrats: quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 2, ny = 1) ## ## Chi-squared test of CSR using quadrat counts ## Pearson X2 statistic ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 1.6667, df = 1, p-value = 0.3934 ## alternative hypothesis: two.sided ## ## Quadrats: 2 by 1 grid of tiles Compare to four quadrats: quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 2, ny = 2) ## ## Chi-squared test of CSR using quadrat counts ## Pearson X2 statistic ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 6, df = 3, p-value = 0.2232 ## alternative hypothesis: two.sided ## ## Quadrats: 2 by 2 grid of tiles And: quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 3, ny = 2) ## ## Chi-squared test of CSR using quadrat counts ## Pearson X2 statistic ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 23.2, df = 5, p-value = 0.0006182 ## alternative hypothesis: two.sided ## ## Quadrats: 3 by 2 grid of tiles Why is the statistic generally smaller when there are fewer quadrats? A different issue emerges when the number of quadrats is large: quadrat.test(split(PointPatterns.ppp)$&quot;Pattern 2&quot;, nx = 4, ny = 4) ## Warning: Some expected counts are small; chi^2 approximation may be ## inaccurate ## ## Chi-squared test of CSR using quadrat counts ## Pearson X2 statistic ## ## data: split(PointPatterns.ppp)$&quot;Pattern 2&quot; ## X2 = 47.2, df = 15, p-value = 6.84e-05 ## alternative hypothesis: two.sided ## ## Quadrats: 4 by 4 grid of tiles A warning now tells you that some expected counts are small: space has been divided so minutely, that the expected number of events per quadrat has become too thin; as a consequence, the approximation to the probability distribution may be inaccurate. While there are no hard rules to select the size/number of quadrats, the following rules of thumb are sometimes suggested: Each quadrat should have a minimum of two events. The number of quadrats is selected based on the area (A) of the region, and the number of events (n): \\[ Q=\\frac{2A}{N} \\] Caution should be exercised when interpreting the results of the analysis based on quadrats, due to the issue of size/number of quadrats. 10.7 Limitations of quadrat analysis: relative position of events Another issue with quadrat analysis is its insensibility to the relative position of the events within the quadrats. Consider for instance the following two patterns in pp1: plot(split(pp1)) These two patterns look quite different. And yet, when we count the events by quadrats: plot(quadratcount(split(pp1), nx = 3, ny = 3)) This example highlights how quadrats are relatively coarse measures of density, and fail to distinguish between fairly different event distributions, in particular because quadrat analysis does not take into account the relative position of the events with respect to each other. 10.8 Kernel density In order to better take into account the relative position of the events with respect to each other, a different technique can be devised. Imagine that a quadrat is a kind of “window”. We use it to observe the landscape. When we count the number of events in a quadrat, we simply peek through that particular window: all events inside the “window” are simply counted, and all events outside the “window” are ignored. Then we visit another quadrat and do the same, until we have visited all quadrats. Imagine now that we define a window that, unlike the quadrats which are fixed, can move and visit different points in space. This window also has the property that, instead of counting the events that are in the window, it gives greater weight to events that are close to the center of the window, and less weight to events that are more distant from the center of the window. We can define such a window by selecting a function that declines with increasing distance. We will call this function a kernel. An example of a function that can work as a moving window is the following. ggplot(data = data.frame(dist = c(-3, 3)), aes(dist)) + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylim(c(0, 0.75)) As you can see, the value of the function declines with increasing distance from the center of the window (when dist == 0; note that the value never becomes zero!). This is a Gaussian kernel. The shape of the Gaussian kernel depends on a variance parameter, which we will call the kernel bandwidth. The bandwidth controls how rapidly the weight assigned to distant events decays. Change the value of the argument sd in the chunk above. You will see that as it becomes smaller, the slope of the kernel becomes steeper (and distant observations are downweighted rapidly). On the contrary, as it becomes larger, the slope becomes less steep (and distant events are weighted almost as highly as close events). Kernel density estimates are usually obtained by creating a fine grid that is superimposed on the region. The kernel function then visits each point on the grid and obtains an estimate of the density by summing the weights of all events as per the kernel function. Kernel density is implemented in spatstat and can be used as follows. The input is a ppp object, and optionally a sigma argument that corresponds to the bandwidth of the kernel: kernel_density &lt;- density(split(pp1), sigma = 0.1) plot(split(pp1)) plot(kernel_density) The plots above illustrate how the map of the kernel density is better able to capture the variations in density across the region. In fact, kernel density is a smooth estimate of the underlying intensity of the process, and the degree of smoothing is controlled by the bandwidth. This concludes Practice 5. "],
["point-pattern-analysis-ii-activity.html", "11 Point Pattern Analysis II-Activity 11.1 Practice questions 11.2 Learning objectives 11.3 Suggested reading 11.4 Preliminaries 11.5 Activity", " 11 Point Pattern Analysis II-Activity 11.1 Practice questions Answer the following questions: How does the quadrat-based test of independence respond to a small number of quadrats? How does the quadrat-based test of independence respond to a large number of quadrats? What are the limitations of quadrat analysis? What is a kernel function? How does the bandwidth affect a kernel function? 11.2 Learning objectives In this activity, you will: Explore a dataset using quadrats and kernel density. Experiment with different parameters (number/size of kernels and bandwidths). Discuss the impacts of selecting different parameters. Hypothesize about the underlying spatial process based on your analysis. 11.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 11.4 Preliminaries For this activity you will need the following: This R markdown notebook. A dataset called Scandinavian Bear Project Example.RData. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(spatstat) In the practice that preceded this activity, you learned about the concepts of intensity and density, about quadrats, and also how to create density maps. Begin by loading the data that you will use in this activity: load(&quot;Scandinavian Bear Project Example.RData&quot;) If you inspect your workspace, you will see that the following ppp object is there: bear.ppp This dataset was sourced from the Scandinavia Bear Project, a Swedish-Noruegian collaboration that aims to study the ecology of brown bears, to provide decision makers with evidence to support bear management, and to provide information regarding bears to the public. You can learn more about this project here. The project involves tagging bears with GPS units, so that their movements can be tracked. The dataset includes coordinates of one bear’s movement over a period of several weeksin 2004. The dataset was originally taken from the adehabitatLT package but was somewhat simplified for this activity. Instead of full date and time information, the point pattern is marked more simply as “Day Time” and “Night Time”, to distinguish between diurnal and nocturnal activity of the bear. You can check the contents of the ppp object by means of summary: summary(bear.ppp) ## Marked planar point pattern: 1000 points ## Average intensity 2.466078e-05 points per square unit ## ## Coordinates are given to 1 decimal place ## i.e. rounded to the nearest multiple of 0.1 units ## ## Multitype: ## frequency proportion intensity ## Day Time 502 0.502 1.237971e-05 ## Night Time 498 0.498 1.228107e-05 ## ## Window: polygonal boundary ## single connected closed polygon with 11 vertices ## enclosing rectangle: [515648.4, 523260.3] x [6812095, 6821583] units ## Window area = 40550200 square units ## Fraction of frame area: 0.562 11.5 Activity Analyze the point pattern for the movements of the bear using quadrat and kernel density methods. Experiment with different quadrat sizes and kernel bandwidths. Explain your choice of parameters (quadrat sizes and kernel bandwidths) to a fellow student. Decide whether these patterns are random, and support your decision. Do you see differences in the activity patterns of the bear by time of day? What could explain those differences, if any? Discuss the limitations of your conclusions, and of quadrat/kernel (density-based) approaches more generally. "],
["point-pattern-analysis-iii.html", "12 Point Pattern Analysis III 12.1 Introduction 12.2 Learning objectives 12.3 Suggested reading 12.4 Preliminaries 12.5 Motivation 12.6 Nearest neighbors 12.7 G-function", " 12 Point Pattern Analysis III 12.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In the last practice/session your learning objectives included: The intuition behind the quadrat-based test of independence. The concept of kernel density. The limitations of density-based analysis More ways to work with ppp objects. Please review the previous practices if you need a refresher on these concepts. For this practice you will need the following: This R markdown notebook. #* A dataset called Data4.RData. Data4.RData includes a spatstat ppp-class object with two sets of spatial events, labeled as “Pattern 1” and “Pattern 2” 12.2 Learning objectives In this practice, you will learn: About clustered and dispersed (or regular) patterns. The concept of nearest neighbors. About distance-based methods for point pattern analysis. About the G-function for the analysis of event-to-event nearest neighbor distances. 12.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 12.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(spatstat) Load the datasets that you will use for this practice: load(&quot;Data4.RData&quot;) 12.5 Motivation Quadrats and kernel density are examples of density-based analysis. These techniques are useful to help you understand the large scale variation of a distribution of events, but as previously discussed, may sometimes be less informative by not taking into account small scale variations in the locations of the events. For this reason, the following two patterns, despite being very different, give identical number of counts per quadrat: plot(split(pp1)) plot(quadratcount(split(pp1), nx = 3, ny = 3)) The two patterns above have similar density, However, “Pattern 1” displays clustering, a situation characterized by events generally being in close proximity to others. “Pattern 2”, on the other hand, displays dispersion or regularity, a situation where points tend to be located at fairly regular distances from each other. With some fiddling of the parameters, quadrats can be coaxed to tease out the variations in density, for instance: plot(quadratcount(split(pp1), nx = 9, ny = 9)) As a visualization technique, this gives a better sense of the variations in density. However, as noted previously, the quality of the test deteriorates when there are many quadrats with small counts. As an alternative, kernel density can be used to visualize the smoothed estimate of the density: plot(density(split(pp1), sigma = 0.075)) However, even when we can visualize the variations in density, we cannot, from the kernel estimate alone, tell if high/low values exceed those of a null landscape — in other words, we lack at the moment a way to test the hypothesis that the density is higher than what would be expected from a null landscape. In this practice you will learn about a family of techniques that instead of measuring the density, explore patterns by means of distance distributions. 12.6 Nearest neighbors Let us begin by introducing the concept of a nearest neighbor. The nearest neighbor of a location is the event that is closest to said location given some metric. This metric is usually Euclidian distance on the plane, that is, distance as measured using a straight line between the location and the event. In principle, the metric can be selected according to the characteristics of a dataset: this could be Euclidean distance, great circle distance, or network distance, for events on networks, for instance (see Figure 1). Figure 1. Examples of distance metrics In this way, the nearest neighbor is the event j with the shortest separation from location i: \\[ \\text{Event }j\\text{ is the nearest neighbor of location }i\\text{ if: }d_{ij}\\le d_{ik} \\forall k \\] Ties are relatively rare in most realistic point patterns (even in regular patterns), and may not have a big impact on the analysis. The package spatstat includes functions to calculate Euclidean distances. Three functions are relevant: pairdist(): returns the pairwise distance between all pairs of events i and j. nndist(): returns a vector of distances from events to to their corresponding nearest neighbors; these distances are obtained by sorting the pairwise distances, and selecting the minimum value for each event. distmap(): returns a pixel image with the distance from each pixel to the nearest event; in effect this is a map of the distances between empty spaces and their corresponding nearest events. With these functions we can calculate, for instance, the following distances: pp1_nn1 &lt;- nndist(split(pp1)$&quot;Pattern 1&quot;) Let us explore the distribution of these distances by means of a histogram: ggplot() + geom_histogram(data = data.frame(dist = pp1_nn1), aes(dist), binwidth = 0.03) Notice how most events have a nearest neighbor at a relatively short distance (&lt;0.05). Compare to the distribution of distances in “Pattern 2” of pp1: pp1_nn2 &lt;- nndist(split(pp1)$&quot;Pattern 2&quot;) ggplot() + geom_histogram(data = data.frame(dist = pp1_nn2), aes(dist), binwidth = 0.03) In this case, most events have a nearest neighbot at a distance of approximately 0.15. Another useful tool is a Stienen diagram. A Steinen diagram is essentially a proportional symbol plot of the events with symbols of size proportional to the distance to their nearest neighbor. For example, for “Pattern 1” in pp1 (Notice the use of %mark% to add an attribute to the ppp object; the attribute is the distance to the nearest neighbor): plot(split(pp1)$&quot;Pattern 1&quot; %mark% (pp1_nn1), markscale = 1, main = &quot;Stienen diagram&quot;) In this diagram, the largest circle is not very large: even events that are relatively isolated are not a long distance away from their nearest neighbor. This fits the definition of clustering as situation where events are close to other events. Compare to “Pattern 2”: plot(split(pp1)$&quot;Pattern 2&quot; %mark% (pp1_nn2), markscale = 1, main = &quot;Stienen diagram&quot;) Notice how all circles are very similar in size: this fits the definition of dispersion, where events are more or less equally distant from their nearest neighbors. Lets use the function runifpoint from the spatstat package to generate a null landscape: rand_ppp &lt;- runifpoint(36) If we plot the Stienen diagram for this point pattern: rand_nn &lt;- nndist(rand_ppp) plot(rand_ppp %mark% (rand_nn), markscale = 1, main = &quot;Stienen diagram&quot;) In a null landscape, the distribution of the size of the symbols would tend to be random! The concept of nearest neighbors is useful to define a family of techniques that are based on the distribution of distances to nearest neighbors. Three such techniques are introduced here. 12.7 G-function As you have seen above, the distribution of distances to nearest neighbors presents distinctive characteristics for different types of patterns. What is needed is a convenient way to summarize the distribution of distances to nearest neighbors. A way to do so is by means of a plot of the cumulative distribution function. A cumulative distribution is simply the proportion of events that are have a nearest neighbor at a distance less than some value x. When the value of x is very small, no events have a nearest neighbor at \\(d_{ij}&lt;x\\). When x is very large all events have a nearest neighbor at \\(d_{ij}&lt;x\\). The cumulative distribution thus depends on the value of x. Imagine for instance the following hypothetical distribution of distances of ten events to their nearest neighbors (the first event’s nearest neighbor is at a distance of 1, the second event’s nearest neighbor is at 2, the third’s at 0.5, and so on): nnd &lt;- c(1, 2, 0.5, 2.5, 1.7, 4, 3.5, 1.2, 2.3, 2.8) When x = 0, zero events have a nearest neighbor. When x = 1, two events have nearest neighbor at dist &lt;= 1. When x = 2, five events have a nearest neighbor at dist &lt;= 2. When x = 3, eight events have a nearest neighbor at dist &lt;= x. When x = 4, ten out of ten events have a nearest neighbor at dist &lt;= 4. We can plot these numbers of events as a proportion: df &lt;- data.frame(x = c(0, 1, 2, 3, 4), proportion = c(0, 3/10, 5/10, 8/10, 10/10)) ggplot() + geom_line(data = df, aes(x = x, y = proportion)) The cumulative distribution function of distances from event to nearest neighbor is called a G-function. This function is defined as follows, with \\(d_{ik}\\) as the distance from the event at i to its nearest neighbor: \\[ \\hat{G}(x)=\\frac{(d_{ik}\\le x, \\forall i)}{n} \\] This function (with a hat, because it is estimated from the data), can be used to explore the spatial point pattern. When doing so, it is useful to know that the theoretical value of G (assuming a null landscape generated by a Poisson distribution) is as follows: \\[ G_{pois}(x) = 1 - exp(-\\lambda \\pi x^2). \\] When the empirical \\(\\hat{G}(x)\\) is greater than the theoretical function, this suggests that the events tend to be closer than expected, compared to the null landscape. On the contrary, when the empirical function is less than the theoretical function, this would suggest a dispersed pattern. The G-function is implemented in spatstat as Gest (for G estimated): g_pattern1 &lt;- Gest(split(pp1)$&quot;Pattern 1&quot;, correction = &quot;none&quot;) (For the moment ignore the argument “correction”; we will discuss corrections later on.) The plot function can be used to visualize the estimated G (with r = x): plot(g_pattern1) In the plot above, the empirical function is the solid black line, and the theoretical is the dashed red line. If you examine these functions, you will see that about 50% of events have a nearest neighbor at a distance of less than approximately 0.04. In the null landscape, in contrast, only about 16% of events have a nearest neighbor at less than 0.04: plot(g_pattern1) lines(x = c(0.04, 0.04), y = c(-0.1, 0.5), lty = &quot;dotted&quot;) lines(x = c(-0.1, 0.04), y = c(0.5, 0.5), lty = &quot;dotted&quot;) lines(x = c(-0.1, 0.04), y = c(0.16, 0.16), lty = &quot;dotted&quot;, col = &quot;red&quot;) What this suggests is that in the actual landscape events tend to be much closer to other events in comparison the null landscape, and would therefore be suggestive of clustering. Compare to “Pattern 2”: g_pattern2 &lt;- Gest(split(pp1)$&quot;Pattern 2&quot;, correction = &quot;none&quot;) plot(g_pattern2) Now the empirical function is below the one for the null landscape. Notice too that all events have a nearest neighbor in a limited range of distances, between 0.14 and 0.18. This is indicative of a dispersed, or regular pattern. And the random pattern that you created before: g_pattern_rnd &lt;- Gest(rand_ppp, correction = &quot;none&quot;) plot(g_pattern_rnd) In this case, the empirical function more closely resembles the theoretical one for the null landscape. By considering the distribution of distances to nearest neighbors, you can generate additional information on a point pattern to complement the density-based analysis of the preceding sessions. This concludes Practice 6. "],
["point-pattern-analysis-iii-activity.html", "13 Point Pattern Analysis III-Activity 13.1 Practice questions 13.2 Learning objectives 13.3 Suggested reading 13.4 Preliminaries 13.5 Activity", " 13 Point Pattern Analysis III-Activity 13.1 Practice questions Answer the following questions: List and explain two limitations of quadrat analysis. What is clustering? What could explain a clustering in a set of events? What is regularity? What could explain it? Describe the concept of nearest neighbors. What is a cumulative distribution function? 13.2 Learning objectives In this activity, you will: Explore a dataset using distance-based approaches. Compare the characteristics of different types of patterns. Discuss ways to evaluate how confident you are that a pattern is random. 13.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 13.4 Preliminaries For this activity you will need the following: This R markdown notebook. A dataset called Scandinavian Bear Project Example.RData. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(spatstat) In the practice that preceded this activity, you learned about the concepts of intensity and density, about quadrats, and also how to create density maps. Begin by loading the data that you will use in this activity: load(&quot;Toronto Business Points.RData&quot;) If you inspect your workspace, you will see that the following ppp objects are there: Fast_Food.ppp Gas_Stands.ppp Paez_Mart.ppp These are locations of fast food restaurants and gas stands in Toronto (data are from 2008). Paez Mart on the other hand is my project to cover Toronto with convenience stores. The points are the planned locations of the stores. Please note that the exact same data are provided in dataframe formand that Toronto is the city boundary. You do not need to use these dataframes for the present activity. You can check the contents of ppp objects by means of summary: summary(Fast_Food.ppp) ## Marked planar point pattern: 614 points ## Average intensity 8663.712 points per square unit ## ## Coordinates are given to 6 decimal places ## ## Multitype: ## frequency proportion intensity ## Chicken 82 0.1335505 1157.043 ## Hamburger 209 0.3403909 2949.048 ## Pizza 164 0.2671010 2314.086 ## Sub 159 0.2589577 2243.534 ## ## Window: polygonal boundary ## 10 separate polygons (no holes) ## vertices area relative.area ## polygon 1 4185 7.05052e-02 9.95e-01 ## polygon 2 600 2.82935e-04 3.99e-03 ## polygon 3 193 2.64610e-05 3.73e-04 ## polygon 4 28 2.96067e-06 4.18e-05 ## polygon 5 52 1.59304e-05 2.25e-04 ## polygon 6 67 1.76751e-05 2.49e-04 ## polygon 7 41 9.31149e-06 1.31e-04 ## polygon 8 30 4.78946e-06 6.76e-05 ## polygon 9 36 3.77806e-06 5.33e-05 ## polygon 10 8 1.23483e-06 1.74e-05 ## enclosing rectangle: [-79.6393, -79.11547] x [43.58107, 43.85539] units ## Window area = 0.0708703 square units ## Fraction of frame area: 0.493 13.5 Activity Calculate the event-to-event distances to nearest neighbors using the function nndist(). Do this for all fast food establishments (pooled) and then for each type of establishment (i.e, “Chicken”, “Hamburger”, “Pizza”, “Sub”). Create Stienen diagrams using the distance vectors obtained in Step 1. Discuss the diagrams with a fellow student. Plot the empirical G-function for all fast food establishments (pooled) and then for each type of establishment (i.e, “Chicken”, “Hamburger”, “Pizza”, “Sub”). Is there evidence of clustering/regularity? How confident are you to make a decision whether the patterns are not random? What could you do to assess your confidence in making a decision whether the patterns are random? Explain. "],
["point-pattern-analysis-iv.html", "14 Point Pattern Analysis IV 14.1 Introduction 14.2 Learning objectives 14.3 Suggested reading 14.4 Preliminaries 14.5 Motivation 14.6 F-function 14.7 K-function", " 14 Point Pattern Analysis IV 14.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In the last practice/session your learning objectives included: Learning about clustered and dispersed (or regular) patterns. Learning the concept of nearest neighbors. Learning about distance-based methods for point pattern analysis. Learning about the G-function for the analysis of event-to-event nearest neighbor distances. Please review the previous practices if you need a refresher on these concepts. For this practice you will need the following: This R markdown notebook. A dataset called Data5.RData. Data5.RData includes five spatstat ppp-class objects. 14.2 Learning objectives In this practice, you will: Learn about the F- or empty space function. Consider the issue of patterns at multiple scales. Learn about the K-function. Apply both of these techniques using a simple example. 14.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 14.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(spatstat) Load the datasets that you will use for this practice: load(&quot;Data5.RData&quot;) 14.5 Motivation Distance-based approaches like the \\(\\hat{G}\\)-function provide a useful complement to density-based approached. They can be implemented in more ways than we have seen so far. In this practice, you will learn about two more tools for doing distance-based analysis, the \\(\\hat{F}\\)-function and the \\(\\hat{K}\\)-function. 14.6 F-function The \\(\\hat{G}\\)-function was defined as the cumulative distribution of the distances from events to their nearest neighboring event. The \\(\\hat{F}\\)-function is based on the same premise, but instead of using event-to-event distances, it uses point-to-event distances. Recall that a point is an arbitrary location on a map that needs not necessarily be the location of an event. It may well be (and typically is) empty space. For this reason, the \\(\\hat{F}\\)-function is sometimes called the empty space function: the more empty space there is in a region, the longer the distances of points to the nearest neighboring event. More formally, this function is defined as follows, with \\(d_{ik}\\) as the distance from the point at i (not necessarily an event!) to its nearest neighboring event at k: \\[ \\hat{F}(x)=\\frac{(d_{ik}\\le x, \\forall i)}{n} \\] Again, we use the hat notation to indicate that the function is estimated from the data. The theoretical distribution of this function is known (based on a null landscape generated by a spatially random Poisson process), and is as follows: \\[ F_{pois}(x) = 1 - exp(-\\lambda \\pi x^2). \\] Notice that the distribution is in fact identical to that for G. This makes sense: if the distribution of events is spatially random, the distribution of empty space in the region must be random as well! The interpretation of \\(\\hat{F}(x)\\) is the opposite of \\(\\hat{G}(x)\\): When the empirical \\(\\hat{F}(x)\\) is greater than the theoretical function, this suggests that empty spaces are closer to events than expected, compared to the null landscape, as in a dispersed pattern. On the contrary, when the empirical function is less than the theoretical function, this would suggest a clustered pattern. The \\(\\hat{F}\\)-function can be implemented in at least two ways: (1) by using a fine grid to measure the distance to events; or (2) by measuring the distance to events from randomly drawn coordinates. The implementation in spatstat is the first one, which results in a pixel-based image of empty space. Let’s illustrate this with the point pattern pp1. You can verify that this is already a ppp object: class(pp1) ## [1] &quot;ppp&quot; Begin by plotting the pattern: plot(pp1) The empty space map is obtained by means of the distmap function: empty_space_map1 &lt;- distmap(pp1) The plot of this is: plot(empty_space_map1) Similar to the Stienen diagrams that you used previously, this map shows the distance from any location on the map to the nearest event: the smaller the value, the closer the point is to an event. Compare the map above to pp2: empty_space_map2 &lt;- distmap(pp2) plot(empty_space_map2) In the second point pattern, there is more open space in the region. This is also apparent from the dot map: plot(pp2) The F-function is implemented in spatstat as Fest (for F estimated): f_pattern1 &lt;- Fest(pp1, correction = &quot;none&quot;) This function can be plotted as follows: plot(f_pattern1) Compare to the second pattern: f_pattern2 &lt;- Fest(pp2, correction = &quot;none&quot;) plot(f_pattern2) In the empirical pattern, points on a grid tend to be more distant from events than what you would expect from the null landscape: this suggests that the points are clustered. Try plotting the \\(\\hat{G}\\)-functions for these patterns, and compare. 14.7 K-function A limitation of the two techniques that you have seen so far is that they deal with a single scale: the k-th nearest neighbor (typically the first, although they can be used for the 2nd, 3rd, and so on nearest neighbor!). Their single scale nature means that these functions can easily miss patterns at different scales. Consider for instance the following point pattern: plot(pp3) The events above seem to be clustered, but then a second pattern becomes apparent at a different scale: a regular distribution of clusters. The following pattern, on the other hand, appears to be a random distribution of regularly spaced events: plot(pp4) Whereas the last point pattern is of clusters of dispersed events that are themselves regularly spaced: plot(pp5) Both \\(\\hat{G}(x)\\) or \\(\\hat{F}(x)\\) when applied to any of these patterns will detect clustering at the scale of the first nearest neighbor, but fail to detect patterns at other scales. For instance: g_pattern3 &lt;- Gest(pp3, correction = &quot;none&quot;) plot(g_pattern3) A different technique, called the \\(\\hat{K}\\)-function, is designed to detect multiple scale patterns. The intuition behind the function is as follows. Imagine that you visit all events in the point patter in sequence. Each time you visit an event you do the following. You create a circle with radius “x” centered on the event, and then you count the number of events that are within the circle. Then you increase “x” by some distance, and repeat the process. Once that you have created the last circle (which will be suitably large to capture patterns at that scale), you move and visit the next event in the series and repeat the exact same process. These counts of events at distances “x” are aggregated and normalized by the estimated intensity of the point pattern. More formally, this is (with \\(A\\) as the area of the region): \\[ \\hat{K}(x)=\\frac{1}{\\hat{\\lambda}A}\\sum_{i}\\sum_{j\\neq i}(d_{ij}\\le x). \\] As before, the theoretical values for this function are known for the case of a null landscape generated by a Poisson process: \\[ K_{pois}(x)=\\pi x^2. \\] When the empirical function is greater than the theoretical function, this would suggest that events are typically surrounded by more events at that distance than what the null landscape would have. This is interpreted as evidence of clustering. In contrast, when the empirical function is less than the theoretical one, this would suggest that events are typically surrounded by fewer events at that distance than what would be expected from a null landscape. This is interpreted as dispersion. The \\(\\hat{K}\\)-function is implemented in the package spatstat as Kest. Lets plot again pp3: plot(pp3) Next, lets calculate and plot the \\(\\hat{K}\\)-function: k_pattern3 &lt;- Kest(pp3, correction = &quot;none&quot;) plot(k_pattern3) As seen from the plot, the function is suggestive of clustering at smaller scales, but regularity at a larger scale. Try this now with the last pattern: plot(pp5) If you calculate and plot the \\(\\hat{K}\\)-function: k_pattern5 &lt;- Kest(pp5, correction = &quot;none&quot;) plot(k_pattern5) You will see that the plot correctly suggests dispersion at the very small scale, followed by clustering at an intermediate scale. There are indeed clusters of nine events surrounded by empty space, before other clusters of regular events are detected at the largest scale, following a regular pattern. Of the distance-based techniques that you have seen so far, \\(\\hat{G}(x)\\) and \\(\\hat{F}(x)\\) are often used as complements. The \\(\\hat{K}(x)\\) is useful when exploring multi-scale patterns. This concludes Practice 7, and our coverage of distance-based techniques. "],
["point-pattern-analysis-iv-activity.html", "15 Point Pattern Analysis IV-Activity 15.1 Practice questions 15.2 Learning objectives 15.3 Suggested reading 15.4 Preliminaries 15.5 Activity", " 15 Point Pattern Analysis IV-Activity 15.1 Practice questions Answer the following questions: What does the \\(\\hat{G}\\)-function measure? What does the \\(\\hat{F}\\)-function measure? How do these two functions relate to one another? Describe the intution behind the \\(\\hat{K}\\)-function. How does the \\(\\hat{K}\\)-function capture patterns at multiple scales? 15.2 Learning objectives In this activity, you will: Explore a dataset using single scale distance-based techniques. Explore the characteristics of a point pattern at multiple scales. Discuss ways to evaluate how confident you are that a pattern is random. 15.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 15.4 Preliminaries For this activity you will need the following: This R markdown notebook. A dataset called Toronto Business Points.RData. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(spatstat) Begin by loading the data that you will use in this activity: load(&quot;Toronto Business Points.RData&quot;) If you inspect your workspace, you will see that the following ppp objects are there: Fast_Food.ppp Gas_Stands.ppp Paez_Mart.ppp These are locations of fast food restaurants and gas stands in Toronto (data are from 2008). Paez Mart on the other hand is my project to cover Toronto with convenience stores. The points are the planned locations of the stores. Please note that the exact same data are provided in dataframe formand that Toronto is the city boundary. You do not need to use these dataframes for the present activity. You can check the contents of ppp objects by means of summary: summary(Fast_Food.ppp) ## Marked planar point pattern: 614 points ## Average intensity 8663.712 points per square unit ## ## Coordinates are given to 6 decimal places ## ## Multitype: ## frequency proportion intensity ## Chicken 82 0.1335505 1157.043 ## Hamburger 209 0.3403909 2949.048 ## Pizza 164 0.2671010 2314.086 ## Sub 159 0.2589577 2243.534 ## ## Window: polygonal boundary ## 10 separate polygons (no holes) ## vertices area relative.area ## polygon 1 4185 7.05052e-02 9.95e-01 ## polygon 2 600 2.82935e-04 3.99e-03 ## polygon 3 193 2.64610e-05 3.73e-04 ## polygon 4 28 2.96067e-06 4.18e-05 ## polygon 5 52 1.59304e-05 2.25e-04 ## polygon 6 67 1.76751e-05 2.49e-04 ## polygon 7 41 9.31149e-06 1.31e-04 ## polygon 8 30 4.78946e-06 6.76e-05 ## polygon 9 36 3.77806e-06 5.33e-05 ## polygon 10 8 1.23483e-06 1.74e-05 ## enclosing rectangle: [-79.6393, -79.11547] x [43.58107, 43.85539] units ## Window area = 0.0708703 square units ## Fraction of frame area: 0.493 15.5 Activity Plot the empirical F-function for all fast food establishments (pooled) and then for each type of establishment (i.e, “Chicken”, “Hamburger”, “Pizza”, “Sub”). Discuss your results with a fellow student. Is there evidence of clustering/regularity? Plot the empirical K-function for all fast food establishments (pooled) and then for each type of establishment (i.e, “Chicken”, “Hamburger”, “Pizza”, “Sub”). What can you say about patterns at multiple-scales based on point 4 above? How confident are you to make a decision whether the patterns are not random? What could you do to assess your confidence in making a decision whether the patterns are random? Explain. "],
["point-pattern-analysis-v.html", "16 Point Pattern Analysis V 16.1 Introduction 16.2 Learning objectives 16.3 Suggested reading 16.4 Preliminaries 16.5 Motivation: hypothesis testing 16.6 Null landscapes revisited 16.7 Simulation envelopes 16.8 Things to keep in mind!", " 16 Point Pattern Analysis V 16.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In the last practice/session your learning objectives included: Learning about the F- or empty space function. Considering the issue of patterns at multiple scales. Learning about the K-function. Applying these techniques using a simple example. Please review the previous practices if you need a refresher on these concepts. For this practice you will need the following: This R markdown notebook. A dataset called Data5.RData. Data5.RData includes five spatstat ppp-class objects. 16.2 Learning objectives In this practice, you will: Revisit the concept of hypothesis testing Revisit the concept of null landscapes. Learn about the use of simulation for hypothesis testing. Learn to implement simulation envelopes Consider some caveats when working with point patterns 16.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 16.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(spatstat) Load the datasets that you will use for this practice: load(&quot;Data5.RData&quot;) 16.5 Motivation: hypothesis testing In the previous sessions you learned about density- and distance-based techniques for the analysis of spatial point patterns. With the exception of the test of independence for quadrats, other techniques (including kernel density, the \\(\\hat{G}\\)- and \\(\\hat{F}\\)-functions, and the \\(\\hat{K}\\)-function), did not have an formal hypothesis framework. The question of “how confident are you in deciding whether a pattern is random” forms the basis of hypothesis testing. In other words, when making a decision whether the reject a null hypothesis, we would like to know what is the probability that we are making a mistake when doing so. Quantifying our uncertainty is a key feature of statistical analysis. In statistics, tests of hypothesis are developed following these general steps: Identify a null hypothesis of interest, and if possible alternative hypotheses as well (although the latter is not always possible). For instance, in point pattern analysis, a null hypothesis of interest is whether a pattern is random. If it is not, we would like to know in which way it is not random (i.e., is it clustered? Or on the contrary, is it regular?) Derive the expected value of the summary statistic of interest. It the case of the \\(\\hat{G}\\)-function, for instance, the expected value of the function under the null hypothesis of a spatially random Poisson process is: \\[ G_{pois}(x) = 1 - exp(-\\lambda \\pi x^2). \\] Similar expressions were presented for the \\(\\hat{F}\\)-function and \\(\\hat{K}\\)-function, but not for kernel density estimates. When the expected value of the function is known, the closer the empirical function is to its expected value, the more likely it is that the null hypothesis is true. For instance, the \\(\\hat{G}\\)-function of the pattern in pp1 is shown below. It is quite close to the theoretical function, so the pattern is probably random. The question is, how probable is this? g_pp1 &lt;- Gest(pp1, correction = &quot;none&quot;) plot(g_pp1) To make a decision whether to reject the null hypothesis (or contrariwise, fail to reject it), we need to know how close is close to the expected value. This step depends on how much variability there is of the random process around its expected value. In other words, we need to know the variance of the expected value under the null hypothesis. Unfortunately, the variance of the theoretical random processes is not known in the case of many spatial point pattern techniques (the quadrat-based test of independence is an exception.) For a long time, this meant that the techniques remained purely descriptive, and it was not possible to quantify uncertainty when trying to decide whether a pattern was random: the decision would remain purely subjective. Fortunately, with the increased use of computers in statistical analysis, the lack of theoretical expressions for the variance can be circumvented by means of simulation. Simulation has many applications in statistics, and is particularly relevant in the analysis of point patterns, allowing us to generate null landscapes with ease. 16.6 Null landscapes revisited A null landscape is a landscape produced by a random process. In previous practices you saw various different ways of generating null landscapes. A useful way of generating null landscapes for point patterns is by means of a Poisson process. The package spatstat implements this by means of the function rpoisp. This function generates a null landscape given an intensity parameter and a window. Before creating a null landscape, we can check the characteristics of the patterns in the dataset. summary(pp1) ## Planar point pattern: 81 points ## Average intensity 81 points per square unit ## ## Coordinates are given to 8 decimal places ## ## Window: rectangle = [0, 1] x [0, 1] units ## Window area = 1 square unit You can verify that the intensity in every case is 81 points per square unit, and the window is a square unit. Lets copy the window from one of the patterns in the sample dataset: W &lt;- pp1$window You can now generate a null landscape as follows: sim1 &lt;- rpoispp(lambda = 81, win = W) The value (i.e., output) of this function is a ppp object that can be analyzed in all the ways that you already know. For instance, you can plot it: plot(sim1) Importantly, you can apply any of the techniques that you have seen so far, for instance, the \\(\\hat{G}\\)-function: g_sim1 &lt;- Gest(sim1, correction = &quot;none&quot;) Lets plot the empirical functions (notice that the result of Gest is a dataframe with the values of r, the distance variable, the raw or empirical function, and the theoretical function). To plot using ggplot2 you can stack the two dataframes as follows (after adding a factor to indicate if it is the empirical function or a simulation): g_all &lt;- transmute(g_pp1, G = raw, x = r, Type = factor(&quot;Empirical&quot;)) g_all &lt;- rbind(g_all, transmute(g_sim1, G = raw, x = r, Type = factor(&quot;Simulation&quot;))) Create a plot: ggplot(data = g_all, aes(x= x, y = G, color = Type)) + geom_line() After seeing the plot above, we notice that the empirical function is very, very similar to the simulated null landscape. But is this purely a coincidence? After all, when we simulate a null landscape, there is the possibility, however improbable, that it will replicate some meaningful process purely by chance. To be sure, we can simulate and analyze a second null landscape: sim2 &lt;- rpoispp(lambda = 81, win = W) g_sim2 &lt;- Gest(sim2, correction = &quot;none&quot;) g_all &lt;- rbind(g_all, transmute(g_sim2, G = raw, x = r, Type = factor(&quot;Simulation&quot;))) Plot again: ggplot(data = g_all, aes(x= x, y = G, color = Type)) + geom_line() The empirical function continues to look very similar to the simulated null landscapes. We could simulate more null landscapes and increase our confidence that the empirical function indeed is similar to a null landscape (notice the use of a for loop to repeat the same instructions multiple times): for(i in 3:99){ g_sim &lt;- Gest(rpoispp(lambda = 81, win = W), correction = &quot;none&quot;) g_all &lt;- rbind(g_all, transmute(g_sim, G = raw, x = r, Type = factor(&quot;Simulation&quot;))) } With this you have generated 99 distinct null landscapes. Try plotting the empirical function with the functions of all your simulated landscapes: ggplot(data = g_all, aes(x= x, y = G, color = Type)) + geom_line() You can see in the plot above that the empirical function is actually not visible! It falls somewhere within the limits of the functions for the simulated patterns. The interpretation of this is as follows: out of 100 patterns (the empirical pattern and 99 null landscapes), the empirical pattern is not noticeably different from the random ones. How confident would you be rejecting the null hypothesis, i.e., deciding that the empirical pattern is not random? Let’s compare now the second pattern pp2 to the simulated null landscapes: g_pp2 &lt;- Gest(pp2, correction = &quot;none&quot;) g_pp2 &lt;- transmute(g_pp2, G = raw, x = r, Type = factor(&quot;Empirical&quot;)) g_all[1:513,] &lt;- g_pp2 ggplot(data = g_all, aes(x= x, y = G, color = Type)) + geom_line() Now the empirical function is quite distinct from the null landscapes! How confident would you be rejecting the null hypothesis now? 16.7 Simulation envelopes Simulation, as seen above, can be quite powerful for hypothesis testing in situations where the theoretical variance is not know. Essentially, the area covered by the G-functions of the simulated landscapes above are an estimate of the variance. The set of functions estimated on the null landscapes are called simulation envelopes. Since we lack a theoretical expression for the variance, we cannot obtain “p-values” to inform our decision to reject the null hypothesis. The simulation, however, provides a pseudo-p-value. If you generate 99 null landscapes, and the empirical pattern is still different, the probability that you are mistaken by rejecting the null hypothesis is at most 1% (since the next simulated landscape could expand the envelopes in such a way that it completely contains the empirical function). As you saw above, using simulation for hypothesis testing is, in general terms, a relatively straightforward process (assuming that the null process is properly defined, etc.) The package spatstat includes a function, called envelope, that can be used to generate simulation envelopes for several statistics used in point pattern analysis. For instance, for the G-function, with 99 simulated landscapes: env_pp1 &lt;- envelope(pp1, Gest, nsim = 99, funargs = list(correction = &quot;none&quot;)) ## Generating 99 simulations of CSR ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, ## 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, ## 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. ## ## Done. The envelopes can be plotted: plot(env_pp1) It is easy to see that in this case the empirical function falls within the simulation envelopes, and thus it is very unlikely to be different from the null landscapes. Also, the F-function: env_pp2 &lt;- envelope(pp2, Fest, nsim = 99, funargs = list(correction = &quot;none&quot;)) ## Generating 99 simulations of CSR ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, ## 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, ## 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. ## ## Done. plot(env_pp2) Now the empirical function lies outside of the simulation envelopes, which makes it very unlikely that it is similar to the null landscapes. And finally, the K-function: env_pp3 &lt;- envelope(pp3, Kest, nsim = 99, funargs = list(correction = &quot;none&quot;)) ## Generating 99 simulations of CSR ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, ## 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, ## 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. ## ## Done. plot(env_pp3) Again, the empirical function lies mostly outside of the simulation envelopes, meaning that it is very improbable that it represents a random process. Simulation envelopes are a powerful way to test the hypothesis of null landscapes in the case of spatial point patterns. 16.8 Things to keep in mind! Before concluding the topic of point pattern analysis, here are a few caveats to keep in mind. 16.8.1 Definition of a region When defining the region (or window) for the analysis, care must be taken that it is reasonable from the perspective of the process under analysis. Defining the region in an inappropriate way can easily lead to misleading results. Consider for instance the first pattern in the dataset. This pattern was defined for a unit-square window. Lets apply the K-function to it: k_env_pp1 &lt;- envelope(pp1, Kest, nsim = 99, funargs = list(correction = &quot;none&quot;)) ## Generating 99 simulations of CSR ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, ## 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, ## 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. ## ## Done. plot(k_env_pp1) Based on this we would most likely conclude that the pattern is random. Lets now replace the unit-square window by a much larger window: W2 &lt;- owin(x = c(-2,4), y = c(-2, 4)) pp1_reg2 &lt;- as.ppp(as.data.frame(pp1), W = W2) plot(pp1_reg2) In the context of the larger window, the point pattern now looks clustered! See how the definition of the window would change your conclusions regarding the pattern: k_env_pp1_reg2 &lt;- envelope(pp1_reg2, Kest, nsim = 99, funargs = list(correction = &quot;none&quot;)) ## Generating 99 simulations of CSR ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, ## 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, ## 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. ## ## Done. plot(k_env_pp1_reg2) 16.8.2 Edge effects As discussed above, definition of the window (region) is critical. If at all possible, the region should be selected in such a way that it is consistent with the underlying process. This is not always possible, either because the underlying process is not known, or because of limitations in data collection capabilities. When this is the case, it is necessary to define a boundary that does not correspond necessarily with the extent of the process of interest. For example, analysis of business locations in Toronto may be limited to the city limits. This does not mean that establishments do not exist beyond those boundaries. When the extent of the process exceeds the window used in the analysis, the point pattern is observed only partially, and it is possible that the information of the location of events beyond the boundary may introduce some bias. Consider the situation illustrated in Figure 1. In the figure, the region is the rectangular window. Events are observed only inside the window, but events still exist beyond the edges of the window. It is straightforward to see how the empty space (F-) function would be biased, since locations near the edge would appear the be more distant from an event than they actually are. Several corrections are available in spatstat to deal with the possibility of edge effects. So far, we have used the argument correction = &quot;none&quot; when applying the functions. The following alternative corrections are implemented: “none”, “rs”, “km”, “cs” and “best”. Alternatively correction = &quot;all&quot; selects all options. These corrections are variations of weighting schemes. In other words, the statistic is weighted to give an unbiased estimator. See: plot(Gest(pp2, correction = &quot;all&quot;)) The different corrections are plotted. It can be seen in this cases that the corrections are relatively small; however, this is not always the case. 16.8.3 Sampled point patterns Whereas edge effects can introduce bias by censoring the observations outside of the window/region, another issue emerges when not all events are observed inside the window. We have assumed so far that any point pattern under analysis consists of a census of events, or in other words, that all relevant events have been recorded. A sampled point pattern, on the other hand, is a pattern where not all events have been recorded (see Figure 2). The bias introduced by sampled point patterns is quite serious, because the findings depend heavily of the observations that were recorded as well as those that were not recorded! Clustered events could easily give the impression of a dispersed pattern, depending on what was observed. Imagine for instance that the events are nests of birds. If the birds tend to nest in the thickest parts of the forest that observers cannot easily access, the “observed” pattern will depend crucially on the trails and other routes of access that the researcher can use. There are no good solutions to bias introduced by sampled point patterns, and it is not recommended to use the techniques discussed here with sampled point patterns. This concludes Practice 8, and the topic of spatial point patterns. "],
["point-pattern-analysis-v-activity.html", "17 Point Pattern Analysis V-Activity 17.1 Practice questions 17.2 Learning objectives 17.3 Suggested reading 17.4 Preliminaries 17.5 Activity", " 17 Point Pattern Analysis V-Activity 17.1 Practice questions Answer the following questions: Describe the process to use simulation for hypothesis testing Why is the selection of an appropriate region critical for the analysis of point patterns? Discuss the issues associated with the edges of a region. What is a sampled point pattern? 17.2 Learning objectives In this activity, you will: Explore a dataset using single scale distance-based techniques. Explore the characteristics of a point pattern at multiple scales. Discuss ways to evaluate how confident you are that a pattern is random. 17.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 17.4 Preliminaries For this activity you will need the following: This R markdown notebook. A dataset of your choice. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(spatstat) Load a dataset of your choice. It could be one of the datasets that we have used before (Toronto Business Points, Bear GPS Locations), or one of the datasets included with the package spatstat. To see what datasets are available through the package, do the following: vcdExtra::datasets(&quot;spatstat.data&quot;) ## Item class dim ## 1 Kovesi list 41x13 ## 2 amacrine ppp 6 ## 3 anemones ppp 6 ## 4 ants ppp 6 ## 5 ants.extra list 7 ## 6 austates list 4 ## 7 bdspots list 3 ## 8 bei ppp 5 ## 9 bei.extra list 2 ## 10 betacells ppp 6 ## 11 bramblecanes ppp 6 ## 12 bronzefilter ppp 6 ## 13 cells ppp 5 ## 14 cetaceans list 9x4 ## 15 cetaceans.extra list 1 ## 16 chicago ppx 3 ## 17 chorley ppp 6 ## 18 chorley.extra list 2 ## 19 clmfires ppp 6 ## 20 clmfires.extra list 2 ## 21 copper list 7 ## 22 demohyper list 3x3 ## 23 demopat ppp 6 ## 24 dendrite ppx 3 ## 25 finpines ppp 6 ## 26 flu list 41x4 ## 27 ganglia ppp 6 ## 28 gordon ppp 5 ## 29 gorillas ppp 6 ## 30 gorillas.extra list 7 ## 31 hamster ppp 6 ## 32 heather list 3 ## 33 humberside ppp 6 ## 34 humberside.convex ppp 6 ## 35 hyytiala ppp 6 ## 36 japanesepines ppp 5 ## 37 lansing ppp 6 ## 38 letterR owin 5 ## 39 longleaf ppp 6 ## 40 mucosa ppp 6 ## 41 mucosa.subwin owin 4 ## 42 murchison list 3 ## 43 nbfires ppp 6 ## 44 nbfires.extra list 2 ## 45 nbw.rect owin 4 ## 46 nbw.seg list 5 ## 47 nztrees ppp 5 ## 48 osteo list 40x5 ## 49 paracou ppp 6 ## 50 ponderosa ppp 5 ## 51 ponderosa.extra list 2 ## 52 pyramidal list 31x2 ## 53 redwood ppp 5 ## 54 redwood3 ppp 5 ## 55 redwoodfull ppp 5 ## 56 redwoodfull.extra list 5 ## 57 residualspaper list 7 ## 58 shapley ppp 6 ## 59 shapley.extra list 3 ## 60 simba list 10x2 ## 61 simdat ppp 5 ## 62 simplenet list 10 ## 63 spiders ppx 3 ## 64 sporophores ppp 6 ## 65 spruces ppp 6 ## 66 swedishpines ppp 5 ## 67 urkiola ppp 6 ## 68 vesicles ppp 5 ## 69 vesicles.extra list 4 ## 70 waka ppp 6 ## 71 waterstriders list 3 ## Title ## 1 Colour Sequences with Uniform Perceptual Contrast ## 2 Hughes&#39; Amacrine Cell Data ## 3 Beadlet Anemones Data ## 4 Harkness-Isham ants&#39; nests data ## 5 Harkness-Isham ants&#39; nests data ## 6 Australian States and Mainland Territories ## 7 Breakdown Spots in Microelectronic Materials ## 8 Tropical rain forest trees ## 9 Tropical rain forest trees ## 10 Beta Ganglion Cells in Cat Retina ## 11 Hutchings&#39; Bramble Canes data ## 12 Bronze gradient filter data ## 13 Biological Cells Point Pattern ## 14 Point patterns of whale and dolphin sightings. ## 15 Point patterns of whale and dolphin sightings. ## 16 Chicago Crime Data ## 17 Chorley-Ribble Cancer Data ## 18 Chorley-Ribble Cancer Data ## 19 Castilla-La Mancha Forest Fires ## 20 Castilla-La Mancha Forest Fires ## 21 Berman-Huntington points and lines data ## 22 Demonstration Example of Hyperframe of Spatial Data ## 23 Artificial Data Point Pattern ## 24 Dendritic Spines Data ## 25 Pine saplings in Finland. ## 26 Influenza Virus Proteins ## 27 Beta Ganglion Cells in Cat Retina, Old Version ## 28 People in Gordon Square ## 29 Gorilla Nesting Sites ## 30 Gorilla Nesting Sites ## 31 Aherne&#39;s hamster tumour data ## 32 Diggle&#39;s Heather Data ## 33 Humberside Data on Childhood Leukaemia and Lymphoma ## 34 Humberside Data on Childhood Leukaemia and Lymphoma ## 35 Scots pines and other trees at Hyytiala ## 36 Japanese Pines Point Pattern ## 37 Lansing Woods Point Pattern ## 38 Window in Shape of Letter R ## 39 Longleaf Pines Point Pattern ## 40 Cells in Gastric Mucosa ## 41 Cells in Gastric Mucosa ## 42 Murchison gold deposits ## 43 Point Patterns of New Brunswick Forest Fires ## 44 Point Patterns of New Brunswick Forest Fires ## 45 Point Patterns of New Brunswick Forest Fires ## 46 Point Patterns of New Brunswick Forest Fires ## 47 New Zealand Trees Point Pattern ## 48 Osteocyte Lacunae Data: Replicated Three-Dimensional Point Patterns ## 49 Kimboto trees at Paracou, French Guiana ## 50 Ponderosa Pine Tree Point Pattern ## 51 Ponderosa Pine Tree Point Pattern ## 52 Pyramidal Neurons in Cingulate Cortex ## 53 California Redwoods Point Pattern (Ripley&#39;s Subset) ## 54 California Redwoods Point Pattern (Ripley&#39;s Subset) ## 55 California Redwoods Point Pattern (Entire Dataset) ## 56 California Redwoods Point Pattern (Entire Dataset) ## 57 Data and Code From JRSS Discussion Paper on Residuals ## 58 Galaxies in the Shapley Supercluster ## 59 Galaxies in the Shapley Supercluster ## 60 Simulated data from a two-group experiment with replication within each group. ## 61 Simulated Point Pattern ## 62 Simple Example of Linear Network ## 63 Spider Webs on Mortar Lines of a Brick Wall ## 64 Sporophores Data ## 65 Spruces Point Pattern ## 66 Swedish Pines Point Pattern ## 67 Urkiola Woods Point Pattern ## 68 Vesicles Data ## 69 Vesicles Data ## 70 Trees in Waka national park ## 71 Waterstriders data. Three independent replications of a point pattern formed by insects. Load a dataset of your choice. You can do this by using the load() function if the dataset is in your drive (e.g., the GPS coordinates of the bear). On the other hand, if the dataset is included with the spatstat package you can do the following, for example to load the gorillas dataset: gorillas.ppp &lt;- gorillas As usual, you can check the object by means of the summary function: summary(gorillas.ppp) ## Marked planar point pattern: 647 points ## Average intensity 3.255566e-05 points per square metre ## ## *Pattern contains duplicated points* ## ## Coordinates are given to 2 decimal places ## i.e. rounded to the nearest multiple of 0.01 metres ## ## Mark variables: group, season, date ## Summary: ## group season date ## major:350 dry :275 Min. :2006-01-06 ## minor:297 rainy:372 1st Qu.:2007-03-15 ## Median :2008-02-05 ## Mean :2007-12-14 ## 3rd Qu.:2008-09-23 ## Max. :2009-05-31 ## ## Window: polygonal boundary ## single connected closed polygon with 21 vertices ## enclosing rectangle: [580457.9, 585934] x [674172.8, 678739.2] metres ## Window area = 19873700 square metres ## Unit of length: 1 metre ## Fraction of frame area: 0.795 17.5 Activity Partner with a fellow student to analyze the chosen dataset. Discuss whether the pattern is random, and how confident you are in your decision. The analysis of the pattern is meant to provide insights about the underlying process. Describe a hypothetical process that is consistent with your observations about the pattern. How would you go about further investigating the process? Discuss the limitations of the analysis, for instance, choice of modeling parameters (size of region, kernel bandwidths, edge effects, etc.) "],
["area-data-i.html", "18 Area Data I 18.1 Introduction 18.2 Learning objectives 18.3 Suggested reading 18.4 Preliminaries 18.5 Area data 18.6 Processes and area data 18.7 Visualizing area data: choropleth maps 18.8 Visualizing area data: cartogram", " 18 Area Data I 18.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In last few practices/sessions, you learned about spatial point patterns. The next few sessions will concentrate on area data. For this practice you will need the following: This R markdown notebook. A shape file called “Hamilton CMA CT” This dataset includes the spatial information for the census tracts in the Hamilton Census Metropolitan Area (as polygons), and a host of demographic variables from the census of Canada, including population and languages. 18.2 Learning objectives In this practice, you will learn: A formal definition of area data. Processes and area data. Visualizing area data: Choropleth maps. Visualizing area data: Cartograms. 18.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 18.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(rgdal) ## Warning: package &#39;rgdal&#39; was built under R version 3.4.4 ## Loading required package: sp ## rgdal: version: 1.2-18, (SVN revision 718) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 2.2.3, released 2017/11/20 ## Path to GDAL shared files: C:/Users/Antonio/Documents/R/win-library/3.4/rgdal/gdal ## GDAL binary built with GEOS: TRUE ## Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493] ## Path to PROJ.4 shared files: C:/Users/Antonio/Documents/R/win-library/3.4/rgdal/proj ## Linking to sp version: 1.2-7 library(broom) ## Warning: package &#39;broom&#39; was built under R version 3.4.4 library(plotly) ## Warning: package &#39;plotly&#39; was built under R version 3.4.4 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggmap&#39;: ## ## wind ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(cartogram) library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine Read the data that you will use for this practice. This is an Esri shape file that will be saved as an object of class SpatialPolygonDataFrame. The function used to read Esri shape files is rgdal::readOGR. Setting integer64 to “allow.loss” keeps the data as integers as opposed to changing to factors or strings: Hamilton_CT &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA CT&quot;, integer64 = &quot;allow.loss&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 To use the plotting functions of ggplot2, the SpatialPolygonDataFrame needs to be “tidied” by means of the tidy function of the broom package: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- dplyr::rename(Hamilton_CT.t, TRACT = id) Tidying the spatial dataframe strips it from the non-spatial information, but we can add all the data by means of the left_join function: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Now the tidy dataframe Hamilton_DA.t contains the spatial information and the data. You can quickly verify the contents of the dataframe by means of summary: summary(Hamilton_CT.t) ## long lat order hole ## Min. :-80.25 Min. :43.05 Min. : 1 Mode :logical ## 1st Qu.:-79.93 1st Qu.:43.21 1st Qu.: 7304 FALSE:29212 ## Median :-79.86 Median :43.24 Median :14606 ## Mean :-79.86 Mean :43.25 Mean :14606 ## 3rd Qu.:-79.80 3rd Qu.:43.28 3rd Qu.:21909 ## Max. :-79.51 Max. :43.48 Max. :29212 ## ## piece group TRACT ID ## 1:29212 5370124.00.1: 822 Length:29212 Min. : 919807 ## 5370121.00.1: 661 Class :character 1st Qu.: 920233 ## 5370142.01.1: 642 Mode :character Median : 937830 ## 5370100.00.1: 615 Mean : 946568 ## 5370101.02.1: 602 3rd Qu.: 959451 ## 5370142.02.1: 595 Max. :1115750 ## (Other) :25275 ## AREA COLORING CMA PROVINCE NAME ## Min. : 0.3154 Min. :0.000 537:29212 00:14311 0124.00: 822 ## 1st Qu.: 1.2217 1st Qu.:0.000 01: 9506 0121.00: 661 ## Median : 2.6824 Median :2.000 02: 4394 0142.01: 642 ## Mean : 18.6962 Mean :1.493 03: 1001 0100.00: 615 ## 3rd Qu.: 11.0990 3rd Qu.:2.000 0101.02: 602 ## Max. :138.4466 Max. :4.000 0142.02: 595 ## (Other):25275 ## ABBREV POPULATION PRIVATE_DW OCCUPIED_D LAND_AREA ## ON:29212 Min. : 5 Min. : 0 Min. : 0 Min. : 0.32 ## 1st Qu.: 2756 1st Qu.:1191 1st Qu.:1170 1st Qu.: 1.22 ## Median : 3901 Median :1526 Median :1436 Median : 2.62 ## Mean : 4218 Mean :1648 Mean :1589 Mean : 18.43 ## 3rd Qu.: 5293 3rd Qu.:2096 3rd Qu.:1987 3rd Qu.: 11.07 ## Max. :11675 Max. :4076 Max. :4016 Max. :137.49 ## ## POP_DENSIT ALL_AGES AGE_4 AGE_5_TO_9 ## Min. : 2.591 Min. : 0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 254.658 1st Qu.: 2755 1st Qu.: 115.0 1st Qu.: 125.0 ## Median : 1511.957 Median : 3905 Median : 175.0 Median : 195.0 ## Mean : 1890.627 Mean : 4218 Mean : 230.5 Mean : 237.4 ## 3rd Qu.: 2807.857 3rd Qu.: 5295 3rd Qu.: 275.0 3rd Qu.: 290.0 ## Max. :14234.286 Max. :11675 Max. :1180.0 Max. :1060.0 ## ## AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 ## Min. : 0.0 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.:140.0 1st Qu.:170.0 1st Qu.: 30.00 1st Qu.: 30.00 ## Median :220.0 Median :260.0 Median : 45.00 Median : 55.00 ## Mean :252.1 Mean :287.6 Mean : 54.38 Mean : 57.07 ## 3rd Qu.:320.0 3rd Qu.:355.0 3rd Qu.: 70.00 3rd Qu.: 75.00 ## Max. :795.0 Max. :800.0 Max. :175.00 Max. :180.00 ## ## AGE_17 AGE_18 AGE_19 AGE_20_TO_ ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.0 ## 1st Qu.: 35.00 1st Qu.: 35.00 1st Qu.: 35.00 1st Qu.:180.0 ## Median : 50.00 Median : 55.00 Median : 55.00 Median :245.0 ## Mean : 58.01 Mean : 58.57 Mean : 59.94 Mean :271.5 ## 3rd Qu.: 75.00 3rd Qu.: 75.00 3rd Qu.: 80.00 3rd Qu.:350.0 ## Max. :200.00 Max. :255.00 Max. :270.00 Max. :835.0 ## ## AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.:135.0 1st Qu.: 125.0 1st Qu.: 145.0 1st Qu.: 180.0 ## Median :205.0 Median : 185.0 Median : 205.0 Median : 245.0 ## Mean :240.1 Mean : 245.7 Mean : 259.9 Mean : 290.2 ## 3rd Qu.:310.0 3rd Qu.: 280.0 3rd Qu.: 305.0 3rd Qu.: 370.0 ## Max. :915.0 Max. :1320.0 Max. :1200.0 Max. :1105.0 ## ## AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ ## Min. : 0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.:215 1st Qu.:220.0 1st Qu.:190.0 1st Qu.:160.0 ## Median :300 Median :310.0 Median :300.0 Median :260.0 ## Mean :338 Mean :333.1 Mean :291.8 Mean :263.5 ## 3rd Qu.:410 3rd Qu.:415.0 3rd Qu.:365.0 3rd Qu.:350.0 ## Max. :880 Max. :740.0 Max. :625.0 Max. :540.0 ## ## AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.:130.0 1st Qu.: 95.0 1st Qu.: 75.0 1st Qu.: 50.0 ## Median :185.0 Median :140.0 Median :105.0 Median : 85.0 ## Mean :200.7 Mean :154.3 Mean :126.3 Mean :100.8 ## 3rd Qu.:255.0 3rd Qu.:200.0 3rd Qu.:165.0 3rd Qu.:130.0 ## Max. :625.0 Max. :540.0 Max. :575.0 Max. :420.0 ## ## AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 ## Min. : 0.00 Min. : 0.00 Min. : 0 Min. : 0.0 ## 1st Qu.: 35.00 1st Qu.:38.00 1st Qu.:1345 1st Qu.: 60.0 ## Median : 75.00 Median :43.00 Median :1925 Median : 90.0 ## Mean : 95.71 Mean :42.06 Mean :2058 Mean :119.3 ## 3rd Qu.:125.00 3rd Qu.:46.00 3rd Qu.:2505 3rd Qu.:145.0 ## Max. :400.00 Max. :57.00 Max. :5685 Max. :605.0 ## ## MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.00 ## 1st Qu.: 65.0 1st Qu.: 75.0 1st Qu.: 85.0 1st Qu.:15.00 ## Median :100.0 Median :120.0 Median :130.0 Median :25.00 ## Mean :122.8 Mean :129.4 Mean :147.7 Mean :27.56 ## 3rd Qu.:150.0 3rd Qu.:155.0 3rd Qu.:185.0 3rd Qu.:40.00 ## Max. :540.0 Max. :410.0 Max. :505.0 Max. :95.00 ## ## MALE_16 MALE_17 MALE_18 MALE_19 ## Min. : 0.00 Min. : 0.0 Min. : 0.0 Min. : 0.00 ## 1st Qu.:15.00 1st Qu.: 15.0 1st Qu.: 15.0 1st Qu.: 20.00 ## Median :25.00 Median : 25.0 Median : 30.0 Median : 30.00 ## Mean :29.37 Mean : 29.2 Mean : 30.7 Mean : 31.35 ## 3rd Qu.:35.00 3rd Qu.: 40.0 3rd Qu.: 40.0 3rd Qu.: 40.00 ## Max. :90.00 Max. :150.0 Max. :165.0 Max. :170.00 ## ## MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0 ## 1st Qu.: 90.0 1st Qu.: 70.0 1st Qu.: 60.0 1st Qu.: 70 ## Median :125.0 Median :105.0 Median : 85.0 Median :100 ## Mean :139.6 Mean :118.1 Mean :117.2 Mean :125 ## 3rd Qu.:180.0 3rd Qu.:155.0 3rd Qu.:135.0 3rd Qu.:145 ## Max. :500.0 Max. :385.0 Max. :635.0 Max. :570 ## ## MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0 ## 1st Qu.: 90.0 1st Qu.:105.0 1st Qu.:110.0 1st Qu.: 95 ## Median :115.0 Median :150.0 Median :145.0 Median :140 ## Mean :140.6 Mean :163.9 Mean :164.7 Mean :140 ## 3rd Qu.:170.0 3rd Qu.:200.0 3rd Qu.:200.0 3rd Qu.:175 ## Max. :570.0 Max. :430.0 Max. :390.0 Max. :320 ## ## MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO ## Min. : 0.0 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 75.0 1st Qu.: 60.00 1st Qu.: 45.00 1st Qu.: 35.00 ## Median :125.0 Median : 90.00 Median : 65.00 Median : 50.00 ## Mean :127.6 Mean : 96.72 Mean : 73.04 Mean : 57.74 ## 3rd Qu.:170.0 3rd Qu.:120.00 3rd Qu.:100.00 3rd Qu.: 80.00 ## Max. :270.0 Max. :265.00 Max. :240.00 Max. :275.00 ## ## MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL ## Min. : 0.00 Min. : 0.0 Min. : 0.00 Min. : 0 ## 1st Qu.: 20.00 1st Qu.: 15.0 1st Qu.:37.00 1st Qu.:1405 ## Median : 35.00 Median : 25.0 Median :42.00 Median :1920 ## Mean : 42.42 Mean : 31.9 Mean :40.91 Mean :2161 ## 3rd Qu.: 50.00 3rd Qu.: 35.0 3rd Qu.:45.00 3rd Qu.:2795 ## Max. :175.00 Max. :140.0 Max. :52.00 Max. :5990 ## ## FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 50.0 1st Qu.: 60.0 1st Qu.: 70.0 1st Qu.: 80.0 ## Median : 85.0 Median : 90.0 Median :105.0 Median :125.0 ## Mean :111.1 Mean :114.7 Mean :123.3 Mean :139.1 ## 3rd Qu.:135.0 3rd Qu.:135.0 3rd Qu.:160.0 3rd Qu.:170.0 ## Max. :580.0 Max. :525.0 Max. :385.0 Max. :640.0 ## ## FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 ## Min. : 0.00 Min. : 0.00 Min. : 0.0 Min. : 0.00 ## 1st Qu.:15.00 1st Qu.:15.00 1st Qu.: 15.0 1st Qu.: 15.00 ## Median :25.00 Median :25.00 Median : 25.0 Median : 25.00 ## Mean :26.67 Mean :28.11 Mean : 28.5 Mean : 27.79 ## 3rd Qu.:35.00 3rd Qu.:35.00 3rd Qu.: 35.0 3rd Qu.: 35.00 ## Max. :85.00 Max. :95.00 Max. :185.0 Max. :225.00 ## ## FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ ## Min. : 0.00 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 15.00 1st Qu.: 80.0 1st Qu.: 65.0 1st Qu.: 65.0 ## Median : 25.00 Median :120.0 Median :100.0 Median : 95.0 ## Mean : 28.53 Mean :131.9 Mean :122.1 Mean :128.3 ## 3rd Qu.: 40.00 3rd Qu.:170.0 3rd Qu.:155.0 3rd Qu.:150.0 ## Max. :130.00 Max. :335.0 Max. :530.0 Max. :700.0 ## ## FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0 ## 1st Qu.: 75.0 1st Qu.: 90.0 1st Qu.:105.0 1st Qu.:110 ## Median :105.0 Median :130.0 Median :155.0 Median :160 ## Mean :135.2 Mean :149.5 Mean :173.3 Mean :168 ## 3rd Qu.:160.0 3rd Qu.:190.0 3rd Qu.:210.0 3rd Qu.:220 ## Max. :660.0 Max. :530.0 Max. :445.0 Max. :355 ## ## FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.00 ## 1st Qu.: 95.0 1st Qu.: 85.0 1st Qu.: 65.0 1st Qu.: 50.00 ## Median :155.0 Median :135.0 Median : 90.0 Median : 70.00 ## Mean :151.4 Mean :135.9 Mean :103.6 Mean : 80.27 ## 3rd Qu.:195.0 3rd Qu.:170.0 3rd Qu.:140.0 3rd Qu.:105.00 ## Max. :325.0 Max. :295.0 Max. :360.0 Max. :300.00 ## ## FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 40.00 1st Qu.: 25.00 1st Qu.: 20.00 1st Qu.:39.00 ## Median : 60.00 Median : 45.00 Median : 45.00 Median :43.00 ## Mean : 68.99 Mean : 58.84 Mean : 63.65 Mean :43.14 ## 3rd Qu.: 85.00 3rd Qu.: 80.00 3rd Qu.: 85.00 3rd Qu.:47.00 ## Max. :305.00 Max. :240.00 Max. :260.00 Max. :62.00 ## ## MARRIED_AG MARRIED_OR MARRIED COMMON_LAW ## Min. : 0 Min. : 0 Min. : 0 Min. : 0.0 ## 1st Qu.:2355 1st Qu.:1250 1st Qu.:1065 1st Qu.:145.0 ## Median :3230 Median :1995 Median :1715 Median :210.0 ## Mean :3497 Mean :2101 Mean :1863 Mean :237.6 ## 3rd Qu.:4515 3rd Qu.:2490 3rd Qu.:2250 3rd Qu.:300.0 ## Max. :8990 Max. :6500 Max. :5765 Max. :730.0 ## ## UNMARRIED SINGLE SEPARATED DIVORCED ## Min. : 0 Min. : 0.0 Min. : 0.00 Min. : 0.0 ## 1st Qu.:1035 1st Qu.: 660.0 1st Qu.: 55.00 1st Qu.:110.0 ## Median :1335 Median : 840.0 Median : 85.00 Median :175.0 ## Mean :1396 Mean : 903.4 Mean : 97.14 Mean :183.8 ## 3rd Qu.:1840 3rd Qu.:1165.0 3rd Qu.:125.00 3rd Qu.:230.0 ## Max. :2850 Max. :1885.0 Max. :300.00 Max. :515.0 ## ## WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M ## Min. : 0.0 Min. : 0 Min. : 0 Min. : 0.0 ## 1st Qu.:120.0 1st Qu.:1145 1st Qu.: 620 1st Qu.: 530.0 ## Median :180.0 Median :1595 Median :1000 Median : 860.0 ## Mean :211.9 Mean :1686 Mean :1048 Mean : 930.7 ## 3rd Qu.:270.0 3rd Qu.:2160 3rd Qu.:1235 3rd Qu.:1115.0 ## Max. :645.0 Max. :4360 Max. :3235 Max. :2880.0 ## ## COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.00 ## 1st Qu.: 70.0 1st Qu.: 475.0 1st Qu.: 355.0 1st Qu.: 25.00 ## Median :105.0 Median : 585.0 Median : 450.0 Median : 35.00 ## Mean :118.2 Mean : 637.6 Mean : 483.6 Mean : 41.16 ## 3rd Qu.:150.0 3rd Qu.: 830.0 3rd Qu.: 610.0 3rd Qu.: 55.00 ## Max. :360.0 Max. :1360.0 Max. :1175.0 Max. :125.00 ## ## DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 ## Min. : 0.00 Min. : 0.00 Min. : 0 Min. : 0 ## 1st Qu.: 45.00 1st Qu.: 25.00 1st Qu.:1225 1st Qu.: 625 ## Median : 60.00 Median : 40.00 Median :1675 Median :1000 ## Mean : 70.22 Mean : 42.61 Mean :1811 Mean :1052 ## 3rd Qu.: 90.00 3rd Qu.: 55.00 3rd Qu.:2345 3rd Qu.:1255 ## Max. :220.00 Max. :130.00 Max. :4635 Max. :3260 ## ## MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 535.0 1st Qu.: 70.0 1st Qu.: 525.0 1st Qu.: 285.0 ## Median : 855.0 Median :105.0 Median : 680.0 Median : 385.0 ## Mean : 933.4 Mean :118.9 Mean : 759.1 Mean : 420.1 ## 3rd Qu.:1135.0 3rd Qu.:150.0 3rd Qu.:1010.0 3rd Qu.: 535.0 ## Max. :2895.0 Max. :370.0 Max. :1520.0 Max. :1100.0 ## ## SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I ## Min. : 0.00 Min. : 0.0 Min. : 0.0 Min. : 0 ## 1st Qu.: 25.00 1st Qu.: 60.0 1st Qu.: 95.0 1st Qu.: 765 ## Median : 55.00 Median :110.0 Median :140.0 Median :1105 ## Mean : 55.76 Mean :113.4 Mean :169.1 Mean :1213 ## 3rd Qu.: 75.00 3rd Qu.:145.0 3rd Qu.:220.0 3rd Qu.:1530 ## Max. :170.00 Max. :320.0 Max. :520.0 Max. :3525 ## ## FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 ## Min. : 0.0 Min. : 0 Min. : 0 Min. : 0.0 ## 1st Qu.: 385.0 1st Qu.:165 1st Qu.:135 1st Qu.: 60.0 ## Median : 515.0 Median :235 Median :225 Median : 90.0 ## Mean : 567.3 Mean :260 Mean :269 Mean :116.9 ## 3rd Qu.: 700.0 3rd Qu.:335 3rd Qu.:340 3rd Qu.:150.0 ## Max. :1485.0 Max. :775 Max. :890 Max. :390.0 ## ## COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 ## Min. : 0 Min. : 0.0 Min. : 0.0 Min. : 0 ## 1st Qu.: 605 1st Qu.: 530.0 1st Qu.: 225.0 1st Qu.: 265 ## Median : 985 Median : 840.0 Median : 360.0 Median : 430 ## Mean :1034 Mean : 915.1 Mean : 390.1 Mean : 525 ## 3rd Qu.:1230 3rd Qu.:1095.0 3rd Qu.: 475.0 3rd Qu.: 660 ## Max. :3225 Max. :2860.0 Max. :1140.0 Max. :1785 ## ## COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.:105.0 1st Qu.:115.0 1st Qu.: 50.0 1st Qu.: 70.0 ## Median :170.0 Median :185.0 Median : 80.0 Median :105.0 ## Mean :183.9 Mean :236.8 Mean :104.3 Mean :118.3 ## 3rd Qu.:220.0 3rd Qu.:295.0 3rd Qu.:135.0 3rd Qu.:150.0 ## Max. :645.0 Max. :835.0 Max. :365.0 Max. :365.0 ## ## COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 40.00 1st Qu.: 25.00 1st Qu.:10.00 1st Qu.:10.00 ## Median : 60.00 Median : 45.00 Median :20.00 Median :15.00 ## Mean : 71.22 Mean : 46.83 Mean :22.93 Mean :16.11 ## 3rd Qu.: 90.00 3rd Qu.: 55.00 3rd Qu.:30.00 3rd Qu.:20.00 ## Max. :265.00 Max. :135.00 Max. :65.00 Max. :55.00 ## ## COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 ## Min. : 0.000 Min. : 0.0 Min. : 0 Min. : 0.00 ## 1st Qu.: 5.000 1st Qu.:105.0 1st Qu.: 75 1st Qu.: 45.00 ## Median : 5.000 Median :165.0 Median :135 Median : 75.00 ## Mean : 7.254 Mean :179.3 Mean :142 Mean : 81.56 ## 3rd Qu.:10.000 3rd Qu.:240.0 3rd Qu.:195 3rd Qu.:110.00 ## Max. :30.000 Max. :620.0 Max. :530 Max. :230.00 ## ## SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 20.00 1st Qu.: 5.00 1st Qu.:25.00 1st Qu.:15.00 ## Median : 35.00 Median : 15.00 Median :40.00 Median :25.00 ## Mean : 42.49 Mean : 17.72 Mean :37.52 Mean :24.74 ## 3rd Qu.: 60.00 3rd Qu.: 25.00 3rd Qu.:50.00 3rd Qu.:35.00 ## Max. :160.00 Max. :135.00 Max. :90.00 Max. :65.00 ## ## SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 ## Min. : 0.000 Min. : 0.000 Min. : 0 Min. : 0 ## 1st Qu.: 5.000 1st Qu.: 0.000 1st Qu.: 805 1st Qu.: 140 ## Median :10.000 Median : 5.000 Median :1195 Median : 210 ## Mean : 9.866 Mean : 3.332 Mean :1367 Mean : 276 ## 3rd Qu.:15.000 3rd Qu.: 5.000 3rd Qu.:1735 3rd Qu.: 330 ## Max. :25.000 Max. :10.000 Max. :4065 Max. :1375 ## ## CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 ## Min. : 0.0 Min. : 0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 240.0 1st Qu.: 95 1st Qu.:180.0 1st Qu.:115.0 ## Median : 375.0 Median :140 Median :290.0 Median :160.0 ## Mean : 440.1 Mean :163 Mean :316.3 Mean :171.9 ## 3rd Qu.: 560.0 3rd Qu.:205 3rd Qu.:400.0 3rd Qu.:220.0 ## Max. :1510.0 Max. :515 Max. :815.0 Max. :420.0 ## ## POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 ## Min. : 0 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 2755 1st Qu.: 295.0 1st Qu.: 50.00 1st Qu.: 50.00 ## Median : 3865 Median : 425.0 Median : 70.00 Median : 70.00 ## Mean : 4149 Mean : 535.2 Mean : 76.49 Mean : 91.68 ## 3rd Qu.: 5125 3rd Qu.: 680.0 3rd Qu.: 95.00 3rd Qu.:105.00 ## Max. :11675 Max. :2260.0 Max. :200.00 Max. :670.00 ## ## POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 ## Min. : 0 Min. : 0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 170 1st Qu.: 2175 1st Qu.: 390.0 1st Qu.:105.0 ## Median : 295 Median : 3340 Median : 545.0 Median :150.0 ## Mean : 367 Mean : 3614 Mean : 628.2 Mean :186.6 ## 3rd Qu.: 475 3rd Qu.: 4370 3rd Qu.: 845.0 3rd Qu.:235.0 ## Max. :1770 Max. :10805 Max. :2270.0 Max. :710.0 ## ## POPULATIO9 POPULATI10 POPULATI11 POPULATI12 ## Min. : 0.00 Min. : 0.000 Min. : 0.0 Min. : 0.0 ## 1st Qu.:20.00 1st Qu.: 5.000 1st Qu.: 70.0 1st Qu.: 260.0 ## Median :30.00 Median :10.000 Median :110.0 Median : 385.0 ## Mean :30.88 Mean : 8.159 Mean :147.1 Mean : 441.5 ## 3rd Qu.:40.00 3rd Qu.:10.000 3rd Qu.:180.0 3rd Qu.: 600.0 ## Max. :90.00 Max. :35.000 Max. :670.0 Max. :1715.0 ## ## PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 ## Min. : 0 Min. : 0 Min. : 0 Min. : 0 ## 1st Qu.:1170 1st Qu.: 750 1st Qu.: 670 1st Qu.: 550 ## Median :1435 Median :1075 Median :1015 Median : 885 ## Mean :1589 Mean :1181 Mean :1085 Mean : 941 ## 3rd Qu.:1985 3rd Qu.:1490 3rd Qu.:1370 3rd Qu.:1100 ## Max. :4015 Max. :3425 Max. :3150 Max. :2930 ## ## PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.00 ## 1st Qu.: 260.0 1st Qu.: 275.0 1st Qu.: 80.0 1st Qu.: 55.00 ## Median : 385.0 Median : 435.0 Median :145.0 Median : 90.00 ## Mean : 420.9 Mean : 520.2 Mean :144.2 Mean : 95.69 ## 3rd Qu.: 505.0 3rd Qu.: 650.0 3rd Qu.:190.0 3rd Qu.:115.00 ## Max. :1180.0 Max. :1755.0 Max. :535.0 Max. :295.00 ## ## PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 40.00 1st Qu.: 25.00 1st Qu.:10.00 1st Qu.: 15.00 ## Median : 60.00 Median : 40.00 Median :15.00 Median : 25.00 ## Mean : 64.42 Mean : 45.05 Mean :14.39 Mean : 30.48 ## 3rd Qu.: 80.00 3rd Qu.: 55.00 3rd Qu.:20.00 3rd Qu.: 40.00 ## Max. :175.00 Max. :145.00 Max. :45.00 Max. :120.00 ## ## PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 ## Min. : 0.00 Min. : 0.00 Min. : 0.0 Min. : 0.0 ## 1st Qu.:10.00 1st Qu.: 15.00 1st Qu.: 190.0 1st Qu.: 170.0 ## Median :15.00 Median : 30.00 Median : 330.0 Median : 300.0 ## Mean :18.97 Mean : 31.37 Mean : 408.6 Mean : 366.8 ## 3rd Qu.:25.00 3rd Qu.: 40.00 3rd Qu.: 545.0 3rd Qu.: 475.0 ## Max. :60.00 Max. :115.00 Max. :1980.0 Max. :1770.0 ## ## PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 ## Min. : 0.00 Min. : 0 Min. : 0 Min. : 0.0 ## 1st Qu.: 20.00 1st Qu.:1170 1st Qu.: 615 1st Qu.: 0.0 ## Median : 30.00 Median :1435 Median : 945 Median : 0.0 ## Mean : 41.43 Mean :1589 Mean :1023 Mean : 172.2 ## 3rd Qu.: 55.00 3rd Qu.:1985 3rd Qu.:1325 3rd Qu.: 225.0 ## Max. :230.00 Max. :4015 Max. :2930 Max. :2340.0 ## ## OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 ## Min. : 0.000 Min. : 0.0 Min. : 0.00 Min. : 0.0 ## 1st Qu.: 0.000 1st Qu.: 100.0 1st Qu.: 5.00 1st Qu.: 0.0 ## Median : 0.000 Median : 365.0 Median : 15.00 Median : 85.0 ## Mean : 5.877 Mean : 387.5 Mean : 49.01 Mean : 204.3 ## 3rd Qu.: 0.000 3rd Qu.: 565.0 3rd Qu.: 65.00 3rd Qu.: 350.0 ## Max. :190.000 Max. :2435.0 Max. :480.00 Max. :1265.0 ## ## OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 ## Min. : 0.00 Min. : 0.00 Min. : 0.000 Min. : 0 ## 1st Qu.: 10.00 1st Qu.: 5.00 1st Qu.: 0.000 1st Qu.:1170 ## Median : 20.00 Median : 45.00 Median : 0.000 Median :1435 ## Mean : 32.12 Mean : 99.37 Mean : 2.394 Mean :1589 ## 3rd Qu.: 40.00 3rd Qu.:130.00 3rd Qu.: 5.000 3rd Qu.:1990 ## Max. :365.00 Max. :885.00 Max. :35.000 Max. :4015 ## ## PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 175.0 1st Qu.: 380.0 1st Qu.:170.0 1st Qu.:145.0 ## Median : 300.0 Median : 470.0 Median :230.0 Median :225.0 ## Mean : 366.6 Mean : 537.5 Mean :259.4 Mean :271.4 ## 3rd Qu.: 475.0 3rd Qu.: 665.0 3rd Qu.:335.0 3rd Qu.:340.0 ## Max. :1765.0 Max. :1340.0 Max. :780.0 Max. :875.0 ## ## PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN ## Min. : 0.0 Min. : 0.00 Min. : 0 Min. : 0 ## 1st Qu.: 50.0 1st Qu.: 25.00 1st Qu.: 2755 1st Qu.: 2755 ## Median : 80.0 Median : 40.00 Median : 3865 Median : 3865 ## Mean :102.5 Mean : 50.76 Mean : 4148 Mean : 4171 ## 3rd Qu.:125.0 3rd Qu.: 65.00 3rd Qu.: 5125 3rd Qu.: 5290 ## Max. :330.0 Max. :200.00 Max. :11675 Max. :11675 ## ## NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 ## Min. : 0 Min. : 0 Min. : 0.00 Min. : 0.0 ## 1st Qu.: 2725 1st Qu.:2120 1st Qu.: 30.00 1st Qu.: 380.0 ## Median : 3840 Median :3035 Median : 50.00 Median : 645.0 ## Mean : 4105 Mean :3238 Mean : 54.85 Mean : 811.6 ## 3rd Qu.: 5195 3rd Qu.:3930 3rd Qu.: 70.00 3rd Qu.:1035.0 ## Max. :11435 Max. :9410 Max. :165.00 Max. :3560.0 ## ## NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 ## Min. :0.0000 Min. :0 Min. :0 Min. :0 Min. :0 ## 1st Qu.:0.0000 1st Qu.:0 1st Qu.:0 1st Qu.:0 1st Qu.:0 ## Median :0.0000 Median :0 Median :0 Median :0 Median :0 ## Mean :0.1523 Mean :0 Mean :0 Mean :0 Mean :0 ## 3rd Qu.:0.0000 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 ## Max. :5.0000 Max. :0 Max. :0 Max. :0 Max. :0 ## ## NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 ## Min. :0 Min. :0 Min. :0.00000 Min. :0 Min. :0 ## 1st Qu.:0 1st Qu.:0 1st Qu.:0.00000 1st Qu.:0 1st Qu.:0 ## Median :0 Median :0 Median :0.00000 Median :0 Median :0 ## Mean :0 Mean :0 Mean :0.07651 Mean :0 Mean :0 ## 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0.00000 3rd Qu.:0 3rd Qu.:0 ## Max. :0 Max. :0 Max. :5.00000 Max. :0 Max. :0 ## ## NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 ## Min. : 0.0 Min. : 0.0000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 375.0 1st Qu.: 0.0000 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 635.0 Median : 0.0000 Median : 0.000 Median : 0.000 ## Mean : 805.2 Mean : 0.9897 Mean : 1.499 Mean : 1.009 ## 3rd Qu.:1025.0 3rd Qu.: 0.0000 3rd Qu.: 0.000 3rd Qu.: 0.000 ## Max. :3540.0 Max. :25.0000 Max. :15.000 Max. :15.000 ## ## NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 ## Min. : 0.000 Min. : 0.0000 Min. : 0.00 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.0000 1st Qu.: 5.00 1st Qu.: 0.000 ## Median : 0.000 Median : 0.0000 Median : 15.00 Median : 0.000 ## Mean : 5.913 Mean : 0.9676 Mean : 38.56 Mean : 1.747 ## 3rd Qu.: 5.000 3rd Qu.: 0.0000 3rd Qu.: 45.00 3rd Qu.: 5.000 ## Max. :130.000 Max. :25.0000 Max. :345.00 Max. :10.000 ## ## NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 ## Min. : 0.000 Min. : 0.000 Min. :0.00000 Min. : 0.0000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.:0.00000 1st Qu.: 0.0000 ## Median : 0.000 Median : 0.000 Median :0.00000 Median : 0.0000 ## Mean : 1.042 Mean : 5.934 Mean :0.01523 Mean : 0.7054 ## 3rd Qu.: 0.000 3rd Qu.: 5.000 3rd Qu.:0.00000 3rd Qu.: 0.0000 ## Max. :15.000 Max. :135.000 Max. :5.00000 Max. :10.0000 ## ## NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 ## Min. : 0.000 Min. : 0.000 Min. : 0.0000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.0000 1st Qu.: 0.000 ## Median : 0.000 Median : 0.000 Median : 0.0000 Median : 5.000 ## Mean : 4.712 Mean : 1.087 Mean : 0.1926 Mean : 9.759 ## 3rd Qu.: 5.000 3rd Qu.: 0.000 3rd Qu.: 0.0000 3rd Qu.:15.000 ## Max. :180.000 Max. :10.000 Max. :10.0000 Max. :95.000 ## ## NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 ## Min. : 0.00 Min. : 0.000 Min. : 0.00 Min. : 0.000 ## 1st Qu.: 5.00 1st Qu.: 0.000 1st Qu.: 5.00 1st Qu.: 0.000 ## Median : 10.00 Median : 0.000 Median : 15.00 Median : 5.000 ## Mean : 24.46 Mean : 1.324 Mean : 33.83 Mean : 7.339 ## 3rd Qu.: 30.00 3rd Qu.: 0.000 3rd Qu.: 35.00 3rd Qu.: 10.000 ## Max. :355.00 Max. :40.000 Max. :315.00 Max. :165.000 ## ## NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 ## Min. : 0.000 Min. : 0.00 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 10.00 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 0.000 Median : 25.00 Median : 0.000 Median : 0.000 ## Mean : 2.013 Mean : 35.28 Mean : 1.839 Mean : 0.908 ## 3rd Qu.: 5.000 3rd Qu.: 55.00 3rd Qu.: 5.000 3rd Qu.: 0.000 ## Max. :10.000 Max. :315.00 Max. :15.000 Max. :10.000 ## ## NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 ## Min. : 0.0000 Min. :0.00000 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 0.0000 1st Qu.:0.00000 1st Qu.: 25.00 1st Qu.: 5.00 ## Median : 0.0000 Median :0.00000 Median : 40.00 Median :10.00 ## Mean : 0.9412 Mean :0.07719 Mean : 44.03 Mean :11.28 ## 3rd Qu.: 0.0000 3rd Qu.:0.00000 3rd Qu.: 65.00 3rd Qu.:15.00 ## Max. :10.0000 Max. :5.00000 Max. :120.00 Max. :50.00 ## ## NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 ## Min. : 0.000 Min. :0.00000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.:0.00000 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 5.000 Median :0.00000 Median : 0.000 Median : 5.000 ## Mean : 7.259 Mean :0.03851 Mean : 1.249 Mean : 7.886 ## 3rd Qu.: 10.000 3rd Qu.:0.00000 3rd Qu.: 0.000 3rd Qu.:10.000 ## Max. :210.000 Max. :5.00000 Max. :25.000 Max. :60.000 ## ## NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 ## Min. : 0.00 Min. : 0.0000 Min. : 0.0000 Min. : 0.0 ## 1st Qu.:10.00 1st Qu.: 0.0000 1st Qu.: 0.0000 1st Qu.: 35.0 ## Median :20.00 Median : 0.0000 Median : 0.0000 Median : 70.0 ## Mean :20.23 Mean : 0.9428 Mean : 0.8548 Mean :109.7 ## 3rd Qu.:30.00 3rd Qu.: 0.0000 3rd Qu.: 0.0000 3rd Qu.:165.0 ## Max. :80.00 Max. :15.0000 Max. :25.0000 Max. :500.0 ## ## NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 ## Min. : 0.000 Min. : 0.000 Min. : 0.00 Min. : 0.0 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.00 1st Qu.: 0.0 ## Median : 0.000 Median : 0.000 Median : 5.00 Median : 0.0 ## Mean : 2.437 Mean : 3.929 Mean : 12.25 Mean : 3.9 ## 3rd Qu.: 5.000 3rd Qu.: 5.000 3rd Qu.: 15.00 3rd Qu.: 5.0 ## Max. :10.000 Max. :320.000 Max. :135.00 Max. :110.0 ## ## NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 ## Min. : 0.000 Min. : 0.00 Min. :0.0000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.00 1st Qu.:0.0000 1st Qu.: 0.000 ## Median : 0.000 Median : 0.00 Median :0.0000 Median : 5.000 ## Mean : 1.791 Mean : 2.38 Mean :0.2011 Mean : 3.927 ## 3rd Qu.: 0.000 3rd Qu.: 5.00 3rd Qu.:0.0000 3rd Qu.: 5.000 ## Max. :25.000 Max. :15.00 Max. :5.0000 Max. :25.000 ## ## NATIVE_L59 ## Min. : 0.000 ## 1st Qu.: 0.000 ## Median : 0.000 ## Mean : 3.454 ## 3rd Qu.: 5.000 ## Max. :40.000 ## 18.5 Area data Every phenomena can be measured at a location (ask yourself, what exists outside of space?). In point pattern analysis, the unit of support is the point, and the source of randomness is the location itself. Many other forms of data are also collected at points. For instance, when the census collects information on population, at its most basic, the information can be georeferenced to an address, that is, a point. In numerous applications, however, data are not reported at their fundamental unit of support, but rather are aggregated to some other geometry, for instance an area. This is done for several reasons, including the privacy and confidentiality of the data. Instead of reporting individual-level information, the information is reported for zoning systems that often are devised without consideration to any underlying social, natural, or economic processes. Census data, for instance, is reported at different levels of geography. In Canada, the smallest publicly available geography is called a Dissemination Area or DA. A DA in Canada contains a population between 400 and 700 persons. Thus, instead of reporting that one person (or more) are located at a point (i.e., an address), the census reports the population for the DA. Other data are aggregated in similar ways (income, residential status, etc.) At the highest level of aggregation, national level statistics are reported, for instance Gross Domestic Product, or GDP. Economic production is not evenly distributed across space; however, the national GDP does not distinguish regional variations in this process. Ideally, a data analyst would work with data in its most fundamental support. This is not alway possible, and therefore many techniques have been developed to work with data that have been agregated to zones. When working with areas, it is less practical to identify the area with the coordinates (as we did with points). After all, areas will be composed of lines and reporting all the relevant coordinates is impractical. Sometimes the geometric centroids of the areas are used instead. More commonly, areas are assigned an index or unique identifier, so that a region will typically consist of a set of \\(n\\) areas as follows: \\[ R = A_1 \\cup A_2 \\cup A_3 \\cup ...\\cup A_n. \\] The above is read as “the Region R is the union of Areas 1 to n”. Regions can have a set of \\(k\\) attributes or variables associated with them, for instance: \\[ \\textbf{X}_i=[x_{i1}, x_{i2}, x_{i3},...,x_{ik}] \\] These attributes will typically be counts (e.g., number of people in a DA), or some summary measure of the underlying data (e.g., mean commute time). 18.6 Processes and area data Imagine that data on income by household were collected as follows: df &lt;- data.frame(x = c(0.3, 0.4, 0.5, 0.6, 0.7), y = c(0.1, 0.4, 0.2, 0.5, 0.3), Income = c(30000, 30000, 100000, 100000, 100000)) Households are geocoded as points with coordinates x and y, whereas income is in dollars. Plot the income as points (hover over the points to see the attributes): p &lt;- ggplot(data = df, aes(x = x, y = y, color = Income)) + geom_point(shape = 17, size = 5) + coord_fixed() ggplotly(p) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` The underlying process is one of income sorting, with lower incomes to the west, and higher incomes to the east. This could be due to a geographical feature of the landscape (for instance, an escarpment), or the distribution of the housing stock (with a neighborhood that has more expensive houses). These are examples of a variable that responds to a common environmental factor. As an alternative, people may display a preference towards being near others that are similar to them (this is called homophily). When this happens, the variable responds to itself in space. The quality of similarity or disimilarity between neighboring observations of the same variable in space is called spatial autocorrelation. You will learn more about this later on. Another reason why variables reported for areas could display similarities in space is as an consequence of the zoning system. Suppose for a moment that the data above can only be reported at the zonal level, perhaps because of privacy and confidentiality concerns. Thanks to the great talent of the designers of the zoning system (or a felicitous coincidence!), the zoning system is such that it is consistent with the underlying process of sorting. The zones, therefore, are as follows: zones1 &lt;- data.frame(x1=c(0.2, 0.45), x2=c(0.45, 0.80), y1=c(0.0, 0.0), y2=c(0.6, 0.6), Zone_ID = c(&#39;1&#39;,&#39;2&#39;)) If you add these zones to the plot: p &lt;- ggplot() + geom_rect(data = zones1, mapping = aes(xmin = x1, xmax = x2, ymin = y1, ymax = y2, fill = Zone_ID), alpha = 0.3) + geom_point(data = df, aes(x = x, y = y, color = Income), shape = 17, size = 5) + coord_fixed() ggplotly(p) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` What is the mean income in zone 1? What is the mean income in zone 2? Not only are the summary measures of income highly representative of the observations they describe, the two zones are also highly distinct. Imagine now that for whatever reason (lack of prior knowledge of the process, convenience for data collection, etc.) the zones instead are as follows: zones2 &lt;- data.frame(x1=c(0.2, 0.55), x2=c(0.55, 0.80), y1=c(0.0, 0.0), y2=c(0.6, 0.6), Zone_ID = c(&#39;1&#39;,&#39;2&#39;)) If you plot these zones: p &lt;- ggplot() + geom_rect(data = zones2, mapping = aes(xmin = x1, xmax = x2, ymin = y1, ymax = y2, fill = Zone_ID), alpha = 0.3) + geom_point(data = df, aes(x = x, y = y, color = Income), shape = 17, size = 5) + coord_fixed() ggplotly(p) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` What is now the mean income of zone 1? What is the mean income of zone 2? The observations have not changed, and the generating spatial process remains the same. You will notice, however, that the summary measures for the two zones are more similar in this case than they were when the zones more closely captured the underlying process. 18.7 Visualizing area data: choropleth maps The initial step when working with spatial area data, perhaps, is to visualize the data. Commonly, area data are visualized by means of choropleth maps. A choropleth map is a map of the polygons that form the areas in the region, each colored in a way to represent the value of an underlying variable. Lets use ggplot2 to create a choropleth map of population in Hamilton. Notice that the fill color for the polygons is given by cutting the values of POPULATION in five equal segments. In other words, the colors represent zones in the bottom 20% of population, zones in the next 20%, and so on, so that the darkest zones are those with populations so large as to be in the top 20% of the population distribution: ggplot() + geom_polygon(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number(POPULATION, 5)),color = NA, size = 0.1) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(fill = &quot;Population&quot;) Inspecting the map above, would you say that the distribution of population is random, or not random? If not random, what do you think might be an underlying process for the distribution of population. Often, creating a choropleth map using the absolute value of a variable can be somewhat misleading. As seen in the map above, the zones with the largest population are also usually large zones. Any process that you might think of will be confounded by the size of the zones. For this reason, it is often more informative when creating a choropleth map to use a variable that is a rate, for instance population divided by area to give population density: pop_den.map &lt;- ggplot() + geom_polygon(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number(POPULATION/AREA, 5)),color = &quot;white&quot;, size = 0.1) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(fill = &quot;Pop Density&quot;) pop_den.map It can be seen now that the population density is higher in the more central parts of Hamilton, Burlington, Dundas, etc. Does the map look random? If not, what might be an underlying process that explains the variations in population density in a city like Hamilton? Other times, it is appropriate to standardize instead of by area, by what might be called the population at risk. For instance, lets say that we wanted to explore the distribution of the population of older adults (say, 65 and older). In this case, normalizing not by area, but by the total population, would remove the “size” effect, giving a proportion: ggplot() + geom_polygon(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number((AGE_65_TO_ + AGE_70_TO_ + AGE_75_TO_ + AGE_80_TO_ + AGE_85)/POPULATION, 5)),color = NA, size = 0.1) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(fill = &quot;Prop Age 65+&quot;) Do you notice a pattern in the distribution of seniors in the Hamilton, CMA? There are a few things to keep in mind when creating choroplet maps. First, what classification scheme to use, with how many classes, and what colors? The examples above were all created using a classification scheme based on the quintiles of the distribution. As noted above, these are obtained by dividing the sample into 5 equal parts to give bottom 20%, etc., of observations. The quintiles are a particular form of a statistical measure known as quantiles, of which the median is value obtained when the sample is divided in two equal sized parts. Other classification schemes may include the mean, standard deviation, and so on. In terms of how many classes to use, often there is little point in using more than six or seven classes, because the human eye cannot distinguish color differences at a much higher resolution. The colors are a matter of style, but there are coloring schemes that are colorblind safe (see here). Secondly, when the zoning system is irregular (as opposed to, say, a raster), large zones can easily become dominant. In effect, much detail in the maps above is lost for small zones, whereas large zones, especially if similarly colored, may mislead the eye as to their relative frequency. Another mapping technique, the cartogram, is meant to reduce the issues with small-large zones. 18.8 Visualizing area data: cartogram A cartogram is a map where the size of the zones is adjusted so that instead of being the land area, it is proportional to some other variable of interest. Lets illustrate the idea behind the cartogram here. In the maps above, the zones are faithful to their geographical properties. Unfortunately, this obscured the relevance of small zones. A cartogram can be weighted by another variable, say for instance, the population. In this way, the size of the zones will depend on the total population. Cartograms are implemented in R in the package cartogram. CT_pop_cartogram &lt;- cartogram(shp = Hamilton_CT, weight = &quot;POPULATION&quot;) ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 1: 5.94063097656517 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 2: 4.22282836609772 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 3: 3.24121918122005 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 4: 2.70487592121738 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 5: 2.44050500280525 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 6: 2.28238579460187 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 7: 2.14591026585647 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 8: 1.95255176465899 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 9: 1.8186943555214 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 10: 1.73775203417466 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 11: 1.64845060853967 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 12: 1.45162213155545 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 13: 1.37178691507706 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 14: 1.32738198642848 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 15: 1.29727221750122 Notice that the value of the function cartogram (i.e., its output) is a SpatialPolygonsDataFrame. This object needs to be tidied if we wish to use ggplot2 to visualize it: CT_pop_cartogram.t &lt;- tidy(CT_pop_cartogram, region = &quot;TRACT&quot;) CT_pop_cartogram.t &lt;- rename(CT_pop_cartogram.t, TRACT = id) As before, the data were stripped from the tidied version of the dataframe, so they need to be restored: CT_pop_cartogram.t &lt;- left_join(CT_pop_cartogram.t, CT_pop_cartogram@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Plotting the cartogram: ggplot() + geom_polygon(data = CT_pop_cartogram.t, aes(x = long, y = lat, group = group, fill = cut_number(POPULATION, 5)), color = &quot;white&quot;, size = 0.1) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(fill = &quot;Population&quot;) Notice how the size of the zones has been adjusted. The cartogram can be combined with coloring schemes, as in choropleth maps: CT_popden_cartogram &lt;- cartogram(Hamilton_CT, weight = &quot;POP_DENSIT&quot;) ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 1: 29.0772344366743 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 2: 26.933147928614 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 3: 25.1986261892098 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 4: 23.7327939549969 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 5: 22.4463390026744 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 6: 21.2818852763495 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 7: 20.2025834378354 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 8: 19.1839656200552 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 9: 18.2099242308777 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 10: 17.2709049215263 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 11: 16.3609019577218 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 12: 15.4772258019554 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 13: 14.619308950628 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 14: 13.7881685736231 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 15: 12.9857607680246 Tidy and restore the data: CT_popden_cartogram.t &lt;- tidy(CT_popden_cartogram, region = &quot;TRACT&quot;) CT_popden_cartogram.t &lt;- rename(CT_popden_cartogram.t, TRACT = id) CT_popden_cartogram.t &lt;- left_join(CT_popden_cartogram.t, CT_popden_cartogram@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector pop_den.cartogram &lt;- ggplot() + geom_polygon(data = CT_popden_cartogram.t, aes(x = long, y = lat, group = group, fill = cut_number(POP_DENSIT, 5)),color = &quot;white&quot;, size = 0.1) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(fill = &quot;Pop Density&quot;) pop_den.cartogram By combining a cartogram with choropleth mapping, it becomes easier to appreciate the way high population density is concentrated in the central parts of Hamilton, Burlington, etc. grid.arrange(pop_den.map, pop_den.cartogram, nrow = 1) This concludes Practice 9. "],
["area-data-i-activity.html", "19 Area Data I-Activity 19.1 Practice questions 19.2 Learning objectives 19.3 Suggested reading 19.4 Preliminaries 19.5 Activity", " 19 Area Data I-Activity 19.1 Practice questions Answer the following questions: What is a key difference between area data and point data? What is a choropleth map? What is a cartogram? What are the advantages and disadvantages of these mapping techniques? 19.2 Learning objectives In this activity, you will: Create choroplet maps using census data. Think about possible underlying process that could explain the pattern. Think about ways to decide whether a landscape is random when working with area data. 19.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 19.4 Preliminaries For this activity you will need the following: This R markdown notebook. A shape file called Hamilton CMA CT. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(rgdal) library(broom) library(cartogram) In the practice that preceded this activity, you learned about the area data and visualization techniques for area data. Begin by loading the data that you will use in this activity: Hamilton_CT &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA CT&quot;, integer64 = &quot;allow.loss&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 You can obtain new (calculated) variables as follows. For instance, to obtain the proportion of residents who are between 20 and 34 years old, and between 35 and 49: Hamilton_CT@data &lt;- mutate(Hamilton_CT@data, Prop20to34 = (AGE_20_TO_ + AGE_25_TO_ + AGE_30_TO_)/POPULATION, Prop35to49 = (AGE_35_TO_ + AGE_40_TO_ + AGE_45_TO_)/POPULATION) This is a SpatialPolygonDataFrame. Convert to a dataframe (“tidy” it) for plotting using ggplot2: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- rename(Hamilton_CT.t, TRACT = id) Rejoin the data: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector 19.5 Activity Create choropleth maps for the proportion of the population who are 20 to 34 years old, 35 to 49 years old, 50 to 65 years old, and 65 and older. Show your maps to a fellow student. What patterns do you notice in the distribution of age in Hamilton? Devise a rule to decide whether the pattern observed in a choropleth map is random. "],
["area-data-i-bonus-material.html", "20 Area Data I - Bonus Material 20.1 Preliminaries 20.2 Activity 20.3 Bonus", " 20 Area Data I - Bonus Material 20.1 Preliminaries To work with this bonus material you will need: This R markdown notebook. A dataset called Toronto Business Points.RData. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(rgdal) library(broom) library(cartogram) library(plotly) In the practice that preceded this activity, you learned about the area data and visualization techniques for area data. Begin by loading the data that you will use in this activity: Hamilton_CT &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA CT&quot;, integer64 = &quot;allow.loss&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 You can obtain new (calculated) variables as follows. For instance, to obtain the proportion of residents who are between 20 and 34 years old, and between 35 and 49: Hamilton_CT@data &lt;- dplyr::transmute(Hamilton_CT@data, AREA = AREA, TRACT = TRACT, POPULATION = POPULATION, POP20to34 = (AGE_20_TO_ + AGE_25_TO_ + AGE_30_TO_), Prop20to34 = POP20to34/POPULATION, POP35to49 = (AGE_35_TO_ + AGE_40_TO_ + AGE_45_TO_), Prop35to49 = POP35to49/POPULATION, POP50to64 = (AGE_50_TO_ + AGE_55_TO_ + AGE_60_TO_), Prop50to64 = POP50to64/POPULATION, POP65Plus = (AGE_65_TO_ + AGE_70_TO_ + AGE_75_TO_ + AGE_80_TO_ + AGE_85), Prop65Plus = POP65Plus/POPULATION) This is a SpatialPolygonDataFrame. Convert to a dataframe (“tidy” it) for plotting using ggplot2: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- rename(Hamilton_CT.t, TRACT = id) Rejoin the data: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector 20.2 Activity Create choropleth maps for the proportion of the population who are 20 to 34 years old, 35 to 49 years old, 50 to 65 years old, and 65 and older. ggplot() + geom_polygon(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number(Prop20to34, 5)), color = &quot;white&quot;) + coord_fixed() + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + labs(fill = &quot;Prop Age 20 to 34&quot;) ggplot() + geom_polygon(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number(Prop35to49, 5)), color = &quot;white&quot;) + coord_fixed() + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + labs(fill = &quot;Prop Age 35 to 49&quot;) ggplot() + geom_polygon(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number(Prop50to64, 5)), color = &quot;white&quot;) + coord_fixed() + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + labs(fill = &quot;Prop Age 50 to 64&quot;) ggplot() + geom_polygon(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number(Prop65Plus, 5)), color = &quot;white&quot;) + coord_fixed() + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + labs(fill = &quot;Prop Age 65+&quot;) Show your maps to a fellow student. What patterns do you notice in the distribution of age in Hamilton? Devise a rule to decide whether the pattern observed in a choropleth map is random. 20.3 Bonus Create cartograms of the same variables above. Since the cartograms are created based on the SpatialPolygonsDataFrame: Prop20to34.cartogram &lt;- cartogram(shp = Hamilton_CT, weight = &quot;POPULATION&quot;) ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 1: 5.94063097656517 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 2: 4.22282836609772 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 3: 3.24121918122005 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 4: 2.70487592121738 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 5: 2.44050500280525 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 6: 2.28238579460187 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 7: 2.14591026585647 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 8: 1.95255176465899 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 9: 1.8186943555214 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 10: 1.73775203417466 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 11: 1.64845060853967 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 12: 1.45162213155545 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 13: 1.37178691507706 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 14: 1.32738198642848 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 15: 1.29727221750122 Prop35to49.cartogram &lt;- cartogram(shp = Hamilton_CT, weight = &quot;POPULATION&quot;) ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 1: 5.94063097656517 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 2: 4.22282836609772 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 3: 3.24121918122005 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 4: 2.70487592121738 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 5: 2.44050500280525 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 6: 2.28238579460187 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 7: 2.14591026585647 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 8: 1.95255176465899 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 9: 1.8186943555214 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 10: 1.73775203417466 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 11: 1.64845060853967 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 12: 1.45162213155545 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 13: 1.37178691507706 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 14: 1.32738198642848 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 15: 1.29727221750122 Prop50to64.cartogram &lt;- cartogram(shp = Hamilton_CT, weight = &quot;POPULATION&quot;) ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 1: 5.94063097656517 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 2: 4.22282836609772 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 3: 3.24121918122005 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 4: 2.70487592121738 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 5: 2.44050500280525 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 6: 2.28238579460187 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 7: 2.14591026585647 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 8: 1.95255176465899 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 9: 1.8186943555214 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 10: 1.73775203417466 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 11: 1.64845060853967 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 12: 1.45162213155545 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 13: 1.37178691507706 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 14: 1.32738198642848 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 15: 1.29727221750122 Prop65Plus.cartogram &lt;- cartogram(shp = Hamilton_CT, weight = &quot;POPULATION&quot;) ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 1: 5.94063097656517 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 2: 4.22282836609772 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 3: 3.24121918122005 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 4: 2.70487592121738 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 5: 2.44050500280525 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 6: 2.28238579460187 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 7: 2.14591026585647 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 8: 1.95255176465899 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 9: 1.8186943555214 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 10: 1.73775203417466 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 11: 1.64845060853967 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 12: 1.45162213155545 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 13: 1.37178691507706 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 14: 1.32738198642848 ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Warning in RGEOSMiscFunc(spgeom, byid, &quot;rgeos_area&quot;): Spatial object is not ## projected; GEOS expects planar coordinates ## Mean size error for iteration 15: 1.29727221750122 Tidy and restore the data: Prop20to34.cartogram.t &lt;- tidy(Prop20to34.cartogram, region = &quot;TRACT&quot;) Prop20to34.cartogram.t &lt;- rename(Prop20to34.cartogram.t, TRACT = id) Prop20to34.cartogram.t &lt;- left_join(Prop20to34.cartogram.t, select(Prop20to34.cartogram@data, TRACT, Prop20to34), by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Prop20to34.cartogram.t &lt;- rename(Prop20to34.cartogram.t, Proportion = Prop20to34) Prop35to49.cartogram.t &lt;- tidy(Prop35to49.cartogram, region = &quot;TRACT&quot;) Prop35to49.cartogram.t &lt;- rename(Prop35to49.cartogram.t, TRACT = id) Prop35to49.cartogram.t &lt;- left_join(Prop35to49.cartogram.t, select(Prop35to49.cartogram@data, TRACT, Prop35to49), by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Prop35to49.cartogram.t &lt;- rename(Prop35to49.cartogram.t, Proportion = Prop35to49) Prop50to64.cartogram.t &lt;- tidy(Prop50to64.cartogram, region = &quot;TRACT&quot;) Prop50to64.cartogram.t &lt;- rename(Prop50to64.cartogram.t, TRACT = id) Prop50to64.cartogram.t &lt;- left_join(Prop50to64.cartogram.t, select(Prop50to64.cartogram@data, TRACT, Prop50to64), by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Prop50to64.cartogram.t &lt;- rename(Prop50to64.cartogram.t, Proportion = Prop50to64) Prop65Plus.cartogram.t &lt;- tidy(Prop65Plus.cartogram, region = &quot;TRACT&quot;) Prop65Plus.cartogram.t &lt;- rename(Prop65Plus.cartogram.t, TRACT = id) Prop65Plus.cartogram.t &lt;- left_join(Prop65Plus.cartogram.t, select(Prop65Plus.cartogram@data, TRACT, Prop65Plus), by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Prop65Plus.cartogram.t &lt;- rename(Prop65Plus.cartogram.t, Proportion = Prop65Plus) Stack all cartograms into a single dataframe, add factor for frames: Cartograms &lt;- rbind(data.frame(Prop20to34.cartogram.t, factor = &quot;Age 20 to 34&quot;), data.frame(Prop35to49.cartogram.t, factor = &quot;Age 35 to 49&quot;), data.frame(Prop50to64.cartogram.t, factor = &quot;Age 50 to 64&quot;), data.frame(Prop65Plus.cartogram.t, factor = &quot;Age 65+&quot;)) Create plot with frames: Cartograms.plot &lt;- ggplot(Cartograms, aes(x= long, y = lat, group = group, fill = cut_number(Proportion, 5))) + geom_polygon(aes(frame = factor), color = &quot;white&quot;) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(fill = &quot;Proportion&quot;) ## Warning: Ignoring unknown aesthetics: frame Plot: ggplotly(Cartograms.plot) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` Try rectangular cartograms: library(recmap) ## Loading required package: GA ## Loading required package: foreach ## ## Attaching package: &#39;foreach&#39; ## The following objects are masked from &#39;package:purrr&#39;: ## ## accumulate, when ## Loading required package: iterators ## Package &#39;GA&#39; version 3.0.2 ## Type &#39;citation(&quot;GA&quot;)&#39; for citing this R package in publications. ## Loading required package: Rcpp ## Warning: package &#39;Rcpp&#39; was built under R version 3.4.4 Rectangular algorithms require the centroids of the zones, which can be obtained by means of sp::coordinates xy_centroids &lt;- coordinates(Hamilton_CT) Create a frame for the rectangular cartograms: rec_frame &lt;- data.frame(x = xy_centroids[,1], y = xy_centroids[,2], dx = sqrt(Hamilton_CT$AREA) / 2 / (0.7 * 60 * cos(xy_centroids[,2] * pi / 180)), dy = sqrt(Hamilton_CT$AREA) / 2 / (0.7 * 60)) Generate the input for the recmap function. This is the frame for the cartograms rec_frame and a variable z: POP.rec &lt;- data.frame(rec_frame, z = Hamilton_CT$POPULATION + 1, name = Hamilton_CT$TRACT) head(POP.rec) ## x y dx dy z name ## 0 -80.09035 43.21641 0.13005043 0.09477718 1792 5370121.00 ## 1 -80.01584 43.17435 0.14359679 0.10472154 2204 5370120.02 ## 2 -79.99604 43.20537 0.02749599 0.02004195 5575 5370122.01 ## 3 -80.00280 43.22201 0.03998909 0.02914027 6742 5370122.02 ## 4 -79.96782 43.21894 0.04455620 0.03246999 7354 5370123.00 ## 5 -79.97516 43.24128 0.06251228 0.04553865 3719 5370124.00 POP.rec.cartogram &lt;- recmap(POP.rec) ## Warning in recmap_(V, E): 5370140.04 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370140.03 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370084.04 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370086.00 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370301.00 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370303.01 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370303.02 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370302.00 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370300.00 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370085.03 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370085.02 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370085.01 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370084.05 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370084.03 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370084.02 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370084.01 could not be placed on the first ## attempt; ## Warning in recmap_(V, E): 5370072.04 could not be placed on the first ## attempt; plot(POP.rec.cartogram) ggplot(data = POP.rec.cartogram) + geom_rect(aes(xmin = x - dx, xmax = x + dx, ymin = y - dy, ymax = y + dy)) + coord_fixed() Cartograms.rec &lt;- rbind(data.frame(POP.rec.cartogram, Proportion = Hamilton_CT$Prop20to34, factor = &quot;Age 20 to 34&quot;), data.frame(POP.rec.cartogram, Proportion = Hamilton_CT$Prop35to49, factor = &quot;Age 35 to 49&quot;), data.frame(POP.rec.cartogram, Proportion = Hamilton_CT$Prop50to64, factor = &quot;Age 50 to 64&quot;), data.frame(POP.rec.cartogram, Proportion = Hamilton_CT$Prop65Plus, factor = &quot;Age 65+&quot;)) Create plot with frames: Cartograms.rec.plot &lt;- ggplot(Cartograms.rec, aes(xmin = x - dx, xmax = x + dx, ymin = y - dy, ymax = y + dy, fill = cut_number(Proportion, 5))) + geom_rect(aes(frame = factor)) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + coord_fixed() + theme(legend.position = &quot;bottom&quot;) + labs(fill = &quot;Proportion&quot;) ## Warning: Ignoring unknown aesthetics: frame Plot: plot &lt;- Cartograms.rec.plot %&gt;% animation_opts(frame = 1000, transition = 1000, easing = &quot;elastic&quot;) ggplotly(plot) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` "],
["area-data-ii.html", "21 Area Data II 21.1 Introduction 21.2 Learning objectives 21.3 Suggested reading 21.4 Preliminaries 21.5 Proximity in area data 21.6 Spatial weights matrices 21.7 Creating spatial weights matrices in R 21.8 Spatial moving averages", " 21 Area Data II 21.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In last previous practice/session, you learned about area data and practiced some visualization techniques for spatial data of this type, specifically choropleth maps and cartograms. You also thought about rules to decide whether a mapped variable displayed a spatially random distribution of values. For this practice you will need the following: This R markdown notebook. A shape file called “Hamilton CMA CT” This dataset includes the spatial information for the census tracts in the Hamilton Census Metropolitan Area (as polygons), and a host of demographic variables from the census of Canada, including population and languages. 21.2 Learning objectives In this practice, you will learn about: The concept of proximity for area data. How to formalize the concept of proximity: spatial weights matrices. How to create spatial weights matrices in R. The use of spatial moving averages. Other criteria for coding proximity. 21.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 21.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(rgdal) library(broom) library(plotly) library(spdep) ## Warning: package &#39;spdep&#39; was built under R version 3.4.4 ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## expand ## Loading required package: spData ## Warning: package &#39;spData&#39; was built under R version 3.4.4 Read the data that you will use for this practice. This is an Esri shape file that will be saved as an object of class SpatialPolygonDataFrame. The function used to read Esri shape files is rgdal::readOGR. Setting integer64 to “allow.loss” keeps the data as integers as opposed to changing to factors or strings: ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 Clear the dataframe, retain only ID, TRACT, and POP_DENSIT: Hamilton_CT@data &lt;- transmute(Hamilton_CT@data, ID = ID, TRACT = TRACT, POP_DENSIT = POP_DENSIT) To use the plotting functions of ggplot2, the SpatialPolygonDataFrame needs to be “tidied” by means of the tidy function of the broom package: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- dplyr::rename(Hamilton_CT.t, TRACT = id) Tidying the spatial dataframe strips it from the non-spatial information, but we can add all the data by means of the left_join function: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Now the tidy dataframe Hamilton_DA.t contains the spatial information and the data. You can quickly verify the contents of the dataframe by means of summary: summary(Hamilton_CT.t) ## long lat order hole ## Min. :-80.25 Min. :43.05 Min. : 1 Mode :logical ## 1st Qu.:-79.93 1st Qu.:43.21 1st Qu.: 7304 FALSE:29212 ## Median :-79.86 Median :43.24 Median :14606 ## Mean :-79.86 Mean :43.25 Mean :14606 ## 3rd Qu.:-79.80 3rd Qu.:43.28 3rd Qu.:21909 ## Max. :-79.51 Max. :43.48 Max. :29212 ## ## piece group TRACT ID ## 1:29212 5370124.00.1: 822 Length:29212 Min. : 919807 ## 5370121.00.1: 661 Class :character 1st Qu.: 920233 ## 5370142.01.1: 642 Mode :character Median : 937830 ## 5370100.00.1: 615 Mean : 946568 ## 5370101.02.1: 602 3rd Qu.: 959451 ## 5370142.02.1: 595 Max. :1115750 ## (Other) :25275 ## POP_DENSIT ## Min. : 2.591 ## 1st Qu.: 254.658 ## Median : 1511.957 ## Mean : 1890.627 ## 3rd Qu.: 2807.857 ## Max. :14234.286 ## 21.5 Proximity in area data In the earlier part of the course, when working with point data, the spatial relationships among events (their proximity) were more or less unambiguously given by their relative location, or more precisely by their distance. Hence, we had quadrat-based techniques (relative location with respect to a grid) and distance-based techniques (event-to-event and point-to-event). In the case of area data, spatial proximity can be represented in more ways, given the characteristics of areas. In particular, an area contains an infinite number of points, and measuring distance between two areas leads to many possible results, depending on which pairs of points within two zones are used to measure the distance. Consider the simple systems shown in Figure 1. Which of zones \\(A_2\\), \\(A_3\\), and \\(A_4\\) is more proximate to \\(A_1\\)? Figure 1. Simple zoning system If points are selected in such a way that they are on the overlapping edges of two contiguous areas, the distance between the two areas clearly is zero, and they must be proximate. This criterion to define proximity is called adjacency. Adjacency means that two zones share a common edge. This is conventionally called the rook criterion, after chess, in which the piece called the rook can move only orthogonally. The rook criterion, however, would dictate that zones \\(A_2\\) and \\(A_6\\) are not proximate, despite being closer than \\(A_2\\) and \\(A_3\\). When this criterion is expanded to allow contact at a single point between zones (say, the corner between \\(A_2\\) and \\(A_6\\)), the adjacency criterion is called queen, again, for the chess piece that moves both orthogonally and diagonally. If we accept adjacency as a reasonable way of expressing relationships of proximity between areas, what we need is a way of coding relationships of adjacency in a way that is convenient and amenable to manipulation for data analysis. One of the most widely used tools to code proximity in area data is the spatial weights matrix. 21.6 Spatial weights matrices A spatial weights matrix is an arrangement of values (or weights) for all pairs of zones in a system. For instance, in a zoning system such as shown in Figure 1, with 6 zones, there will be \\(6 \\times 6\\) such weights. The weights are organized by rows, in such a way that each zone has a corresponding row of weights. For example, zone \\(A_1\\) in Figure 1 has weights: \\[ w_{1\\cdot} = [w_{11}, w_{12}, w_{13}, w_{14}, w_{15}, w_{16}] \\] The values of the weights depend on the adjacency criterion adopted. The simplest coding scheme is when we assign a value of 1 to pairs of zones that are adjacent, and a value of 0 to pairs of zones that are not. Lets formalize the two criteria mentioned above: Rook criterion \\[ w_{ij}=\\bigg\\{\\begin{array}{l l} 1\\text{ if } A_i \\text{ and } A_j \\text{ share an edge}\\\\ 0\\text{ otherwise}\\\\ \\end{array} \\] Queen criterion \\[ w_{ij}=\\bigg\\{\\begin{array}{l l} 1\\text{ if } A_i \\text{ and } A_j \\text{ share an edge or a vertex}\\\\ 0\\text{ otherwise}\\\\ \\end{array} \\] If queen adjacency is used, the weights for zone \\(A_6\\) are as follows: \\[ w_{6\\cdot} = [0, 1, 0, 1, 1, 0]. \\] As you can see, the adjacent areas from the perspective of \\(A_6\\) are \\(A_4\\) and \\(A_5\\) (by virtue of sharing an edge), and \\(A_2\\) (by virtue of sharing a vertex). These three areas receive weights of 1. On the other hand, \\(A_1\\) and \\(A_3\\) are not adjacent, and therefore receive a weight of zero. Notice how the weight \\(w_{66}\\) is set to zero. By convention, an area is not its own neighbor! The set of weights above define the neighborhood of \\(A_6\\). The spatial weights matrix for the whole system in Figure 1 is as follows: \\[ \\textbf{W}=\\left (\\begin{array}{c c c c c c} 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1\\\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0\\\\ \\end{array} \\right). \\] Compare the matrix to the zoning system. The spatial weights matrix has the following properties: The main diagonal elements are all zeros (no area is its own neighbor). Each zone has a row of weights in the matrix: row number one corresponds to \\(A_1\\), row number two corresponds to \\(A_2\\), and so on. Likewise, each zone has a column of weights. The sum of all values in a row gives the total number of neighbors for an area. That is: \\[ \\text{The total number of neighbors of } A_i \\text{ is given by: }\\sum_{j=1}^{n}{w_{ij}} \\] The spatial weights matrix is often processed to obtain a row-standardized spatial weights matrix. This procedure consists of dividing all weights by the sum of their corresponding row (i.e., by the total number of neighbors), as follows: \\[ w_{ij}^{st}=\\frac{w_{ij}}{\\sum_{j=1}^n{w_{ij}}} \\] The row-standardized weights matrix for the system in Figure 1 is: \\[ \\textbf{W}^{st}=\\left (\\begin{array}{c c c c c c} 0 &amp; 1/3 &amp; 1/3 &amp; 1/3 &amp; 0 &amp; 0\\\\ 1/4 &amp; 0 &amp; 0 &amp; 1/4 &amp; 1/4 &amp; 1/4\\\\ 1/2 &amp; 0 &amp; 0 &amp; 1/2 &amp; 0 &amp; 0\\\\ 1/5 &amp; 1/5 &amp; 1/5 &amp; 0 &amp; 1/5 &amp; 1/5\\\\ 0 &amp; 1/3 &amp; 0 &amp; 1/3 &amp; 0 &amp; 1/3\\\\ 0 &amp; 1/3 &amp; 0 &amp; 1/3 &amp; 1/3 &amp; 0\\\\ \\end{array} \\right). \\] The row-standardized spatial weights matrix has the following properties: Each weight now represents the proportion of a neighbor out of the total of neighbors. For instance, since the total of neighbors of \\(A_1\\) is 3, each neighbor contributes 1/3 to that total. The sum of all weights over a row equals 1, or 100% of all neighbors for that zone. 21.7 Creating spatial weights matrices in R Coding spatial weights matrices by hand is a tedious and error-prone process. Fortunately, functions to generate them exist in R. The package spdep, for instance, has a number of useful utilities for working with spatial weights matrices. The first step to create a spatial weights matrix is to find the neighbors (i.e., areas adjacent to) for each area. The function poly2nb is used for this. The input argument is a SpatialPolygonDataFrame. You can verify that the object Hamilton_CT is a SpatialPolygonDataFrame: class(Hamilton_CT) ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; The following finds the neighbors (note that the default adjacency criterion is queen): Hamilton_CT.nb &lt;- poly2nb(pl = Hamilton_CT) The value (output) of the function is an object of class nb: class(Hamilton_CT.nb) ## [1] &quot;nb&quot; The function summary applied to an object of this class gives some useful information about the neighbors, including the number of areas in this system (188), the total number of neighbors (1180), and the percentage of neighbors out of all pairs of areas (3.34%). Other information includes the distribution of neighbors (3 zones have two neighbors, 8 zones have three neighbors, 22 zones have four neighbors, and so on): summary(Hamilton_CT.nb) ## Neighbour list object: ## Number of regions: 188 ## Number of nonzero links: 1180 ## Percentage nonzero weights: 3.338615 ## Average number of links: 6.276596 ## Link number distribution: ## ## 2 3 4 5 6 7 8 9 10 11 12 14 ## 3 8 22 32 35 45 30 6 1 1 4 1 ## 3 least connected regions: ## 173 174 187 with 2 links ## 1 most connected region: ## 32 with 14 links The nb object is a list that contains the neighbors for each area. For instance, the neighbors of census tract 5370001.01 (the first tract in the dataframe) are the following tracts: Hamilton_CT$TRACT[Hamilton_CT.nb[[1]]] ## [1] 5370120.02 5370122.01 5370122.02 5370124.00 5370142.01 5370133.01 ## [7] 5370130.03 ## 188 Levels: 5370001.01 5370001.02 5370001.04 5370001.05 ... 5370303.02 The list of neighbors can be converted into a list of entries in a spatial weights matrix W by means of the function nb2list2 (for “neighbors to W in list form”): Hamilton_CT.w &lt;- nb2listw(Hamilton_CT.nb) We can visualize the neighbors (adjacent) areas: plot(Hamilton_CT, border = &quot;gray&quot;) plot(Hamilton_CT.nb, coordinates(Hamilton_CT), col = &quot;red&quot;, add = TRUE) 21.8 Spatial moving averages The spatial weights matrix, and in particular its row-standardized version, is useful to calculate a spatial statistic, the spatial moving average. The spatial moving average is a variation on the mean statistic. Recall that the mean is calculated as the sum of all relevant values divided by the number of values summed. In the case of spatial data, the mean is what we would call a global statistics, since it is calculated using all data for a region: \\[ \\overline{x}=\\frac{1}{n}\\sum_{j=1}^{n}{x_j} \\] where \\(\\overline{x}\\) (read x-bar) is the mean of all values of x. A spatial moving average is calculated in the same way, but for each area, and based only on the values of proximate areas: \\[ \\overline{x_i}=\\frac{1}{n_i}\\sum_{j\\in N(i)}{x_j} \\] where \\(n_i\\) is the number of neighbors of \\(A_i\\), and the sum is only for \\(x_j\\) that are in the neighborhood of i (\\(j\\in N(i)\\) is read “j in the neighborhood of i”). Lets illustrate this by making reference again to Figure 1. Consider \\(A_1\\). The spatial weights matrix indicates that the neighborhood of \\(A_1\\) consists of three areas: \\(A_2\\), \\(A_3\\), and \\(A_4\\). Therefore \\(n_1=3\\), and \\(j\\in N(1)\\) are 2, 3, and 4. The spatial moving average of \\(A_1\\) for a variable \\(x\\) would then be calculated as: \\[ \\overline{x_1}=\\frac{x_2 + x_3 + x_4}{3} \\] Notice that another way of writing the spatial moving average expression is as follows, since membership in the neighborhood of \\(i\\) is implicit in the definition of \\(w_{ij}\\)! Since \\(w_{ij}\\) takes values of zero and one, the effect is to turn on and off the values of \\(x\\) depending on whether they are for areas adjacent to \\(i\\): \\[ \\overline{x_i}=\\frac{1}{n_i}\\sum_{j=1}^n{w_{ij}x_j} \\] This means that the spatial moving average of \\(A_1\\) for a variable \\(x\\) on this system can also be calculated using the spatial weights matrix as: \\[ \\overline{x_1}=\\frac{w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + w_{15}x_5 + w_{12}x_6}{3} \\] Substituting the spatial weights: \\[ \\overline{x_1}=\\frac{0x_1 + 1x_2 + 1x_3 + 1x_4 + 0x_5 + 0x_6}{3} = \\frac{x_2 + x_3 + x_4}{3} \\] In other words, the spatial weights can be used directly in the calculation of spatial moving averages. Further, notice that: \\[ n_i=\\sum_{j=1}^{n}w_{ij} \\] which is simply the total number of neighbors of \\(A_i\\), and the value we used to row-standardize the spatial weights. Since the row-standardized weights have already been divided by the number of neighbors, we can use them to express the spatial moving average as follows: \\[ \\overline{x_i}=\\sum_{j=1}^{n}{w_{ij}^{st}x_j} \\] Continuing the example, if we use the row-standardized weights, the spatial moving average at \\(A_1\\) is: \\[ \\overline{x_i}=0x_1 + \\frac{1}{3}x_2 + \\frac{1}{3}x_3 + \\frac{1}{3}x_4 + 0x_5 + 0x_6 \\] which is the same as: \\[ \\overline{x_i}=\\frac{x_2 + x_3 + x_4}{3} \\] Consider the following map of Hamilton’s population density: map &lt;- ggplot() + geom_polygon(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number(POP_DENSIT, 5), POP_DENSIT = round(POP_DENSIT), TRACT = TRACT), color = &quot;black&quot;) + geom_polygon(data = subset(Hamilton_CT.t, TRACT == &quot;5370142.02&quot;), aes(x = long, y = lat, group = group, POP_DENSIT = round(POP_DENSIT), TRACT = TRACT), color = &quot;red&quot;, weight = 3, fill = NA) + geom_polygon(data = subset(Hamilton_CT.t, TRACT == &quot;5370144.01&quot;), aes(x = long, y = lat, group = group, POP_DENSIT = round(POP_DENSIT), TRACT = TRACT), color = &quot;green&quot;, weight = 3, fill = NA) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + labs(fill = &quot;Pop Density&quot;) + coord_equal() ## Warning: Ignoring unknown aesthetics: POP_DENSIT, TRACT ## Warning: Ignoring unknown parameters: weight ## Warning: Ignoring unknown aesthetics: POP_DENSIT, TRACT ## Warning: Ignoring unknown parameters: weight ## Warning: Ignoring unknown aesthetics: POP_DENSIT, TRACT ggplotly(map, tooltip = c(&quot;TRACT&quot;, &quot;POP_DENSIT&quot;)) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` Manually calculate the spatial moving average for tract 5370142.02 (with the red boundary) and tract (with the green boundary). Tip: hover over the tracts to see their population densities. (32 + 109 + 48)/3 ## [1] 63 (48 + 55 + 125)/3 ## [1] 76 Spatial moving averages can be calculated in a straighforward way by means of the function lag.listw function of the spdep package. This function uses a spatial weights matrix and automatically selects the row-standardized weights. Lets calculate the spatial moving average of population density: POP_DENSIT.sma &lt;- lag.listw(x = Hamilton_CT.w, Hamilton_CT$POP_DENSIT) Lets now plot the spatial moving average of population density. First we add this variable to our tidy dataframe: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, data.frame(TRACT = Hamilton_CT$TRACT, POP_DENSIT.sma)) ## Joining, by = &quot;TRACT&quot; ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector And plot: map.sma &lt;- ggplot() + geom_polygon(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number(POP_DENSIT.sma, 5), POP_DENSIT.sma = round(POP_DENSIT.sma), TRACT = TRACT), color = &quot;black&quot;) + geom_polygon(data = subset(Hamilton_CT.t, TRACT == &quot;5370142.02&quot;), aes(x = long, y = lat, group = group, POP_DENSIT.sma = round(POP_DENSIT.sma), TRACT = TRACT), color = &quot;red&quot;, weight = 3, fill = NA) + geom_polygon(data = subset(Hamilton_CT.t, TRACT == &quot;5370144.01&quot;), aes(x = long, y = lat, group = group, POP_DENSIT.sma = round(POP_DENSIT.sma), TRACT = TRACT), color = &quot;green&quot;, weight = 3, fill = NA) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + labs(fill = &quot;Pop Density SMA&quot;) + coord_equal() ## Warning: Ignoring unknown aesthetics: POP_DENSIT.sma, TRACT ## Warning: Ignoring unknown parameters: weight ## Warning: Ignoring unknown aesthetics: POP_DENSIT.sma, TRACT ## Warning: Ignoring unknown parameters: weight ## Warning: Ignoring unknown aesthetics: POP_DENSIT.sma, TRACT ggplotly(map.sma, tooltip = c(&quot;TRACT&quot;, &quot;POP_DENSIT.sma&quot;)) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` Verify that your manual calculations for the two tracts above are correct. What differences do you notice between the map of population density and the map of spatial moving averages of population density? 3#Other criteria for coding proximity Adjacenty is not the only criterion for coding proximity. Occasionally, the distance between areas is calculated by using the centroids of the areas as their representative points. A centroid is simply the mean of the coordinates of the edges of an area, and in this way represent the “centre of gravity” of the area. The inter-centroid distance allows us to define additional criteria for proximity, including neighbors within a certain distance threshold, and k-nearest neighbors. Distance-based criterion \\[ w_{ij}=\\bigg\\{\\begin{array}{l l} 1\\text{ if inter-centroid distance } d_{ij}\\leq \\delta\\\\ 0\\text{ otherwise}\\\\ \\end{array} \\] where \\(\\delta\\) is a distance threshold. Distance-based nearest neighbors can be obtained in R by means of the function dnearneigh. First we need to obtain the coordinates of the centroids of the areas: CT_centroids &lt;- coordinates(Hamilton_CT) A nearest neighbors object nb is produced as follows (selecting a distance threshold between 0 and 5 km): Hamilton_CT.dnb &lt;- dnearneigh(CT_centroids, d1 = 0, d2 = 5, longlat = TRUE) We can visualize the neighbors (adjacent) areas: plot(Hamilton_CT, border = &quot;gray&quot;) plot(Hamilton_CT.dnb, CT_centroids, col = &quot;red&quot;, add = TRUE) Try changing the distance threshold to see how different neighborhoods are defined. k-nearest neighbors \\[ w_{ij}=\\bigg\\{\\begin{array}{l l} 1\\text{ if } A_j \\text{ is one of k-nearest neighbors of } A_i\\\\ 0\\text{ otherwise}\\\\ \\end{array} \\] A potential disadvantage of using a distance-based criterion is that for zoning systems with areas of vastly different sizes, small areas will end up having many neighbors, whereas large areas will have few or none. The criterion of k-nearest neighbors allows for some adaptation to the size of the areas. Under this criterion, all areas have the exact same number of neighbors, but the geographical extent of the neighborhood may (and likely will) change. In R, k-nearest neighbors can be obtained by means of the function knearneigh, and the arguments include the value of k: Hamilton_CT.knb &lt;- knn2nb(knearneigh(CT_centroids, k = 3, longlat = TRUE)) We can again visualize the neighbors (adjacent) areas: plot(Hamilton_CT, border = &quot;gray&quot;) plot(Hamilton_CT.knb, CT_centroids, col = &quot;red&quot;, add = TRUE) Try changing the value of k to see how the neighborhoods change. This concludes Practice 10. "],
["area-data-ii-activity.html", "22 Area Data II-Activity 22.1 Practice questions 22.2 Learning objectives 22.3 Suggested reading 22.4 Preliminaries 22.5 Activity", " 22 Area Data II-Activity 22.1 Practice questions Answer the following questions: List and describe two criteria to define proximity in area data analysis. What is a spatial weights matrix? Why do spatial weight matrices have zeros in the main diagonal? How is a spatials weights matrix row-standardized? Write the spatial weights matrices for the sample systems in Figures 1 and 2. Explain the criteria used to do so. Figure 1. Sample areal system 1 Figure 2. Sample areal system 2 22.2 Learning objectives In this activity, you will: Create spatial weights matrices. Calculate the spatial moving average of a variable. Create scatterplots of a variable and its spatial moving average. Think about ways to decide whether a landscape is random when working with area data. 22.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 22.4 Preliminaries For this activity you will need the following: This R markdown notebook. A shape file called Hamilton CMA CT. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(rgdal) library(broom) library(spdep) library(plotly) In the practice that preceded this activity, you learned about the area data and visualization techniques for area data. Begin by loading the data that you will use in this activity: Hamilton_CT &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA CT&quot;, integer64 = &quot;allow.loss&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 You can obtain new (calculated) variables as follows. For instance, to obtain the proportion of residents who are between 20 and 34 years old, and between 35 and 49: Hamilton_CT@data &lt;- mutate(Hamilton_CT@data, Prop20to34 = (AGE_20_TO_ + AGE_25_TO_ + AGE_30_TO_)/POPULATION, Prop35to49 = (AGE_35_TO_ + AGE_40_TO_ + AGE_45_TO_)/POPULATION) This is a SpatialPolygonDataFrame. Convert to a dataframe (“tidy” it) for plotting using ggplot2: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- rename(Hamilton_CT.t, TRACT = id) Rejoin the data: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector 22.5 Activity Create a spatial weights matrix for the census tracts in the Hamilton CMA. Calculate the proportion of spatial moving average for the proportion of the population who are 20 to 34 years old, 35 to 49 years old, 50 to 65 years old, and 65 and older. Append the spatial moving averages to your dataframe. Choose one age group and create a scatterplot of the proportion of population in that group versus its spatial moving average. (Hint: if creating the scatterplot in ggplot2 you can add the 45 degree line by using geom_abline(slope = 1, intercept = 0)). What would you expect the scatterplots of the variables and their spatial moving averages to look like if the variable was spatially random? Discuss. "],
["area-data-iii.html", "23 Area Data III 23.1 Introduction 23.2 Learning objectives 23.3 Suggested reading 23.4 Preliminaries 23.5 Spatial moving averages and simulation 23.6 Spatial autocorrelation and Moran’s I coefficient 23.7 Moran’s I and Moran’s scatterplot 23.8 Hypothesis testing for spatial autocorrelation", " 23 Area Data III 23.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In last previous practice/session, you learned about different ways to define proximity for area data, about spatial weights matrices, and how spatial weights matrices could be used to calculate spatial moving averages. For this practice you will need the following: This R markdown notebook. A shape file called “Hamilton CMA CT” This dataset includes the spatial information for the census tracts in the Hamilton Census Metropolitan Area (as polygons), and a host of demographic variables from the census of Canada, including population and languages. 23.2 Learning objectives In this practice, you will learn about: Spatial moving averages and simulation. The concept of spatial autocorrelation. Moran’s I coefficient and Moran’s scatterplot. Hypothesis testing for spatial autocorrelation. 23.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 23.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(rgdal) library(broom) #library(plotly) library(spdep) library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths library(gridExtra) Read the data that you will use for this practice. This is an Esri shape file that will be saved as an object of class SpatialPolygonDataFrame. The function used to read Esri shape files is rgdal::readOGR. Setting integer64 to “allow.loss” keeps the data as integers as opposed to changing to factors or strings: ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 Clear the dataframe, retain only ID, TRACT, and POP_DENSIT: Hamilton_CT@data &lt;- transmute(Hamilton_CT@data, ID = ID, TRACT = TRACT, POP_DENSIT = POP_DENSIT) To use the plotting functions of ggplot2, the SpatialPolygonDataFrame needs to be “tidied” by means of the tidy function of the broom package: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- dplyr::rename(Hamilton_CT.t, TRACT = id) Tidying the spatial dataframe strips it from the non-spatial information, but we can add all the data by means of the left_join function: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Now the tidy dataframe Hamilton_DA.t contains the spatial information and the data. You can quickly verify the contents of the dataframe by means of summary: summary(Hamilton_CT.t) ## long lat order hole ## Min. :-80.25 Min. :43.05 Min. : 1 Mode :logical ## 1st Qu.:-79.93 1st Qu.:43.21 1st Qu.: 7304 FALSE:29212 ## Median :-79.86 Median :43.24 Median :14606 ## Mean :-79.86 Mean :43.25 Mean :14606 ## 3rd Qu.:-79.80 3rd Qu.:43.28 3rd Qu.:21909 ## Max. :-79.51 Max. :43.48 Max. :29212 ## ## piece group TRACT ID ## 1:29212 5370124.00.1: 822 Length:29212 Min. : 919807 ## 5370121.00.1: 661 Class :character 1st Qu.: 920233 ## 5370142.01.1: 642 Mode :character Median : 937830 ## 5370100.00.1: 615 Mean : 946568 ## 5370101.02.1: 602 3rd Qu.: 959451 ## 5370142.02.1: 595 Max. :1115750 ## (Other) :25275 ## POP_DENSIT ## Min. : 2.591 ## 1st Qu.: 254.658 ## Median : 1511.957 ## Mean : 1890.627 ## 3rd Qu.: 2807.857 ## Max. :14234.286 ## 23.5 Spatial moving averages and simulation Last practice/activity you learned about spatial weights matrices as a way to define proximity in the analysis of area data. You also used spatial moving averages to explore spatial patterns in area data. Lets briefly revisit these notions. Here, you create a spatial weights matrix for Hamilton CMA census tracts: Hamilton_CT.nb &lt;- poly2nb(pl = Hamilton_CT) Hamilton_CT.w &lt;- nb2listw(Hamilton_CT.nb) The spatial moving averages are calculated as follows: POP_DENSIT.sma &lt;- lag.listw(Hamilton_CT.w, Hamilton_CT$POP_DENSIT) Let’s append the spatial moving averages to our dataframes, in the SpatialPolygonsDataFrame and the tidied version of it both: Hamilton_CT$POP_DENSIT.sma &lt;- POP_DENSIT.sma Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, data.frame(TRACT = Hamilton_CT$TRACT, POP_DENSIT.sma)) ## Joining, by = &quot;TRACT&quot; ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector The spatial moving average can be used in two ways to explore the spatial pattern of an area variable: as a smoother and by means of a scatterplot, combined with the original variable. 23.5.1 Spatial moving average as a smoother First, when mapped, it is essentially a smoothing technique, in that it reduces the amount of variability to make it easier to distinguish the overall pattern. This can be illustrated with the help of a little simulation. To simulate a random spatial variable, we can randomize the observations that we already have, reassigning them at random to areas in the system. This is accomplished as follows: POP_DENSITs1 &lt;- sample(Hamilton_CT$POP_DENSIT) Calculate the spatial moving average for this randomized variable: POP_DENSITs1.sma &lt;- lag.listw(Hamilton_CT.w, POP_DENSITs1) Once that you know how to randomize the variable, simulate a total of eight variables, and calculate their spatial moving averages: POP_DENSITs2 &lt;- sample(Hamilton_CT$POP_DENSIT) POP_DENSITs2.sma &lt;- lag.listw(Hamilton_CT.w, POP_DENSITs2) POP_DENSITs3 &lt;- sample(Hamilton_CT$POP_DENSIT) POP_DENSITs3.sma &lt;- lag.listw(Hamilton_CT.w, POP_DENSITs3) POP_DENSITs4 &lt;- sample(Hamilton_CT$POP_DENSIT) POP_DENSITs4.sma &lt;- lag.listw(Hamilton_CT.w, POP_DENSITs4) POP_DENSITs5 &lt;- sample(Hamilton_CT$POP_DENSIT) POP_DENSITs5.sma &lt;- lag.listw(Hamilton_CT.w, POP_DENSITs5) POP_DENSITs6 &lt;- sample(Hamilton_CT$POP_DENSIT) POP_DENSITs6.sma &lt;- lag.listw(Hamilton_CT.w, POP_DENSITs6) POP_DENSITs7 &lt;- sample(Hamilton_CT$POP_DENSIT) POP_DENSITs7.sma &lt;- lag.listw(Hamilton_CT.w, POP_DENSITs7) POP_DENSITs8 &lt;- sample(Hamilton_CT$POP_DENSIT) POP_DENSITs8.sma &lt;- lag.listw(Hamilton_CT.w, POP_DENSITs8) Now, append the spatial moving averages to the dataframes: Hamilton_CT$POP_DENSITs1 &lt;- POP_DENSITs1 Hamilton_CT$POP_DENSITs1.sma &lt;- POP_DENSITs1.sma Hamilton_CT$POP_DENSITs2 &lt;- POP_DENSITs2 Hamilton_CT$POP_DENSITs2.sma &lt;- POP_DENSITs2.sma Hamilton_CT$POP_DENSITs3 &lt;- POP_DENSITs3 Hamilton_CT$POP_DENSITs3.sma &lt;- POP_DENSITs3.sma Hamilton_CT$POP_DENSITs4 &lt;- POP_DENSITs4 Hamilton_CT$POP_DENSITs4.sma &lt;- POP_DENSITs4.sma Hamilton_CT$POP_DENSITs5 &lt;- POP_DENSITs5 Hamilton_CT$POP_DENSITs5.sma &lt;- POP_DENSITs5.sma Hamilton_CT$POP_DENSITs6 &lt;- POP_DENSITs6 Hamilton_CT$POP_DENSITs6.sma &lt;- POP_DENSITs6.sma Hamilton_CT$POP_DENSITs7 &lt;- POP_DENSITs7 Hamilton_CT$POP_DENSITs7.sma &lt;- POP_DENSITs7.sma Hamilton_CT$POP_DENSITs8 &lt;- POP_DENSITs8 Hamilton_CT$POP_DENSITs8.sma &lt;- POP_DENSITs8.sma Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, data.frame(TRACT = Hamilton_CT$TRACT, POP_DENSITs1, POP_DENSITs1.sma, POP_DENSITs2, POP_DENSITs2.sma, POP_DENSITs3, POP_DENSITs3.sma, POP_DENSITs4, POP_DENSITs4.sma, POP_DENSITs5, POP_DENSITs5.sma, POP_DENSITs6, POP_DENSITs6.sma, POP_DENSITs7, POP_DENSITs7.sma, POP_DENSITs8, POP_DENSITs8.sma)) ## Joining, by = &quot;TRACT&quot; ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Now, create choropleth maps of the empirical variable and the eight simulated variables (don’t worry too much here about the mechanics of creating and plotting the maps). First tidy the data for convenience: Hamilton_CT.t &lt;- Hamilton_CT.t %&gt;% melt(id.vars = c(&quot;long&quot;, &quot;lat&quot;, &quot;order&quot;, &quot;hole&quot;, &quot;piece&quot;, &quot;group&quot;, &quot;TRACT&quot;, &quot;ID&quot;, &quot;POP_DENSIT&quot;, &quot;POP_DENSITs1&quot;, &quot;POP_DENSITs2&quot;, &quot;POP_DENSITs3&quot;, &quot;POP_DENSITs4&quot;, &quot;POP_DENSITs5&quot;, &quot;POP_DENSITs6&quot;, &quot;POP_DENSITs7&quot;, &quot;POP_DENSITs8&quot;)) %&gt;% rename(Map.SMA = variable, SMA = value) And then plot using facet_wrap: ggplot(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = cut_number(SMA, 5))) + geom_polygon() + theme(axis.text.x = element_blank(), axis.text.y = element_blank()) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + coord_equal() + facet_wrap(~ as.numeric(Map.SMA), ncol = 3, nrow = 3) + labs(fill = &quot;Pop Den SMA&quot;) The empirical variable is map number 1 above. Maps 2 through 9 are simulated variables. Would you say the map of the empirical variable is fairly different from the map of the simulated variables? What are the key differences? Perhaps similar insights could be derived from randomizing the original population density variable, instead of the spatial moving average. An additional advantage of the spatial moving average is its use in the development of scatterplots. 23.5.2 Spatial moving average scatterplots Let us explore the use of spatial moving average scatterplots (again, do not worry too much about the mechanics of how create these plots). Reorganize the data for convenience: df_scatterplots &lt;- rbind(data.frame(select(Hamilton_CT@data, Density = POP_DENSIT, SMA = POP_DENSIT.sma), Type = &quot;Empirical&quot;), data.frame(select(Hamilton_CT@data, Density = POP_DENSITs1, SMA = POP_DENSITs1.sma), Type = &quot;Sim 1&quot;), data.frame(select(Hamilton_CT@data, Density = POP_DENSITs2, SMA = POP_DENSITs2.sma), Type = &quot;Sim 2&quot;), data.frame(select(Hamilton_CT@data, Density = POP_DENSITs3, SMA = POP_DENSITs3.sma), Type = &quot;Sim 3&quot;), data.frame(select(Hamilton_CT@data, Density = POP_DENSITs4, SMA = POP_DENSITs4.sma), Type = &quot;Sim 4&quot;), data.frame(select(Hamilton_CT@data, Density = POP_DENSITs5, SMA = POP_DENSITs5.sma), Type = &quot;Sim 5&quot;), data.frame(select(Hamilton_CT@data, Density = POP_DENSITs6, SMA = POP_DENSITs6.sma), Type = &quot;Sim 6&quot;), data.frame(select(Hamilton_CT@data, Density = POP_DENSITs7, SMA = POP_DENSITs7.sma), Type = &quot;Sim 7&quot;), data.frame(select(Hamilton_CT@data, Density = POP_DENSITs8, SMA = POP_DENSITs8.sma), Type = &quot;Sim 8&quot;)) Create the scatterplot of the empirical population density and its spatial moving average, and the scatterplots of the simulated variables and their spatial moving averages for comparison (the plots include the 45 degree line): ggplot(data = df_scatterplots, aes(x = Density, y = SMA, color = Type)) + geom_point() + geom_abline(slope = 1, intercept = 0) + coord_equal() + facet_wrap(~ Type, ncol = 3, nrow = 3) What difference do you see between the empirical and simulated variables in these scatterplots? Fitting a line to the scatterplots (i.e., adding a regression line), makes the difference between the empirical and simulated variables easier to appreciate. This line would take the following form, with \\(\\beta\\) as the slope of the line, and \\(\\alpha\\) the intercept: \\[ \\overline{x_i} =\\alpha + \\beta x_i \\] Plot the scatterplots with fitted lines: ggplot(data = df_scatterplots, aes(x = Density, y = SMA, color = Type)) + geom_point(alpha = 0.1) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;) + coord_equal() + facet_wrap(~ Type, ncol = 3, nrow = 3) You will notice that the slope of the line tends to be flat in the simulated variables; this is to be expected, since these variables are spatially random: the values of the variable at \\(i\\) are independent of the values of their local means!. In other words, the possibility of a non-random spatial pattern is low. The empirical variable, on the other hand, has a slope that is much closer to the 45 degree line. This indicates that the values of the variable at \\(i\\) are not independent of their local means: in other words, \\(x_i\\) is correlated with \\(\\overline{x_i}\\), and the possibility of a non-random pattern is high. This phenomenon is called spatial autocorrelation. 23.6 Spatial autocorrelation and Moran’s I coefficient As seen above, the spatial moving average can provide evidence of the phenomenon of spatial autocorrelation, that is, when a variable displays spatial patterns whereby values at \\(i\\) are not independent of the values of the variable in their neighborhood. A convenient modification to the concept of the spatial moving average is as follows. Instead of using the variable \\(x\\) for the calculation of the spatial moving average, we first center it on the global mean: \\[ z_i = x_i - \\overline{x} \\] In this way, the values of \\(z_i\\) are given in deviations from the mean. By forcing the variable to be centered on the mean, the slope of the fit line is forced to pass through the origin. Calculate the mean-centered version of POP_DENSIT, and then its spatial moving average: df_mean_center_scatterplot &lt;- transmute(Hamilton_CT@data, Density_z = POP_DENSIT - mean(POP_DENSIT), SMA_z = lag.listw(Hamilton_CT.w, Density_z)) Compare the following two plots. You will see that they are identical, but in the mean-centered one the origin of the axes coincides with the means of \\(x\\) and the spatial moving average: sc1 &lt;- ggplot(data = subset(df_scatterplots, Type == &quot;Empirical&quot;), aes(x = Density, y = SMA)) + geom_point(alpha = 0.1) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;) + ggtitle(&quot;Population Density&quot;) + coord_equal() sc2 &lt;- ggplot(data = df_mean_center_scatterplot, aes(x = Density_z, y = SMA_z)) + geom_point(alpha = 0.1) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x-1) + ggtitle(&quot;Mean-Centered Population Density&quot;) + coord_equal() grid.arrange(sc1, sc2, ncol = 1) Notice what happens when the variable \\(z_i\\) multiplies its spatial moving average: \\[ z_i\\overline{z_i} = z_i\\sum_{j=1}^n{w_{ij}^{st}z_j} \\] When \\(z_i\\) is above its mean, it is a positive value and negative otherwise. Likewise, when \\(\\overline{z_i}\\) is above its mean, it is a positive value, and negative otherwise. There are four posibilities with respect to the combinations of (relatively) high and low values. Quadrant 1 (high &amp; high): If \\(z_i\\) is above the mean, it is a relatively high value in the distribution (signed positive). If its neighbors are also relatively high values, the spatial moving average will be above the mean, and also signed positive. Their product will be positive (positive times positive equals positive). Quadrant 2 (low &amp; high): If \\(z_i\\) is below the mean, it is a relatively low value in the distribution (signed negative). If its neighbors in contrast are relatively high values, the spatial moving average will be above the mean, and signed positive. Their product will be negative (negative times positive equals negative). Quadrant 3 (low &amp; low): If \\(z_i\\) is below the mean, it is a relatively low value in the distribution (signed negative). If its neighbors are also relatively low values, the spatial moving average will be below the mean, and also signed negative. Their product will be positive (negative times negative equals positive). Quadrant 4: If \\(z_i\\) is above the mean, it is a relatively high value in the distribution (signed positive). If its neighbors are relatively low values, the spatial moving average will be below the mean, and signed negative. Their product will be negative (positive times negative equals negative). These four quadrants are shown in the following plot: ggplot(data = df_mean_center_scatterplot, aes(x = Density_z, y = SMA_z)) + geom_point(color = &quot;gray&quot;) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) + annotate(&quot;text&quot;, label = &quot;Q1: Positive&quot;, x= 2000, y = 2500) + annotate(&quot;text&quot;, label = &quot;Q4: Negative&quot;, x= 2000, y = -2500) + annotate(&quot;text&quot;, label = &quot;Q2: Negative&quot;, x= -2000, y = 2500) + annotate(&quot;text&quot;, label = &quot;Q3: Positive&quot;, x= -2000, y = -2500) + coord_equal() Lets say that we add all such products: \\[ \\sum_{i=1}^n{z_i\\overline{z_i}} = \\sum_{i=1}^n{z_i\\sum_{j=1}^n{w_{ij}^{st}z_j}} \\] The more such products are positive, meaning more dots in Quadrants 1 and 3 in the scatterplot, the larger (and positive) the total sum will be. Likewise, as more such products are negative, meaning more dots in Quadrants 2 and 4, the larger (but negative!) the total sum will be. Either case would be indicative of a pattern. If the sum is positive, this would suggest that high &amp; high values tend to be together, while low &amp; low values also tend to be together. In contrast, if the sum is negative, this would suggest that high values tend to be surrounded by low values, and viceversa. 23.7 Moran’s I and Moran’s scatterplot Based on the discussion above, lets define the following coefficient, called Moran’s I: \\[ I = \\frac{\\sum_{i=1}^n{z_i\\sum_{j=1}^n{w_{ij}^{st}z_j}}}{\\sum_{i=1}^{n}{z_i^2}} \\] The numerator in this expression is the sum of the products described above. The denominator is the variance of variable \\(x_i\\), and is used here to scale Moran’s I so that it is contained roughly in the interval \\((-1, 1)\\) (the exact bounds depend on the characteristics of the zoning system). Moran’s I is a coefficient of spatial autocorrelation. We can calculate Moran’s I as follows (notice how it is the sum of the products of \\(z\\) by its spatial moving average, divided by the variance): sum(df_mean_center_scatterplot$Density_z * df_mean_center_scatterplot$SMA_z) / sum(df_mean_center_scatterplot$Density_z^2) ## [1] 0.5179736 Since the value is positive, and relatively high, this would suggest a non-random spatial pattern of similar values (i.e., high &amp; high and low &amp; low). Moran’s I is implemented in R in the spdep package, which makes its calculation easy, since you do not have to go manually through the process of calculating the spatial moving averages, etc. The function moran requires as input arguments a variable, a set of spatial weights, the number of zones (\\(n\\)), and the total sum of all weights (termed \\(S_0\\)) - which in the case of row-standardized spatial weights is equal to the number of zones. Therefore: mc &lt;- moran(Hamilton_CT$POP_DENSIT, Hamilton_CT.w, n = 188, S0 = 188) mc$I ## [1] 0.5179736 You can verify that this matches the value you calculated above. The kind of scatterplots that we used above (called Moran’s scatterplots) can also be created easily by means of the moran.plot function: mp &lt;- moran.plot(Hamilton_CT$POP_DENSIT, Hamilton_CT.w) 23.8 Hypothesis testing for spatial autocorrelation As usual, we need some criterion to decide whether the pattern is random. Moran’s I can be used to develop a test of hypothesis. The expected value of Moran’s I under the null hypothesis of spatial independence and its variance have been derived. A test for autocorrelation based on Moran’s I is implemented in the spdep package: moran.test(Hamilton_CT$POP_DENSIT, Hamilton_CT.w) ## ## Moran I test under randomisation ## ## data: Hamilton_CT$POP_DENSIT ## weights: Hamilton_CT.w ## ## Moran I statistic standard deviate = 12.722, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.517973553 -0.005347594 0.001691977 The null hypothesis is of spatial independence. The p-value is interpreted as the probability of making a mistake by rejecting the null hypothesis. In the present case, the p-value is such a small number that we can reject the null hypothesis with a high degree of confidence. Moran’s I and Moran’s scatterplots are amongst the most widely used tools in the analysis of spatial area data. This concludes Practice 11. "],
["area-data-iii-activity.html", "24 Area Data III-Activity 24.1 Practice questions 24.2 Learning objectives 24.3 Suggested reading 24.4 Preliminaries 24.5 Activity", " 24 Area Data III-Activity 24.1 Practice questions Answer the following questions: What does the 45 degree line in the scatterplot of spatial moving averages indicate? What is the effect of centering a variable around the mean? In your own words, describe the phenomenon of spatial autocorrelation. What is the null hypothesis in the test of autocorrelation based on Moran’s I? 24.2 Learning objectives In this activity, you will: Calculate Moran’s I coefficient of autocorrelation for area data. Create Moran’s scatterplots. Examine the results of the tests/scatterplots for further insights. Think about ways to decide whether a landscape is random when working with area data. 24.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 24.4 Preliminaries For this activity you will need the following: This R markdown notebook. A shape file called Hamilton CMA CT. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spatstat, a package designed for the analysis of point patterns (you can learn about spatstat here and here): library(tidyverse) library(rgdal) library(broom) library(spdep) Begin by loading the data that you will use in this activity: Hamilton_CT &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA CT&quot;, integer64 = &quot;allow.loss&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 You can obtain new (calculated) variables as follows. For instance, to obtain the proportion of residents who are between 20 and 34 years old, and between 35 and 49: Hamilton_CT@data &lt;- mutate(Hamilton_CT@data, Prop20to34 = (AGE_20_TO_ + AGE_25_TO_ + AGE_30_TO_)/POPULATION, Prop35to49 = (AGE_35_TO_ + AGE_40_TO_ + AGE_45_TO_)/POPULATION) This is a SpatialPolygonDataFrame. Convert to a dataframe (“tidy” it) for plotting using ggplot2: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- rename(Hamilton_CT.t, TRACT = id) Rejoin the data: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector 24.5 Activity Create a spatial weights matrix for the census tracts in the Hamilton CMA. Use moran.test to test the following variables for autocorrelation: proportion of the population who are 20 to 34 years old, 35 to 49 years old, 50 to 65 years old, and 65 and older. How confident are you deciding whether these variables are not spatially random? What can you say regarding the relative strength of these variables’ spatial patterns? Use moran.plot to create Moran’s scatterplots to complement your tests of spatial autocorrelation. Discuss the patterns that you observe. The scatterplots created using moran.plot include some observations that are labeled with their id and a different symbol. Why do you think these observations are highlighted in such a way? "],
["area-data-iv.html", "25 Area Data IV 25.1 Introduction 25.2 Learning objectives 25.3 Suggested reading 25.4 Preliminaries 25.5 Decomposing Moran’s I 25.6 Local Moran’s I and mapping 25.7 A concentration approach for local analysis of spatial association 25.8 A short note on hypothesis testing 25.9 Detection of hot and cold spots 25.10 Bonus", " 25 Area Data IV 25.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In last previous practice/session, you learned about different ways to define proximity for area data, about spatial weights matrices, and how spatial weights matrices could be used to calculate spatial moving averages. For this practice you will need the following: This R markdown notebook. A shape file called “Hamilton CMA CT” A dataset called Data6.RData The shape file includes the spatial information for the census tracts in the Hamilton Census Metropolitan Area (as polygons), and a host of demographic variables from the census of Canada, including population and languages. Two dataframes in Data6.RData are simulated landscapes, one random, one with a strong systematic pattern. 25.2 Learning objectives In this practice, you will learn about: Decomposing Moran’s I. Local Moran’s I and mapping. A concentration approach for local analysis of spatial association. A short note on hypothesis testing. Detection of hot and cold spots. 25.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 25.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(rgdal) library(broom) library(plotly) library(spdep) library(reshape2) #library(gridExtra) Load the datasets, first the .RData and then the shape file. load(file = &quot;Data6.RData&quot;) This file contains two dataframes with measurements of a variable on a landscape: summary(df1) ## x y z ## Min. : 1.00 Min. : 1.00 Min. :24.40 ## 1st Qu.:27.00 1st Qu.:19.00 1st Qu.:27.89 ## Median :46.50 Median :33.00 Median :30.33 ## Mean :45.61 Mean :31.63 Mean :34.38 ## 3rd Qu.:66.00 3rd Qu.:45.00 3rd Qu.:38.25 ## Max. :87.00 Max. :61.00 Max. :69.59 summary(df2) ## x y z ## Min. : 1.00 Min. : 1.00 Min. :24.40 ## 1st Qu.:27.00 1st Qu.:19.00 1st Qu.:27.89 ## Median :46.50 Median :33.00 Median :30.33 ## Mean :45.61 Mean :31.63 Mean :34.38 ## 3rd Qu.:66.00 3rd Qu.:45.00 3rd Qu.:38.25 ## Max. :87.00 Max. :61.00 Max. :69.59 Note that the descriptive statistics of both variables are identical. Next, read the shape file as an object of class SpatialPolygonDataFrame. The function used to read Esri shape files is rgdal::readOGR. Setting integer64 to “allow.loss” keeps the data as integers as opposed to changing to factors or strings: Hamilton_CT &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA CT&quot;, integer64 = &quot;allow.loss&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 Clear the dataframe, retain only ID, TRACT, and POP_DENSIT: Hamilton_CT@data &lt;- transmute(Hamilton_CT@data, AreaID = 1:188, TRACT = TRACT, POP_DENSIT = POP_DENSIT) To use the plotting functions of ggplot2, the SpatialPolygonDataFrame needs to be “tidied” by means of the tidy function of the broom package: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- dplyr::rename(Hamilton_CT.t, TRACT = id) Tidying the spatial dataframe strips it from the non-spatial information, but we can add all the data by means of the left_join function: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Now the tidy dataframe Hamilton_DA.t contains the spatial information and the data. You can quickly verify the contents of the dataframe by means of summary: summary(Hamilton_CT.t) ## long lat order hole ## Min. :-80.25 Min. :43.05 Min. : 1 Mode :logical ## 1st Qu.:-79.93 1st Qu.:43.21 1st Qu.: 7304 FALSE:29212 ## Median :-79.86 Median :43.24 Median :14606 ## Mean :-79.86 Mean :43.25 Mean :14606 ## 3rd Qu.:-79.80 3rd Qu.:43.28 3rd Qu.:21909 ## Max. :-79.51 Max. :43.48 Max. :29212 ## ## piece group TRACT AreaID ## 1:29212 5370124.00.1: 822 Length:29212 Min. : 1.00 ## 5370121.00.1: 661 Class :character 1st Qu.: 18.00 ## 5370142.01.1: 642 Mode :character Median : 78.00 ## 5370100.00.1: 615 Mean : 79.82 ## 5370101.02.1: 602 3rd Qu.:128.00 ## 5370142.02.1: 595 Max. :188.00 ## (Other) :25275 ## POP_DENSIT ## Min. : 2.591 ## 1st Qu.: 254.658 ## Median : 1511.957 ## Mean : 1890.627 ## 3rd Qu.: 2807.857 ## Max. :14234.286 ## 25.5 Decomposing Moran’s I Recall from the previous practice/session that Moran’s I coefficient of spatial autocorrelation was derived based on the idea of aggregating the products of a (mean-centered) variable by its spatial moving average, and then dividing by the variance: \\[ I = \\frac{\\sum_{i=1}^n{z_i\\sum_{j=1}^n{w_{ij}^{st}z_j}}}{\\sum_{i=1}^{n}{z_i^2}} \\] Also, you will have seen that when plotting Moran’s scatterplot some observations are highlighted. This is because said observations make a particularly large contribution to I. It turns out that those contributions are informative in and of themselves, and their analysis can provide more focused information about the spatial pattern. Consider again the variable POP_DENSIT, population density in the Hamilton CMA. Lets create spatial weights for the census tracts in this system: Hamilton_CT.w &lt;- nb2listw(poly2nb(pl = Hamilton_CT)) Although Moran’s scatterplot can be obtained easily with the functionmoran.plot, here we will create the scatterplot manually to have better control of its aspect. First, create a dataframe with the mean-centered variable and scaled variable \\(z_i=(x_i-\\overline{x})/\\sum z_i^2\\), and its spatial moving average. Notice that this includes as well a factor variable Type to identify the type of spatial relationship (Low &amp; Low, if both \\(z_i\\) and its spatial moving average are negative, High &amp; High, if both \\(z_i\\) and its spatial moving average are positive, and Low &amp; High/High &amp; Low otherwise). This is information is useful for mapping the results: df_msc &lt;- transmute(Hamilton_CT@data, AreaID = 1:188, TRACT = TRACT, Z = (POP_DENSIT-mean(POP_DENSIT)) / var(POP_DENSIT), SMA = lag.listw(Hamilton_CT.w, Z), Type = factor(ifelse(Z &lt; 0 &amp; SMA &lt; 0, &quot;LL&quot;, ifelse(Z &gt; 0 &amp; SMA &gt; 0, &quot;HH&quot;, &quot;HL/LH&quot;)))) Next, create the scatterplot and a choropleth map of the population density (the package plotly is used to create interactive plots): msc &lt;- ggplot(data = df_msc, aes(x = Z, y = SMA, size = Z * SMA, color = Z * SMA, AreaID = AreaID)) + geom_point() + scale_color_distiller(palette = &quot;YlOrRd&quot;, trans = &quot;reverse&quot;, guide = FALSE) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) + coord_equal() p1 &lt;- ggplotly(msc, tooltip = c(&quot;size&quot;, &quot;AreaID&quot;)) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` map &lt;- ggplot(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, fill = round(POP_DENSIT), AreaID = AreaID)) + geom_polygon() + scale_fill_distiller(palette = &quot;YlOrRd&quot;, trans = &quot;reverse&quot;, guide = FALSE) + coord_equal() p2 &lt;- ggplotly(map, showlegend = FALSE) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` subplot(p1, p2, nrows = 1, margin = 0.05, titleX = TRUE, titleY = TRUE) The darker colors/larger dots in the scatterplot are the zones with the largest contributions to Moran’s I. The darker colors in the choropleth map are higher population densities. Can you identify in the map the zones that most contribute to Moran’s I? The direct relationship between the dots in the scatterplot and the values of the variable in the map suggest the following decomposition of Moran’s I. 25.6 Local Moran’s I and mapping A possible decomposition of Moran’s I into local components is as follows (see Anselin, 1995): \\[ I_i = \\frac{z_i}{m_2}\\sum_{j=1}^n{w_{ij}^{st}z_j} \\] where \\(z_i\\) is a mean-centered variable, and: \\[ m_2 = \\sum_{i=1}^n{z_i^2} \\] is its variance. It is straightforward to see that: \\[ I = \\sum_{i=1}^n{I_i} \\] The above shows how the local Moran’s I coefficients can be aggregated to the global coefficient. An advantage of the local decomposition described here is that it allows the analyst to map the statistic to better understand the spatial pattern. The local version of Moran’s I is implemented in spdep as localmoran, and can be called with a variable and a set of spatial weights as arguments: POP_DENSIT.lm &lt;- localmoran(Hamilton_CT$POP_DENSIT, Hamilton_CT.w) The value (output) of the function is a matrix with local Moran’s I coefficients, and their corresponding expected values and variances (used for hypothesis testing; more on this next). You can check the summary to verify the contents: summary(POP_DENSIT.lm) ## Ii E.Ii Var.Ii ## Min. :-0.62144 Min. :-0.005348 Min. :0.06421 ## 1st Qu.: 0.00478 1st Qu.:-0.005348 1st Qu.:0.13340 ## Median : 0.12523 Median :-0.005348 Median :0.15647 ## Mean : 0.51797 Mean :-0.005348 Mean :0.16681 ## 3rd Qu.: 0.59384 3rd Qu.:-0.005348 3rd Qu.:0.18876 ## Max. : 8.30454 Max. :-0.005348 Max. :0.47938 ## Z.Ii Pr(z &gt; 0) ## Min. :-1.67885 Min. :0.00000 ## 1st Qu.: 0.02345 1st Qu.:0.07352 ## Median : 0.33935 Median :0.36718 ## Mean : 1.32117 Mean :0.31317 ## 3rd Qu.: 1.45104 3rd Qu.:0.49065 ## Max. :19.12671 Max. :0.95341 Similar to the global version of Moran’s I, hypothesis testing can be conducted by comparing the empirical statistic to its distribution under the null hypothesis of spatial independence. The function localmoran reports p-values to this end. For further exploration, append the local statistics to your dataframe: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, data.frame(TRACT = Hamilton_CT$TRACT, POP_DENSIT.lm)) ## Joining, by = &quot;TRACT&quot; ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Hamilton_CT.t &lt;- rename(Hamilton_CT.t, p.val = Pr.z...0.) For mapping it is also convenient to append the values that were used to create the scatterplot, and we are particularly interested in the factor for the type of relationship (i.e., HH, LL, HL/LH): Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, df_msc) ## Joining, by = c(&quot;TRACT&quot;, &quot;AreaID&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Now it is possible to map the local statistics: map &lt;- ggplot(data = Hamilton_CT.t, aes(x = long, y = lat, group = group, p.val = p.val, POP_DENSIT = round(POP_DENSIT))) + geom_polygon(aes(fill = Type, color = p.val &lt; 0.05)) + scale_fill_brewer(palette = &quot;RdBu&quot;) + scale_color_manual(values = c(NA, &quot;Black&quot;) ) + labs(color = &quot;Prob &lt; 0.05&quot;) + coord_equal() + theme(legend.title = element_blank()) ggplotly(map, tooltip = c(&quot;p.val&quot;, &quot;POP_DENSIT&quot;)) #%&gt;% layout(legend = list(x = 0.9, y = 0.0)) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` The map above shows whether population density in a zone is high, surrounded by other zones with high population densities (HH), or low, surrounded by zones that also have low population density (LL). Other zones have either low population densities and are surrounded by zones with high population density, or viceversa (HL/LH). A black border around the zone indicates that the local statistic is significant at p&lt;0.05 or better. This map allows you to identify what we could call the downtown core (from the perspective of population density), and the most suburban-rural census tracts in the Hamilton CMA. While mapping \\(I_i\\) or their corresponding p-values is straightforward, I personally find it more useful to map whether the zones are of type HH, LH, or HL/LH. Since such maps are not (to the best of my knowledge) the output of an existing function in an R package, we’ll create one here. localmoran.map &lt;- function(spat_pol = spat_pol, listw = listw, VAR = VAR, ID = ID){ require(tidyverse) require(broom) require(spdep) require(plotly) spat_pol@data &lt;- data.frame(ID = ID, VAR = VAR) spat_pol.t &lt;- broom::tidy(spat_pol, region = &quot;ID&quot;) spat_pol.t &lt;- dplyr::rename(spat_pol.t, ID = id) spat_pol.t &lt;- dplyr::left_join(spat_pol.t, spat_pol@data, by = &quot;ID&quot;) df_msc &lt;- transmute(spat_pol@data, ID = ID, Z = (VAR-mean(VAR)) / var(VAR), SMA = lag.listw(listw, Z), Type = factor(ifelse(Z &lt; 0 &amp; SMA &lt; 0, &quot;LL&quot;, ifelse(Z &gt; 0 &amp; SMA &gt; 0, &quot;HH&quot;, &quot;HL/LH&quot;)))) local_I &lt;- localmoran(spat_pol$VAR, listw) spat_pol.t &lt;- left_join(spat_pol.t, data.frame(ID = spat_pol$ID, local_I)) spat_pol.t &lt;- rename(spat_pol.t, p.val = Pr.z...0.) spat_pol.t &lt;- left_join(spat_pol.t, df_msc) map &lt;- ggplot(data = spat_pol.t, aes(x = long, y = lat, group = group, p.val = p.val, VAR = round(VAR))) + geom_polygon(aes(fill = Type, color = p.val &lt; 0.05)) + scale_fill_brewer(palette = &quot;RdBu&quot;) + scale_color_manual(values = c(NA, &quot;Black&quot;) ) + labs(color = &quot;Prob &lt; 0.05&quot;) + coord_equal() + theme(legend.title = element_blank()) ggplotly(map, tooltip = c(&quot;p.val&quot;, &quot;VAR&quot;)) } Notice how this function simply replicates the steps that we followed earlier to create the map with the results of the local Moran’s Is. To use this function you need as inputs a SpatialPolygonDataFrame, a listw object with spatial weights, and to define the variable of interest and a unique identifier for the areas (such as their tract identifiers). For example: localmoran.map(Hamilton_CT, Hamilton_CT.w, Hamilton_CT$POP_DENSIT, Hamilton_CT$TRACT) ## Warning: Column `ID` joining character vector and factor, coercing into ## character vector ## Joining, by = &quot;ID&quot; ## Warning: Column `ID` joining character vector and factor, coercing into ## character vector ## Joining, by = &quot;ID&quot; ## Warning: Column `ID` joining character vector and factor, coercing into ## character vector ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` There, the function creates the map as desired. 25.6.1 A quick note on functions Once that you know the steps needed to complete a task, if the task needs to be repeated many times possibly using different inputs, a function is a way of packing those instructions in a convenient way. That is all. 25.7 A concentration approach for local analysis of spatial association The local version of Moran’s I is one of the most widely used tools of a family of measures called Local Statistics of Spatial Association or LISA. It is not the only one, however. In this section, we will see an alternative way of exploring spatial patterns locally, by means of a concentration approach. Imagine a landscape with a variable that can be measured in a ratio scale with a true zero point (say, population, income, a contaminant, or property values, variables that do not take negative values and the value of zero indicates complete absence). Imagine that you stand at a given location on that landscape and survey your surroundings. If your surroundings look very similar to you (i.e., if their elevation is similar, relative to the rest of the landscape), you would take that as evidence of a spatial pattern, at least locally. This is the idea behind spatial autocorrelation analysis. As an alternative, imagine for instance that the variable of interest is, say, personal income. You might ask “how much of the regional wealth can be found in my neighborhood?” (or, if you prefer, imagine that the variable is a contaminant, and your question would be, how much of it is around here?) Imagine now that personal income is spatially random. What would you expect the share of the wealth to be in your neighborhood? Would that share change if you moved to any other location? Lets elaborate this thought experiment. Take the df1 dataframe. The total sum of this variable in the region is 12,034.34. See: sum(df1$z) ## [1] 12034.34 The following is an interactive plot of variable z in the sample dataframe df1. This variable is spatially random: plot_ly(df1, x = ~x, y = ~y, z = ~z, marker = list(color = ~z, colorscale = c(&#39;#FFE1A1&#39;, &#39;#683531&#39;), showscale = TRUE)) %&gt;% add_markers() Imagine that you stand at coordinates x = 53 and y = 34 (lets call this the focal point), and you survey the landscape within a radius r of 10 (units of distance) of this location. How much wealth is concentrated in the neighborhood of the focal point? Lets see: xy0 &lt;- c(53, 34) r &lt;- 10 df1_xy0 &lt;- subset(df1, sqrt((x - xy0[1])^2 + (y - xy0[2])^2) &lt; r) sum(df1_xy0$z) ## [1] 832.0156 Recall that the total of the variable for the region is 12,034.34. If you change the radius r to a very large number, the concentration of the variable will simply become the total sum for the region. Essentially, the whole region is the “neighborhood” of the focal point. Try it. Now, for a fixed radius, change the focal point, and see how much the concentration of the variable changes for its neighborhood. How does the concentration of the variable by focal point? Lets repeat the thought experiment but now with the landscape shown in the following figure: plot_ly(df2, x = ~x, y = ~y, z = ~z, marker = list(color = ~z, colorscale = c(&#39;#FFE1A1&#39;, &#39;#683531&#39;), showscale = TRUE)) %&gt;% add_markers() Imagine that you stand at the focal point with coordinates x = 53 and y = 34. Can you identify the point in the plot? If you surveyed the neighborhood, what would be the concentration of wealth there? How would that change as you visited different focal points? Lets see (again, recall that the total of the variable for the whole region is 12,034.34): xy0 &lt;- c(71, 10) r &lt;- 10 df2_xy0 &lt;- subset(df2, sqrt((x - xy0[1])^2 + (y - xy0[2])^2) &lt; r) sum(df2_xy0$z) ## [1] 542.8224 Change the focal point. How does the concentration of the variable change? Lets define the following measure of local concentration (see Getis and Ord, 1992): \\[ G_i*(d)=\\frac{\\sum_{j=1}^n{w_{ij}x_j}}{\\sum_{i=i}^{n}x_{i}} \\] Notice that the spatial weights are not row-standardized, and in fact must be a binary variable as follows: \\[ w_{ij}=\\bigg\\{\\begin{array}{l l} 1\\text{ if } d_{ij}\\leq d\\\\ 0\\text{ otherwise}\\\\ \\end{array} \\] This is because in this measure of concentration, we do not calculate the spatial moving average for the neighborhood, but the total of the variable in the neighborhood. A variant of this statistic removes from the sum the value of the variable at i: \\[ G_i^(d)=\\frac{\\sum_{j\\neq i}^n{w_{ij}x_j}}{\\sum_{i=i}^{n}x_{i}} \\] I do not find this definition to be particularly useful. I suspect it was defined to resemble Moran’s I where an area is not it’s own neighbor - which makes sense in an autocorrelation sense (an area is perfectly autocorrelated with itself). In a concentration approach, not using the value at \\(i\\) is less appealing. As with the local version of Moran’s I, it is possible to map the statistic to better understand the spatial pattern. The \\(G_i(d)\\) statistic is implemented in spdep as localG, and can be called with a variable and a set of spatial weights as arguments. Lets calculate this statistic for the two datasets in the example above. This requires that we create binary spatial weights. Begin by creating neighbors by distance: xy_coord &lt;- cbind(df1$x, df1$y) dn10 &lt;- dnearneigh(xy_coord, 0, 10) Two differences with the procedure that you used before to create spatial weights is that we wish to include the observation at \\(i\\) as well (so include.self(), and the style of the matrix is “B” (for binary): wb10 &lt;- nb2listw(include.self(dn10), style = &quot;B&quot;) The local statistics can be obtained as follows: df1.lg &lt;- localG(df1$z, wb10) The value (output) of the function is a ’vector localG object with normalized local statistics. Normalized means that the mean under the null hypothesis has been substracted and the result has been divided by the variance under the null. Normalized statistics can be compared to the standard normal distribution for hypothesis testing. You can check the summary to verify the contents: summary(df1.lg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.6345 -0.5085 0.1401 0.0657 0.5911 2.6638 Lets add p-values to this: df1.lg &lt;- as.numeric(df1.lg) df1.lg &lt;- data.frame(Gstar = df1.lg, p.val = 2 * pnorm(abs(df1.lg), lower.tail = FALSE)) How many of the p-values are less than the conventional decision cutoff of 0.05? Now the second example: df2.lg &lt;- localG(df2$z, wb10) summary(df2.lg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -4.2400 -2.6791 -1.3999 0.1503 2.3938 12.2401 Adding p-values: df2.lg &lt;- as.numeric(df2.lg) df2.lg &lt;- data.frame(Gstar = df2.lg, p.val = 2 * pnorm(abs(df2.lg), lower.tail = FALSE)) If we append the results of the analysis to the dataframe, we can plot the results for further exploration. We will classify the results by their type, in this case high and low concentrations: df2 &lt;- cbind(df2[,1:3],df2.lg) df2 &lt;- mutate(df2, Type = factor(ifelse(Gstar &lt; 0 &amp; p.val &lt;= 0.05, &quot;Low Concentration&quot;, ifelse(Gstar &gt; 0 &amp; p.val &lt;= 0.05, &quot;High Concentration&quot;, &quot;Not Signicant&quot;)))) And then the plot: plot_ly(df2, x = ~x, y = ~y, z = ~z, color = ~Type, colors = c(&quot;red&quot;, &quot;blue&quot;, &quot;gray&quot;), marker = list()) %&gt;% add_markers() What kind of pattern do you observe? 25.8 A short note on hypothesis testing Local tests as introduced above are affected by an issue called multiple testing. Typically, when attempting to assess the significance of a statistic, a level of significance is adopted (conventionally 0.05, for instance). When working with local statistics, we typically conduct many tests of hypothesis simultaneously (in the example above, one for each observation). A risk when conducting a large number of tests is that some of them might appear significant purely by chance! The more tests we conduct, the more likely that at least a few of them will be significant by chance. For instance, in the preceding example the variable in df1 was spatially random, and yet a few observations had p-values smaller than 0.05. What this suggests is that some correction to the level of significance used is needed. A crude rule to make this adjustment in called Bonferroni correction. This correction is as follows: \\[ \\alpha_B = \\frac{\\alpha_{nominal}}{m} \\] where \\(\\alpha_{nominal}\\) is the nominal level of significance, \\(\\alpha_B\\) is the adjusted level of significance, and \\(m\\) is the number of simultaneous tests. This correction requires that each test be evaluated at a lower level of significance \\(\\alpha_B\\) in order to to achieve a nominal level of significance of 0.05. If we apply this correction to the analysis above, we see that instead of 0.05, the p-value needed for significance is much lower: alpha_B &lt;- 0.05/nrow(df1) alpha_B ## [1] 0.0001428571 You can verify now that no observations in df1 show up as significant: sum(df1.lg$p.val &lt;= alpha_B) ## [1] 0 If we examine the variable in df2: df2 &lt;- mutate(df2, Type = factor(ifelse(Gstar &lt; 0 &amp; p.val &lt;= alpha_B, &quot;Low Concentration&quot;, ifelse(Gstar &gt; 0 &amp; p.val &lt;= alpha_B, &quot;High Concentration&quot;, &quot;Not Signicant&quot;)))) plot_ly(df2, x = ~x, y = ~y, z = ~z, color = ~Type, colors = c(&quot;red&quot;, &quot;blue&quot;, &quot;gray&quot;), marker = list()) %&gt;% add_markers() You will see that fewer observations are significant, but it is still possible to detect two regions of high concentration, and two of low concentration. Bonferroni correction is known to be overly strict, and sharper approaches exist to correct for multiple testing. Between the nominal level (no correction) and Bonferroni correction, it is still possible to assess the gravity of the issue of multiple comparisons. Observations that are flagged as significant with the Bonferroni correction, will also be significant under more refined corrections, so it provides a most conservative decision rule. 25.9 Detection of hot and cold spots As the examples above illustrate, local statistics can be very useful in detecting what might be termed “hot” and “cold” spots. A hot spot is a group of observations that are significantly high, whereas a cold spot is a group of observations that are significantly low. There are many different applications where hot/cold spot detection is important. For instance, in many studies of urban form, it is important to identify centers and subcenters - by population, by property values, by incidence of trips, and so on. In spatial criminology, detecting hot spots of crime can help with prevention and law enforcement efforts. In environmental studies, remediation efforts can be greatly assisted by identification of hot areas. And so on. 25.10 Bonus Check a cool app that illustrates the \\(G_i^*\\) statistic here This concludes Practice 12. "],
["area-data-iv-activity.html", "26 Area Data IV-Activity 26.1 Practice questions 26.2 Learning objectives 26.3 Suggested reading 26.4 Preliminaries 26.5 Activity", " 26 Area Data IV-Activity 26.1 Practice questions Answer the following questions: How are row-standardized and binary spatial weights interpreted? What is the reason for using a Bonferroni correction for multiple tests? What types of spatial patterns can the local version of Moran’s I detect? What types of spatial patterns can the \\(G_i(d)\\) statistic detect? What is the utility of detecting hot and cold spatial spots? 26.2 Learning objectives In this activity, you will: Calculate Moran’s I coefficient of autocorrelation for area data. Create Moran’s scatterplots. Examine the results of the tests/scatterplots for further insights. Think about ways to decide whether a landscape is random when working with area data. 26.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 26.4 Preliminaries For this activity you will need the following: This R markdown notebook. A shape file called Hamilton CMA CT. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spdep, a package designed for the analysis of spatial data (you can learn about spdep here and here): library(tidyverse) library(rgdal) library(broom) library(spdep) Begin by loading the data that you will use in this activity: Hamilton_CT &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA CT&quot;, integer64 = &quot;allow.loss&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 You can obtain new (calculated) variables as follows. For instance, to obtain the proportion of residents who are between 20 and 34 years old, and between 35 and 49: Hamilton_CT@data &lt;- mutate(Hamilton_CT@data, Prop20to34 = (AGE_20_TO_ + AGE_25_TO_ + AGE_30_TO_)/POPULATION, Prop35to49 = (AGE_35_TO_ + AGE_40_TO_ + AGE_45_TO_)/POPULATION) This is a SpatialPolygonDataFrame. Convert to a dataframe (“tidy” it) for plotting using ggplot2: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- rename(Hamilton_CT.t, TRACT = id) Rejoin the data: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector This function is used to create local Moran maps: localmoran.map &lt;- function(spat_pol = spat_pol, listw = listw, VAR = VAR, ID = ID){ require(tidyverse) require(broom) require(spdep) require(plotly) spat_pol@data &lt;- data.frame(ID = ID, VAR = VAR) spat_pol.t &lt;- broom::tidy(spat_pol, region = &quot;ID&quot;) spat_pol.t &lt;- dplyr::rename(spat_pol.t, ID = id) spat_pol.t &lt;- dplyr::left_join(spat_pol.t, spat_pol@data, by = &quot;ID&quot;) df_msc &lt;- transmute(spat_pol@data, ID = ID, Z = (VAR-mean(VAR)) / var(VAR), SMA = lag.listw(listw, Z), Type = factor(ifelse(Z &lt; 0 &amp; SMA &lt; 0, &quot;LL&quot;, ifelse(Z &gt; 0 &amp; SMA &gt; 0, &quot;HH&quot;, &quot;HL/LH&quot;)))) local_I &lt;- localmoran(spat_pol$VAR, listw) spat_pol.t &lt;- left_join(spat_pol.t, data.frame(ID = spat_pol$ID, local_I)) spat_pol.t &lt;- rename(spat_pol.t, p.val = Pr.z...0.) spat_pol.t &lt;- left_join(spat_pol.t, df_msc) map &lt;- ggplot(data = spat_pol.t, aes(x = long, y = lat, group = group, p.val = p.val, VAR = VAR)) + geom_polygon(aes(fill = Type, color = p.val &lt; 0.05)) + scale_fill_brewer(palette = &quot;RdBu&quot;) + scale_color_manual(values = c(NA, &quot;Black&quot;) ) + labs(color = &quot;Prob &lt; 0.05&quot;) + coord_equal() + theme(legend.title = element_blank()) ggplotly(map, tooltip = c(&quot;p.val&quot;, &quot;VAR&quot;)) } This function is used to create \\(G_i^*\\) maps: gistar.map &lt;- function(spat_pol = spat_pol, listw = listw, VAR = VAR, ID = ID){ require(tidyverse) require(broom) require(spdep) require(plotly) spat_pol@data &lt;- data.frame(ID = ID, VAR = VAR) spat_pol.t &lt;- broom::tidy(spat_pol, region = &quot;ID&quot;) spat_pol.t &lt;- dplyr::rename(spat_pol.t, ID = id) spat_pol.t &lt;- dplyr::left_join(spat_pol.t, spat_pol@data, by = &quot;ID&quot;) df.lg &lt;- localG(VAR, listw) df.lg &lt;- as.numeric(df.lg) df.lg &lt;- data.frame(Gstar = df.lg, p.val = 2 * pnorm(abs(df.lg), lower.tail = FALSE)) df.lg &lt;- mutate(df.lg, Type = factor(ifelse(Gstar &lt; 0 &amp; p.val &lt;= 0.05, &quot;Low Concentration&quot;, ifelse(Gstar &gt; 0 &amp; p.val &lt;= 0.05, &quot;High Concentration&quot;, &quot;Not Signicant&quot;)))) spat_pol.t &lt;- left_join(spat_pol.t, data.frame(ID = spat_pol$ID, df.lg)) map &lt;- ggplot(data = spat_pol.t, aes(x = long, y = lat, group = group, p.val = p.val, VAR = VAR)) + geom_polygon(aes(fill = Type, color = p.val &lt; 0.05)) + scale_fill_brewer(palette = &quot;RdBu&quot;) + scale_color_manual(values = c(NA, &quot;Black&quot;) ) + labs(color = &quot;Prob &lt; 0.05&quot;) + coord_equal() + theme(legend.title = element_blank()) ggplotly(map, tooltip = c(&quot;p.val&quot;, &quot;VAR&quot;)) } Create spatial weights. By contiguity: Hamilton_CT.w &lt;- nb2listw(poly2nb(pl = Hamilton_CT)) Binary, by distance (3 km threshold) including self. Hamilton_CT.3knb &lt;- Hamilton_CT %&gt;% coordinates() %&gt;% dnearneigh(d1 = 0, d2 = 3, longlat = TRUE) Hamilton_CT.3kw &lt;- nb2listw(include.self(Hamilton_CT.3knb), style = &quot;B&quot;) 26.5 Activity Create local Moran maps for the population and proportion of population in the age group 20-34. What is the difference between using population (absolute) and proportion of population (rate)? Is there a reason to prefer either variable in analysis? Discuss. Use the \\(G_i^*\\) statitic to analyze the population and proportion of population in the age group 20-34. What is the difference between using population (absolute) and proportion of population (rate)? Is there a reason to prefer either variable in analysis? Discuss. Now create local Moran maps for the population and population density in the age group 20-34. What is the difference between using population (absolute) and population density (rate)? More generally, what do you think should guide the decision of whether to analyze variables as absolute values or rates? "],
["area-data-v.html", "27 Area Data V 27.1 Introduction 27.2 Learning objectives 27.3 Suggested reading 27.4 Preliminaries 27.5 Regression analysis in R 27.6 Autocorrelation as a model diagnostic 27.7 Variable transformations 27.8 A note about spatial autocorrelation in regression analysis", " 27 Area Data V 27.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In previous practice/session, you learned about how to use local spatial statistics for exploratory spatial data analysis. For this practice you will need the following: This R markdown notebook. A shape file called “Hamilton CMA CT” The shape file includes the spatial information for the census tracts in the Hamilton Census Metropolitan Area (as polygons), and a host of demographic variables from the census of Canada, including population and languages. 27.2 Learning objectives In this practice, you will learn: How to estimate regression models in R. About autocorrelation as a model diagnostic. About variable transformations. How to use autocorrelation analysis to improve regression models. 27.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 27.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(rgdal) library(broom) library(geosphere) ## ## Attaching package: &#39;geosphere&#39; ## The following object is masked from &#39;package:spatstat&#39;: ## ## perimeter library(ggmap) library(gmapsdistance) library(spdep) library(reshape2) library(plotly) #library(gridExtra) Next, read the shape file as an object of class SpatialPolygonDataFrame. The function used to read Esri shape files is rgdal::readOGR. Setting integer64 to “allow.loss” keeps the data as integers as opposed to changing to factors or strings: Hamilton_CT &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA CT&quot;, integer64 = &quot;allow.loss&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA CT&quot; ## with 188 features ## It has 255 fields ## Integer64 fields read as signed 32-bit integers: ID POPULATION PRIVATE_DW OCCUPIED_D ALL_AGES AGE_4 AGE_5_TO_9 AGE_10_TO_ AGE_15_TO_ AGE_15 AGE_16 AGE_17 AGE_18 AGE_19 AGE_20_TO_ AGE_25_TO_ AGE_30_TO_ AGE_35_TO_ AGE_40_TO_ AGE_45_TO_ AGE_50_TO_ AGE_55_TO_ AGE_60_TO_ AGE_65_TO_ AGE_70_TO_ AGE_75_TO_ AGE_80_TO_ AGE_85 MEDIAN_AGE MALE_ALL_A MALE_4 MALE_5_TO_ MALE_10_TO MALE_15_TO MALE_15 MALE_16 MALE_17 MALE_18 MALE_19 MALE_20_TO MALE_25_TO MALE_30_TO MALE_35_TO MALE_40_TO MALE_45_TO MALE_50_TO MALE_55_TO MALE_60_TO MALE_65_TO MALE_70_TO MALE_75_TO MALE_80_TO MALE_85 MALE_MEDIA FEMALE_ALL FEMALE_4 FEMALE_5_T FEMALE_10_ FEMALE_15_ FEMALE_15 FEMALE_16 FEMALE_17 FEMALE_18 FEMALE_19 FEMALE_20_ FEMALE_25_ FEMALE_30_ FEMALE_35_ FEMALE_40_ FEMALE_45_ FEMALE_50_ FEMALE_55_ FEMALE_60_ FEMALE_65_ FEMALE_70_ FEMALE_75_ FEMALE_80_ FEMALE_85 FEMALE_MED MARRIED_AG MARRIED_OR MARRIED COMMON_LAW UNMARRIED SINGLE SEPARATED DIVORCED WIDOWED MARRIED_A1 MARRIED_O1 MARRIED_M COMMON_LA1 UNMARRIED_ SINGLE_M SEPARATED_ DIVORCED_M WIDOWED_M MARRIED_A2 MARRIED_O2 MARRIED_F COMMON_LA2 UNMARRIED1 SINGLE_F SEPARATED1 DIVORCED_F WIDOWED_F FAMILIES_I FAMILY_SIZ FAMILY_SI1 FAMILY_SI2 FAMILY_SI3 COUPLE_FAM COUPLE_MAR COUPLE_MA1 COUPLE_MA2 COUPLE_MA3 COUPLE_MA4 COUPLE_MA5 COUPLE_COM COUPLE_CO1 COUPLE_CO2 COUPLE_CO3 COUPLE_CO4 COUPLE_CO5 SINGLE_PAR SINGLE_PA1 SINGLE_PA2 SINGLE_PA3 SINGLE_PA4 SINGLE_PA5 SINGLE_PA6 SINGLE_PA7 SINGLE_PA8 CHILDREN_F CHILDREN_1 CHILDREN_2 CHILDREN_3 CHILDREN_4 CHILDREN_5 POPULATIO1 POPULATIO2 POPULATIO3 POPULATIO4 POPULATIO5 POPULATIO6 POPULATIO7 POPULATIO8 POPULATIO9 POPULATI10 POPULATI11 POPULATI12 PRIVATE_HO PRIVATE_HH PRIVATE_H1 PRIVATE_H2 PRIVATE_H3 PRIVATE_H4 PRIVATE_H5 PRIVATE_H6 PRIVATE_H7 PRIVATE_H8 PRIVATE_H9 PRIVATE_10 PRIVATE_11 PRIVATE_12 PRIVATE_13 PRIVATE_14 PRIVATE_15 OCC_PRIVAT OCC_PRIVA1 OCC_PRIVA2 OCC_PRIVA3 OCC_PRIVA4 OCC_PRIVA5 OCC_PRIVA6 OCC_PRIVA7 OCC_PRIVA8 OCC_PRIVA9 PRIVATE_16 PRIVATE_17 PRIVATE_18 PRIVATE_19 PRIVATE_20 PRIVATE_21 PRIVATE_22 PRIVATE_23 NATIVE_LAN NATIVE_LA1 NATIVE_LA2 NATIVE_LA3 NATIVE_LA4 NATIVE_LA5 NATIVE_LA6 NATIVE_LA7 NATIVE_LA8 NATIVE_LA9 NATIVE_L10 NATIVE_L11 NATIVE_L12 NATIVE_L13 NATIVE_L14 NATIVE_L15 NATIVE_L16 NATIVE_L17 NATIVE_L18 NATIVE_L19 NATIVE_L20 NATIVE_L21 NATIVE_L22 NATIVE_L23 NATIVE_L24 NATIVE_L25 NATIVE_L26 NATIVE_L27 NATIVE_L28 NATIVE_L29 NATIVE_L30 NATIVE_L31 NATIVE_L32 NATIVE_L33 NATIVE_L34 NATIVE_L35 NATIVE_L36 NATIVE_L37 NATIVE_L38 NATIVE_L39 NATIVE_L40 NATIVE_L41 NATIVE_L42 NATIVE_L43 NATIVE_L44 NATIVE_L45 NATIVE_L46 NATIVE_L47 NATIVE_L48 NATIVE_L49 NATIVE_L50 NATIVE_L51 NATIVE_L52 NATIVE_L53 NATIVE_L54 NATIVE_L55 NATIVE_L56 NATIVE_L57 NATIVE_L58 NATIVE_L59 Clear the dataframe, retain only ID, TRACT, and POP_DENSIT: Hamilton_CT@data &lt;- transmute(Hamilton_CT@data, AreaID = 1:188, TRACT = TRACT, POP_DENSIT = POP_DENSIT) To use the plotting functions of ggplot2, the SpatialPolygonDataFrame needs to be “tidied” by means of the tidy function of the broom package: Hamilton_CT.t &lt;- tidy(Hamilton_CT, region = &quot;TRACT&quot;) Hamilton_CT.t &lt;- dplyr::rename(Hamilton_CT.t, TRACT = id) Tidying the spatial dataframe strips it from the non-spatial information, but we can add all the data by means of the left_join function: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, Hamilton_CT@data, by = &quot;TRACT&quot;) ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Now the tidy dataframe Hamilton_DA.t contains the spatial information and the data. You can quickly verify the contents of the dataframe by means of summary: summary(Hamilton_CT.t) ## long lat order hole ## Min. :-80.25 Min. :43.05 Min. : 1 Mode :logical ## 1st Qu.:-79.93 1st Qu.:43.21 1st Qu.: 7304 FALSE:29212 ## Median :-79.86 Median :43.24 Median :14606 ## Mean :-79.86 Mean :43.25 Mean :14606 ## 3rd Qu.:-79.80 3rd Qu.:43.28 3rd Qu.:21909 ## Max. :-79.51 Max. :43.48 Max. :29212 ## ## piece group TRACT AreaID ## 1:29212 5370124.00.1: 822 Length:29212 Min. : 1.00 ## 5370121.00.1: 661 Class :character 1st Qu.: 18.00 ## 5370142.01.1: 642 Mode :character Median : 78.00 ## 5370100.00.1: 615 Mean : 79.82 ## 5370101.02.1: 602 3rd Qu.:128.00 ## 5370142.02.1: 595 Max. :188.00 ## (Other) :25275 ## POP_DENSIT ## Min. : 2.591 ## 1st Qu.: 254.658 ## Median : 1511.957 ## Mean : 1890.627 ## 3rd Qu.: 2807.857 ## Max. :14234.286 ## Finally, this function is used to create local Moran maps: localmoran.map &lt;- function(spat_pol = spat_pol, listw = listw, VAR = VAR, ID = ID){ require(tidyverse) require(broom) require(spdep) require(plotly) spat_pol@data &lt;- data.frame(ID = ID, VAR = VAR) spat_pol.t &lt;- broom::tidy(spat_pol, region = &quot;ID&quot;) spat_pol.t &lt;- dplyr::rename(spat_pol.t, ID = id) spat_pol.t &lt;- dplyr::left_join(spat_pol.t, spat_pol@data, by = &quot;ID&quot;) df_msc &lt;- transmute(spat_pol@data, ID = ID, Z = (VAR-mean(VAR)) / var(VAR), SMA = lag.listw(listw, Z), Type = factor(ifelse(Z &lt; 0 &amp; SMA &lt; 0, &quot;LL&quot;, ifelse(Z &gt; 0 &amp; SMA &gt; 0, &quot;HH&quot;, &quot;HL/LH&quot;)))) local_I &lt;- localmoran(spat_pol$VAR, listw) spat_pol.t &lt;- left_join(spat_pol.t, data.frame(ID = spat_pol$ID, local_I)) spat_pol.t &lt;- rename(spat_pol.t, p.val = Pr.z...0.) spat_pol.t &lt;- left_join(spat_pol.t, df_msc) map &lt;- ggplot(data = spat_pol.t, aes(x = long, y = lat, group = group, p.val = p.val, VAR = VAR)) + geom_polygon(aes(fill = Type, color = p.val &lt; 0.05)) + scale_fill_brewer(palette = &quot;RdBu&quot;) + scale_color_manual(values = c(NA, &quot;Black&quot;) ) + labs(color = &quot;Prob &lt; 0.05&quot;) + coord_equal() + theme(legend.title = element_blank()) ggplotly(map, tooltip = c(&quot;p.val&quot;, &quot;VAR&quot;)) } 27.5 Regression analysis in R As a graduate of SGES GEOG/ENVSC/EARTHSC 3MB3 Statistical Analysis, you are already familiar with regression analysis. This section provides a refresher on linear regression, before reviewing the estimation of regression models in R. A linear regression model posits relationships between an outcome, called a dependent variable, and one or more covariates, called independent variables. Regression models capture statistical relationships, not causal relationships, but often the causality is implied by the choice of independent variables. This is the form of a linear regression model: \\[ y_i = \\beta_0 + \\sum_{j=1}^k{\\beta_jx_{ij}} + \\epsilon_i \\] where \\(y_i\\) is the dependent variable and \\(x_ij\\) (\\(j=1,...,k\\)) are the independent variables. The coefficients \\(\\beta\\) are not known, but can be estimated from the data. And \\(\\epsilon_i\\) is a term called a residual (or error), because it is the difference between the systematic term of the model and the value of \\(y_i\\): \\[ \\epsilon_i = y_i - \\bigg(\\beta_0 + \\sum_{j=1}^k{\\beta_jx_{ij}}\\bigg) \\] Estimation of a linear regression model is the procedure used to obtain values for the coefficients. This typically involves defining a loss function that needs to be minimized. In the case of linear regression, a widely used estimation procedure is least squares. This procedure allows a modeler to find the coefficients that minimize the sum of squared residuals. In very simple terms, the protocol is as follows: \\[ \\text{Find the values of }\\beta\\text{ that minimize }\\sum_{i=1}^n{\\epsilon_i^2} \\] For this procedure to be valid, there are a few assumptions that need to be satisfied, including: The linear form of the model is correct. The independent variables are not collinear; this is often diagnosed by calculating the correlations among the independent variables, with values greater than 0.8 often being problematic. The residuals have a mean of zero: \\[ E[\\epsilon_i|X]=0 \\] The residuals have constant variance: \\[ Var[\\epsilon_i|X] = \\sigma^2 \\text{ }\\forall i \\] The residuals are independent, that is, they are not correlated among them: \\[ E[\\epsilon_i\\epsilon_j|X] = 0 \\text{ }\\forall i\\neq j \\] The last three assumptions ensure that the residuals are random. Violation of these assumptions is often a consequence of a failure in the first two (i.e., the model was not properly specified, and/or the residuals are not exogenous). When all these assumptions are met, the coefficiens are said to be BLUE: Best Linear Unbiased Estimates - a desirable property because we wish to be able to quantify the relationships between covariates without bias. The basic command for multivariate linear regression in R is lm(), for “linear model”. This is the help file of this function: ?lm Lets see how to estimate a model using this function. To illustrate this, we will use the example of population density gradients. These gradients are representations of the variation of population density in cities, and are of interest because they are related to land rent, urban form, and commuting patterns, among other things (see accompanying reading for more information). Urban economic theory suggests that population density declines with distance from the central business district of a city, or its CBD. This leads to the following model, where the population density at \\(i\\) is a function of the distance of \\(i\\) to the CBD. Since this is likely a stochastic process, we allow for some randomness by means of the residuals: \\[ P_i = f(D_i) + \\epsilon_i \\] Lets add distance to the CBD as a covariate to your dataframe. We will use Jackson Square as the CBD of Hamilton. The function geocode from the ggmap package can be used to retrieved the coordinates of this location: xy_cbd &lt;- geocode(&quot;Jackson Square, Hamilton, Ontario&quot;) ## Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=Jackson%20Square,%20Hamilton,%20Ontario&amp;sensor=false Further, the centroid coordinates of the census tracts can be retrieved from the SpatialPolygonsDataFrame by means of the function spdedp::coordinates: xy_ct &lt;- coordinates(Hamilton_CT) Given these coordinates, the function distGeo from the package geosphere can be used to calculate the great circle distance between the centroids of the census tracts and Hamilton’s CBD: dist2cbd.sl &lt;- distGeo(xy_ct, xy_cbd) We can now append this variable to the dataframe for analysis: Hamilton_CT@data$dist.sl &lt;- dist2cbd.sl The arguments of the model include an object of type “formula” and a dataframe. Other arguments include conditions for subsetting the data, using sampling weights, and so on. A formula is written in the form y ~ x1 + x2, and more complex expressions are possible too, as we will see below. For the time being, the formula is simply POP_DENSIT ~ dist.sl: model1 &lt;- lm(formula = POP_DENSIT ~ dist.sl, data = Hamilton_CT@data) summary(model1) ## ## Call: ## lm(formula = POP_DENSIT ~ dist.sl, data = Hamilton_CT@data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3841.4 -1338.2 -177.0 950.8 10008.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4404.91078 250.17281 17.607 &lt; 2e-16 *** ## dist.sl -0.17985 0.02418 -7.437 3.66e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1892 on 186 degrees of freedom ## Multiple R-squared: 0.2292, Adjusted R-squared: 0.2251 ## F-statistic: 55.31 on 1 and 186 DF, p-value: 3.661e-12 The value of the function is an object of class lm that contains the results of the estimation, including the coefficients with their significance diagnostics, and the coefficient of multiple determination, among other items. Notice how the coefficient for distance is negative (and significant). This indicates that population density declines with increasing distance: \\[ P_i = f(D_i) + \\epsilon_i = 4404.91 - 0.17895D_i + \\epsilon_i \\] 27.6 Autocorrelation as a model diagnostic Let quickly explore the fit of the model. The scatterplot is of the actual population density, whereas the blue line is the regression model: ggplot(data = Hamilton_CT@data, aes(x = dist.sl, y = POP_DENSIT)) + geom_point() + geom_abline(slope = model1$coefficients[2], intercept = model1$coefficients[1], color = &quot;blue&quot;, size = 1) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) Clearly, there is a fair amount of noise (the scatter of the dots around the regression line). In this case, the regression line captures the general trend of the data, but underestimates most of the high population density areas, and overestimates most of the low population areas. If the pattern of under- and over-estimation is random (i.e., the residuals are random), that would indicate that the model successfully retrieved all the systematic pattern. If the pattern is not random, there is a violation of assumption of independence. Lets add the residuals of the model to your dataframes: Hamilton_CT@data$model1.e &lt;- model1$residuals Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, data.frame(TRACT = Hamilton_CT$TRACT, model1$residuals)) ## Joining, by = &quot;TRACT&quot; ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Hamilton_CT.t &lt;- rename(Hamilton_CT.t, model1.e = model1.residuals) And now we can create a map of the residuals, with red indicating negative residuals and blue positive: map_model1.e &lt;- ggplot(data = Hamilton_CT.t, aes(x= long, y = lat, group = group, fill = model1.e &lt; 0)) + geom_polygon(color = &quot;white&quot;) + scale_fill_manual(values=c(&quot;blue&quot;, &quot;red&quot;)) + coord_equal() ggplotly(map_model1.e) %&gt;% layout(showlegend = FALSE) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` Does the distribution of residuals look random? Fortunately, we already have the tools to help us with this question. Lets create a set of spatial weights: Hamilton_CT.w &lt;- nb2listw(poly2nb(Hamilton_CT)) With this, we can calculate Moran’s I: moran.test(Hamilton_CT$model1.e, Hamilton_CT.w) ## ## Moran I test under randomisation ## ## data: Hamilton_CT$model1.e ## weights: Hamilton_CT.w ## ## Moran I statistic standard deviate = 9.7458, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.395430853 -0.005347594 0.001691104 This supports the visual inspection of the map, since the test of hypothesis rejects the null hypothesis of independence at a very high level of confidence (note the extremely small value of p). Spatial autocorrelation, as mentioned above, is a violation of a key assumption of linear regression, and likely the consequence of a model that was not correctly specified, either because the functional form was incorrect (e.g., the relationship was not linear), or there are missing covariates. Lets explore the first of these possibilities by means of variable transformations. 27.7 Variable transformations The term linear regression refers to the linearity in the coefficients. Variable transformations allow you to consider non-linear relationships between covariates, while still preserving the linearity of the coefficients. For instance, a possible transformation of the variable distance could be as inverse distance: \\[ f(D_i) = \\beta_0 + \\beta_1\\frac{1}{D_i} \\] Lets create a new covariate that is the inverse distance: Hamilton_CT@data &lt;- mutate(Hamilton_CT@data, invdist.sl = 1/dist.sl) Use inverse distance as the covariate in the model: model2 &lt;- lm(formula = POP_DENSIT ~ invdist.sl, data = Hamilton_CT@data) summary(model2) ## ## Call: ## lm(formula = POP_DENSIT ~ invdist.sl, data = Hamilton_CT@data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6763.0 -1377.3 -52.6 1107.1 9681.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2303.1 164.3 14.014 &lt; 2e-16 *** ## invdist.sl 2244627.5 340725.5 6.588 4.45e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1941 on 186 degrees of freedom ## Multiple R-squared: 0.1892, Adjusted R-squared: 0.1848 ## F-statistic: 43.4 on 1 and 186 DF, p-value: 4.451e-10 As the scatterplot below shows (the model is the blue line), we can capture a non-linear relationship, which does a better job of describing the high density of tracts close to the CBD, but unfortunately is a poor description of low density tracts elsewhere: ggplot(data = Hamilton_CT@data, aes(x = dist.sl, y = POP_DENSIT)) + geom_point() + stat_function(fun=function(x)2303.1 + 2244627/x, geom=&quot;line&quot;, color = &quot;blue&quot;, size = 1) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) Add the residuals of this model to the dataframe for further examination: Hamilton_CT@data$model2.e &lt;- model2$residuals Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, data.frame(TRACT = Hamilton_CT$TRACT, model2$residuals)) ## Joining, by = &quot;TRACT&quot; ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Hamilton_CT.t &lt;- rename(Hamilton_CT.t, model2.e = model2.residuals) If we calculate Moran’s I, you will notice that the coefficient is lower than for the previous model but still significant, wich means that the residuals are not random: moran.test(Hamilton_CT$model2.e, Hamilton_CT.w) ## ## Moran I test under randomisation ## ## data: Hamilton_CT$model2.e ## weights: Hamilton_CT.w ## ## Moran I statistic standard deviate = 8.8312, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.358445970 -0.005347594 0.001696969 The literature on population density gradients suggests other non-linear transformations, including: \\[ f(D_i) = exp(\\beta_0)exp(\\beta_1x_i) \\] This function is no longer linear in the coefficients (since all the coefficientsare transformed by the exponential). Fortunately, there is a simple way of changing this to a linear expression, by taking the logarithm on both sides of the equation: \\[ ln(P_i) = \\beta_0 + \\beta_1x_i + \\epsilon_i \\] Lets create these covariates: Hamilton_CT@data &lt;- mutate(Hamilton_CT@data, lnPOP_DEN = log(POP_DENSIT)) And estimate the model: model3 &lt;- lm(formula = lnPOP_DEN ~ dist.sl, data = Hamilton_CT@data) summary(model3) ## ## Call: ## lm(formula = lnPOP_DEN ~ dist.sl, data = Hamilton_CT@data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.5858 -0.3398 0.2968 0.6897 2.4220 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.465e+00 1.588e-01 53.29 &lt; 2e-16 *** ## dist.sl -1.161e-04 1.535e-05 -7.56 1.79e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.202 on 186 degrees of freedom ## Multiple R-squared: 0.235, Adjusted R-squared: 0.2309 ## F-statistic: 57.15 on 1 and 186 DF, p-value: 1.789e-12 ggplot(data = Hamilton_CT@data, aes(x = dist.sl, y = POP_DENSIT)) + geom_point() + stat_function(fun=function(x)exp(8.465 - 0.0001161 * x), geom=&quot;line&quot;, color = &quot;blue&quot;, size = 1) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) While this latest model provides a somewhat better fit, there is still systematic under- and over-prediction, as seen in the map below: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, data.frame(TRACT = Hamilton_CT$TRACT, model3$residuals)) ## Joining, by = &quot;TRACT&quot; ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Hamilton_CT.t &lt;- rename(Hamilton_CT.t, model3.e = model3.residuals) map_model3.e &lt;- ggplot(data = Hamilton_CT.t, aes(x= long, y = lat, group = group, fill = model3.e &lt; 0)) + geom_polygon(color = &quot;white&quot;) + scale_fill_manual(values=c(&quot;blue&quot;, &quot;red&quot;)) + coord_equal() ggplotly(map_model3.e) %&gt;% layout(showlegend = FALSE) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` Moran’s I as well strongly suggests that the residuals are not independent: Hamilton_CT@data$model3.e &lt;- model3$residuals moran.test(Hamilton_CT$model3.e, Hamilton_CT.w) ## ## Moran I test under randomisation ## ## data: Hamilton_CT$model3.e ## weights: Hamilton_CT.w ## ## Moran I statistic standard deviate = 8.017, p-value = 5.418e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.325984803 -0.005347594 0.001708061 27.8 A note about spatial autocorrelation in regression analysis Spatial autocorrelation was originally seen as a problem in regression analysis. It is not difficult to see why, after testing three models in this section. I prefer to view spatial autocorrelation as an opportunity for discovery. For instance, the models above all seem to struggle capturing the large variations in population density between the central parts and the suburbs of Hamilton. Perhaps this could be due to a regime change, that is, the underlying process operates somewhat differently in different parts parts of the city. The last model, for instance, suggests that Burlington might have an effect. The analysis that follows is somewhat more advanced, but serves to illustrate the idea of spatial autocorrelation as a tool for discovery. Lets begin by creating local Moran maps to identify potential spatial regimes: localmoran.map(Hamilton_CT, Hamilton_CT.w, Hamilton_CT$POP_DENSIT, Hamilton_CT$TRACT) ## Warning: Column `ID` joining character vector and factor, coercing into ## character vector ## Joining, by = &quot;ID&quot; ## Warning: Column `ID` joining character vector and factor, coercing into ## character vector ## Joining, by = &quot;ID&quot; ## Warning: Column `ID` joining character vector and factor, coercing into ## character vector ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` Examination of the map above, suggests that there are possibly three regimes: a CBD (“HH” and significant tracts), Suburbs (“LL” and significant tracts), and Other (not significant tracts). Based on this, we will create two indicator variables, one for census tracts in the CBD and another for census tracts in the Suburbs. An indicator variable takes values of 1 or zero, depending on whether a condition is true. For instance, all census tracts in the CBD will take the value of 1 in the CBD indicator variable, and all others will be zero. Begin by computing the local statistics: POP_DEN.lm &lt;- localmoran(Hamilton_CT$POP_DENSIT, listw = Hamilton_CT.w) Next, identify the type of tract based on the spatial relationships (i.e., “HH”, “LL”, or “HL/LH”). After that, identify as CBD all tracts for which Type is “HH” and the p-value is less than or equal to 0.05. Likewise, identify as Suburb all tracts for which Type is “LL” and the p-value is also less than or equal to 0.05: df_msc &lt;- transmute(Hamilton_CT@data, TRACT = TRACT, Z = (POP_DENSIT-mean(POP_DENSIT)) / var(POP_DENSIT), SMA = lag.listw(Hamilton_CT.w, Z), Type = factor(ifelse(Z &lt; 0 &amp; SMA &lt; 0, &quot;LL&quot;, ifelse(Z &gt; 0 &amp; SMA &gt; 0, &quot;HH&quot;, &quot;HL/LH&quot;)))) df_msc &lt;- cbind(df_msc, POP_DEN.lm) CBD &lt;- ifelse(df_msc$Type == &quot;HH&quot; &amp; df_msc$`Pr(z &gt; 0)` &lt; 0.05, 1, 0) Suburb &lt;- ifelse(df_msc$Type == &quot;LL&quot; &amp; df_msc$`Pr(z &gt; 0)` &lt; 0.05, 1, 0) Add the indicator variables to the dataframe: Hamilton_CT$CBD &lt;- CBD Hamilton_CT$Suburb &lt;- Suburb The model that I propose to estimate is a variation of the last non-linear specification, but with regime breaks: \\[ ln(P_i) = \\beta_0 + \\beta_1x_i + \\beta_2CBD_i + \\beta_3Suburb_i + \\beta_4CBD_ix_i + \\beta_5Suburb_ix_i + \\epsilon_i \\] Since the indicator variables for CBD and Suburb take values of zero and one, effectively we have the following: \\[ ln(P_i)=\\Bigg\\{\\begin{array}{l l} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_2)x_i + \\epsilon_i \\text{ if census tract } i \\text{ is in the CBD}\\\\ (\\beta_0 + \\beta_3) + (\\beta_1 + \\beta_5)x_i + \\epsilon_i \\text{ if census tract } i \\text{ is in the Suburbs}\\\\ \\beta_0 + \\beta_1x_i + \\epsilon_i \\text{ otherwise}\\\\ \\end{array} \\] Notice how the model now allows for different slopes and intercepts for observations in different parts of the city. Estimate the model: model4 &lt;- lm(formula = lnPOP_DEN ~ dist.sl + CBD * dist.sl + Suburb * dist.sl, Hamilton_CT@data) summary(model4) ## ## Call: ## lm(formula = lnPOP_DEN ~ dist.sl + CBD * dist.sl + Suburb * dist.sl, ## data = Hamilton_CT@data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4523 -0.2128 0.1805 0.4908 2.3235 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.943e+00 1.570e-01 50.579 &lt; 2e-16 *** ## dist.sl -3.247e-05 1.560e-05 -2.081 0.03883 * ## CBD 9.537e-01 3.787e-01 2.518 0.01265 * ## Suburb -9.539e-01 5.089e-01 -1.874 0.06249 . ## dist.sl:CBD -2.730e-05 1.409e-04 -0.194 0.84658 ## dist.sl:Suburb -1.005e-04 3.556e-05 -2.827 0.00522 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9384 on 182 degrees of freedom ## Multiple R-squared: 0.5435, Adjusted R-squared: 0.5309 ## F-statistic: 43.33 on 5 and 182 DF, p-value: &lt; 2.2e-16 The model is a much better fit (see the the coefficient of multiple determination). Lets examine the residuals: Hamilton_CT.t &lt;- left_join(Hamilton_CT.t, data.frame(TRACT = Hamilton_CT$TRACT, model4$residuals)) ## Joining, by = &quot;TRACT&quot; ## Warning: Column `TRACT` joining character vector and factor, coercing into ## character vector Hamilton_CT.t &lt;- rename(Hamilton_CT.t, model4.e = model4.residuals) map_model4.e &lt;- ggplot(data = Hamilton_CT.t, aes(x= long, y = lat, group = group, fill = model4.e &lt; 0)) + geom_polygon(color = &quot;white&quot;) + scale_fill_manual(values=c(&quot;blue&quot;, &quot;red&quot;)) + coord_equal() ggplotly(map_model4.e) %&gt;% layout(showlegend = FALSE) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` It is not clear from the visual inspection that the residuals are independent, but this can be tested as usual by means of Moran’s I coefficient: Hamilton_CT@data$model4.e &lt;- model4$residuals moran.test(Hamilton_CT$model4.e, Hamilton_CT.w) ## ## Moran I test under randomisation ## ## data: Hamilton_CT$model4.e ## weights: Hamilton_CT.w ## ## Moran I statistic standard deviate = 0.74518, p-value = 0.2281 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.024440811 -0.005347594 0.001597969 Based on the results, we fail to reject the null hypothesis, and can be confident that the residuals are now random. The following figure illustrates the final model: fun.1 &lt;- function(x)exp(7.943 + 0.9537 - (0.00003247 + 0.0000273) * x) #CBD fun.2 &lt;- function(x)exp(7.943 - 0.9539 - (0.00003247 + 0.0001005) * x) #Suburb fun.3 &lt;- function(x)exp(7.943 - 0.00003247 * x) #Other ggplot(data = Hamilton_CT@data, aes(x = dist.sl, y = POP_DENSIT)) + geom_point() + stat_function(fun= fun.1, geom=&quot;line&quot;, size = 1, aes(color = &quot;CBD&quot;)) + stat_function(fun=fun.2, geom=&quot;line&quot;, size = 1, aes(color = &quot;Suburb&quot;)) + stat_function(fun=fun.3, geom=&quot;line&quot;, size = 1, aes(color = &quot;Other&quot;)) + scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;black&quot;), labels = c(&quot;CBD&quot;, &quot;Suburb&quot;, &quot;Other&quot;)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) This concludes Practice 13. "],
["area-data-v-activity.html", "28 Area Data V-Activity 28.1 Practice questions 28.2 Learning objectives 28.3 Suggested reading 28.4 Preliminaries 28.5 Activity", " 28 Area Data V-Activity 28.1 Practice questions Answer the following questions: Explain the main assumptions for linear regression models. How is Moran’s I used as a diagnostic in regression analysis? Residual spatial autocorrelation is symptomatic of what issues in regression analysis? What does it mean for a model to be linear in the coefficients? What is the purpose of transforming variables for regression analysis? 28.2 Learning objectives In this activity, you will: Explore a spatial dataset. Conduct linear regression analysis. Conduct diagnostics for residual spatial autocorrelation. Propose ways to improve your analysis. 28.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 28.4 Preliminaries For this activity you will need the following: This R Notebook. A shape file called Hamilton CMA TTS06.shp with the Trafic Analysis Zones (TAZ) for Hamilton CMA, according to the Transportation Tomorrow Survey of 2011. An Excel file called Hamilton CMA Trips by Mode.xlsx with the number of trips by mode of transportation by TAZ, and other useful information from the 2011 census for Hamilton CMA. An Excel file called travel_time_car.xlsx with the travel distance/time from TAZ centroids to Jackson Square in downtown Hamilton. The data for this activity were retrieved from the 2011 Transportation Tomorrow Survey TTS, the periodic travel survey of the Greater Toronto and Hamilton Area, as well as data from the 2011 Canadian Census Census Program. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity. In addition to tidyverse, you will need spdep, a package designed for the analysis of spatial data (you can learn about spdep here and here): library(tidyverse) library(spdep) library(rgdal) library(readxl) ## Warning: package &#39;readxl&#39; was built under R version 3.4.4 library(broom) Begin by loading the shape file: Hamilton_TAZ &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA tts06&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA tts06&quot; ## with 297 features ## It has 12 fields ## Integer64 fields read as strings: ID NUM GTA06 GTA01 AREA_M AREA_H The shape file includes the geometry of the zones only. The unique identifier used to link tables is “GTA06”. Now load the data in the two Excel files: travel_data &lt;- read_excel(&quot;Hamilton CMA Trips by Mode.xlsx&quot;) travel_time &lt;- read_excel(&quot;travel_time_car.xlsx&quot;) Attach this file to the SpatialPolygonsDataFrame. Note that to complete the join, the identifier (in this case GTA06) must be in the same format in both data frames: # Travel data Hamilton_TAZ@data &lt;- left_join(Hamilton_TAZ@data, travel_data) ## Joining, by = &quot;GTA06&quot; ## Warning: Column `GTA06` joining factor and character vector, coercing into ## character vector # Travel time information travel_time$GTA06 &lt;- as.character(travel_time$GTA06) Hamilton_TAZ@data &lt;- left_join(Hamilton_TAZ@data, travel_time) ## Joining, by = &quot;GTA06&quot; The analysis will be on travel by car in the Hamilton CMA. Calculate the proportion of trips by car by TAZ: Hamilton_TAZ@data &lt;- mutate(Hamilton_TAZ@data, Auto_driver.prop = Auto_driver / (Auto_driver + Cycle + Walk)) Note that the proportion of people who travelled by car as passengers are not included in the denominator of the proportion! This is because every trip as a passenger is already included in trips with one driver. Hamilton_TAZ is a SpatialPolygonDataFrame. Convert to a dataframe (“tidy” it) for plotting using ggplot2: Hamilton_TAZ.t &lt;- tidy(Hamilton_TAZ, region = &quot;GTA06&quot;) Hamilton_TAZ.t &lt;- rename(Hamilton_TAZ.t, GTA06 = id) Rejoin the data: Hamilton_TAZ.t &lt;- left_join(Hamilton_TAZ.t, Hamilton_TAZ@data, by = &quot;GTA06&quot;) 28.5 Activity Examine your dataframe. What variables are included? Are there any missing values? Map the variable Auto_driver.prop, and use Moran’s I to test for spatial autocorrelation. What do you find? Is autocorrelation in this variable a sign that autocorrelation will be an issue in regression analysis? Estimate regression model using the variables Pop_Density and travel time in minutes. Discuss this model. Examine residuals of the model. Are they spatially independent? Propose ways to improve your model. "],
["area-data-vi.html", "29 Area Data VI 29.1 Introduction 29.2 Learning objectives 29.3 Suggested reading 29.4 Preliminaries 29.5 Residual spatial autocorrelation revisited 29.6 Remedial action 29.7 Flexible functional forms and models with spatially-varying coefficients 29.8 Spatial Error Model (SEM)", " 29 Area Data VI 29.1 Introduction NOTE: This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. In previous practice/session, you learned about how to use local spatial statistics for exploratory spatial data analysis. For this practice you will need the following: This R markdown notebook. A shape file called “Hamilton CMA TTS06” The shape file includes spatial information for Traffic Analysis Zones (TAZ) in the Hamilton Census Metropolitan Area (as polygons). 29.2 Learning objectives In this practice, you will: Revisit the notion of autocorrelation as a model diagnostic. Remedial action. Flexible functional forms and models with spatially-varying coefficients. 3.1 Trend surface analysis. 3.2 The expansion method. 3.3 Geographically weighted regression (GWR). Spatial error model (SEM). 29.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 7. John Wiley &amp; Sons: New Jersey. 29.4 Preliminaries As usual, it is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity: library(tidyverse) library(rgdal) library(broom) library(spdep) library(reshape2) library(plotly) library(knitr) ## Warning: package &#39;knitr&#39; was built under R version 3.4.4 library(kableExtra) ## Warning: package &#39;kableExtra&#39; was built under R version 3.4.4 library(spgwr) ## NOTE: This package does not constitute approval of GWR ## as a method of spatial analysis; see example(gwr) Begin by loading the shape file: Hamilton_TAZ &lt;- readOGR(&quot;.&quot;, layer = &quot;Hamilton CMA tts06&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Antonio\\Courses\\GEOG 4GA3 - Applied Spatial Analysis\\Practical Data Analysis for the Spatial Sciences in R&quot;, layer: &quot;Hamilton CMA tts06&quot; ## with 297 features ## It has 12 fields ## Integer64 fields read as strings: ID NUM GTA06 GTA01 AREA_M AREA_H The shape file includes the geometry of the zones only. To use the plotting functions of ggplot2, the SpatialPolygonDataFrame needs to be “tidied” by means of the tidy function of the broom package: Hamilton_TAZ.t &lt;- tidy(Hamilton_TAZ, region = &quot;GTA06&quot;) Hamilton_TAZ.t &lt;- dplyr::rename(Hamilton_TAZ.t, GTA06 = id) Tidying the spatial dataframe strips it from the non-spatial information, but we can add all the data by means of the left_join function: Hamilton_TAZ.t &lt;- left_join(Hamilton_TAZ.t, Hamilton_TAZ@data, by = &quot;GTA06&quot;) ## Warning: Column `GTA06` joining character vector and factor, coercing into ## character vector Now the tidy dataframe Hamilton_DA.t contains the spatial information and the data. You can quickly verify the contents of the dataframe by means of summary: summary(Hamilton_TAZ.t) ## long lat order hole ## Min. :-80.25 Min. :43.05 Min. : 1 Mode :logical ## 1st Qu.:-79.90 1st Qu.:43.21 1st Qu.: 2949 FALSE:11784 ## Median :-79.85 Median :43.25 Median : 5896 TRUE :8 ## Mean :-79.83 Mean :43.26 Mean : 5896 ## 3rd Qu.:-79.79 3rd Qu.:43.29 3rd Qu.: 8844 ## Max. :-79.51 Max. :43.48 Max. :11792 ## ## piece group GTA06 ID ## 1:11772 4050.1 : 266 Length:11792 2299 : 266 ## 2: 16 4052.1 : 239 Class :character 2301 : 239 ## 3: 4 6007.1 : 239 Mode :character 583 : 239 ## 5211.1 : 226 2842 : 226 ## 5191.1 : 191 2823 : 191 ## 6018.1 : 148 594 : 148 ## (Other):10483 (Other):10483 ## AREA NUM PD REGION ## Min. : 0.1083 0 :10560 Min. : 0.00 Min. : 5.000 ## 1st Qu.: 1.0316 11007 : 239 1st Qu.: 0.00 1st Qu.: 6.000 ## Median : 1.7958 11018 : 148 Median : 0.00 Median : 6.000 ## Mean : 6.1606 11008 : 127 Mean : 8.68 Mean : 6.305 ## 3rd Qu.: 4.1323 11006 : 107 3rd Qu.: 0.00 3rd Qu.: 6.000 ## Max. :90.7720 11012 : 107 Max. :40.00 Max. :11.000 ## (Other): 504 ## GTA01 AREA_M AREA_H UTMX_CENT UTMY_CENT DISTRICT_N ## 0 :9104 0:11792 0:11792 Min. :0 Min. :0 11:1232 ## 2050 : 266 1st Qu.:0 1st Qu.:0 5 :2559 ## 2052 : 239 Median :0 Median :0 6 :8001 ## 2556 : 110 Mean :0 Mean :0 ## 2091 : 96 3rd Qu.:0 3rd Qu.:0 ## 2098 : 63 Max. :0 Max. :0 ## (Other):1914 29.5 Residual spatial autocorrelation revisited Previously you learned about the use of Moran’s I coefficient as a diagnostic in regression analysis. Residual spatial autocorrelation is a symptom of a model that has not been properly specified. There are two reasons for this that are of interest: The functional form is incorrect. The model failed to include relevant variables. Lets explore these in turn. 29.5.1 Incorrect functional form To illustrate this, we will simulate a spatial process as follows: \\[ z = f(x,y) = exp(\\beta_0)exp(\\beta_1x)exp(\\beta_2y) + \\epsilon_i \\] Clearly, this is a non-linear spatial process. The simulation is as follows, with a random term with a mean of zero and standard deviation of 1. The random terms are independent by design: set.seed(10) b0 = 1 b1 = 2 b2 = 4 xy_coords &lt;- coordinates(Hamilton_TAZ) Hamilton_TAZ@data &lt;- mutate(Hamilton_TAZ@data, x = xy_coords[,1] - min(xy_coords[,1]), y = xy_coords[,2] - min(xy_coords[,2]), z = exp(b0) * exp(b1 * x) * exp(b2 * y) + rnorm(n = 297, mean = 0, sd = 1)) summary(Hamilton_TAZ@data[,13:15]) ## x y z ## Min. :0.0000 Min. :0.0000 Min. : 3.761 ## 1st Qu.:0.2810 1st Qu.:0.1226 1st Qu.: 8.222 ## Median :0.3316 Median :0.1550 Median : 9.958 ## Mean :0.3348 Mean :0.1681 Mean :10.885 ## 3rd Qu.:0.3850 3rd Qu.:0.1976 3rd Qu.:12.534 ## Max. :0.6518 Max. :0.3683 Max. :22.358 Suppose that we estimate the model as a linear regression that does not correctly capture the non-linearity. The model would be as follows: model1 &lt;- lm(formula = z ~ x + y, data = Hamilton_TAZ@data) summary(model1) ## ## Call: ## lm(formula = z ~ x + y, data = Hamilton_TAZ@data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.73799 -0.85271 0.01062 0.77256 3.07456 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.2480 0.3169 -13.41 &lt;2e-16 *** ## x 21.9485 0.6798 32.29 &lt;2e-16 *** ## y 46.2989 1.0019 46.21 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.196 on 294 degrees of freedom ## Multiple R-squared: 0.9014, Adjusted R-squared: 0.9007 ## F-statistic: 1343 on 2 and 294 DF, p-value: &lt; 2.2e-16 At first glance, the model gives the impression of a very good fit: all coefficients are significant, and the coefficient of multiple determination \\(R^2\\) is very high. At this point, it is important to examine the residuals to verify that they are independent. Lets add the residuals of this model to your dataframes: Hamilton_TAZ@data$model1.e &lt;- model1$residuals Hamilton_TAZ.t &lt;- left_join(Hamilton_TAZ.t, data.frame(GTA06 = Hamilton_TAZ$GTA06, model1$residuals)) ## Joining, by = &quot;GTA06&quot; ## Warning: Column `GTA06` joining character vector and factor, coercing into ## character vector Hamilton_TAZ.t &lt;- rename(Hamilton_TAZ.t, model1.e = model1.residuals) A map of the residuals can help examine their spatial pattern: map.e1 &lt;- ggplot(data = Hamilton_TAZ.t, aes(x = long, y = lat, group = group, fill = model1.e)) + geom_polygon(color = &quot;white&quot;) + coord_equal() + scale_fill_distiller(palette = &quot;RdBu&quot;) ggplotly(map.e1) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` To test the residuals for spatial autocorrelation we first create a set of spatial weights: Hamilton_TAZ.w &lt;- nb2listw(poly2nb(Hamilton_TAZ)) With this, we can now calculate Moran’s I: moran.test(Hamilton_TAZ$model1.e, Hamilton_TAZ.w) ## ## Moran I test under randomisation ## ## data: Hamilton_TAZ$model1.e ## weights: Hamilton_TAZ.w ## ## Moran I statistic standard deviate = 9.3976, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.317160954 -0.003378378 0.001163393 The test does not allow us to reject the null hypothesis of spatial independence. Thus, despite the apparent goodness of fit of the model, there is reason to believe something is missing. Lets now use a variable transformation to approximate the underlying non-linear process: model2 &lt;- lm(formula = log(z) ~ x + y, data = Hamilton_TAZ@data) summary(model2) ## ## Call: ## lm(formula = log(z) ~ x + y, data = Hamilton_TAZ@data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.31731 -0.06268 0.00447 0.07483 0.28175 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.96804 0.02721 35.58 &lt;2e-16 *** ## x 2.07419 0.05837 35.53 &lt;2e-16 *** ## y 3.97461 0.08603 46.20 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1027 on 294 degrees of freedom ## Multiple R-squared: 0.9066, Adjusted R-squared: 0.9059 ## F-statistic: 1426 on 2 and 294 DF, p-value: &lt; 2.2e-16 This model does not necessarily have a better goodness of fit. However, when we test for spatial autocorrelation: Hamilton_TAZ@data$model2.e &lt;- model2$residuals moran.test(Hamilton_TAZ$model2.e, Hamilton_TAZ.w) ## ## Moran I test under randomisation ## ## data: Hamilton_TAZ$model2.e ## weights: Hamilton_TAZ.w ## ## Moran I statistic standard deviate = 0.58286, p-value = 0.28 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.016485452 -0.003378378 0.001161455 Once that the correct functional form has been specified, the model is better at capturing the underlying process (check how the coefficients approximate to a high degree the true coefficients of the model). In addition, we can conclude that the residuals are independent, and therefore are now also spatially random: meaning the there is nothing left of the process but white noise. 29.5.2 Omitted variables Using the same example, suppose now that the function form is correctly specified, but a relevant variable is missing: model3 &lt;- lm(formula = log(z) ~ x, data = Hamilton_TAZ@data) summary(model3) ## ## Call: ## lm(formula = log(z) ~ x, data = Hamilton_TAZ@data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.70236 -0.17279 -0.04807 0.13260 0.83999 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.81774 0.05753 31.60 &lt;2e-16 *** ## x 1.53226 0.16406 9.34 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2947 on 295 degrees of freedom ## Multiple R-squared: 0.2282, Adjusted R-squared: 0.2256 ## F-statistic: 87.23 on 1 and 295 DF, p-value: &lt; 2.2e-16 As before, lets append the residuals to the dataframes: Hamilton_TAZ@data$model3.e &lt;- model3$residuals Hamilton_TAZ.t &lt;- left_join(Hamilton_TAZ.t, data.frame(GTA06 = Hamilton_TAZ$GTA06, model3$residuals)) ## Joining, by = &quot;GTA06&quot; ## Warning: Column `GTA06` joining character vector and factor, coercing into ## character vector Hamilton_TAZ.t &lt;- rename(Hamilton_TAZ.t, model3.e = model3.residuals) A map of the residuals can help examine their spatial pattern: map.e3 &lt;- ggplot(data = Hamilton_TAZ.t, aes(x = long, y = lat, group = group, fill = model3.e)) + geom_polygon(color = &quot;white&quot;) + coord_equal() + scale_fill_distiller(palette = &quot;RdBu&quot;) ggplotly(map.e3) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` In this case, the visual inspection makes it clear that there is an issue with spatially autocorrelated residuals, something that a test reinforces: moran.test(Hamilton_TAZ$model3.e, Hamilton_TAZ.w) ## ## Moran I test under randomisation ## ## data: Hamilton_TAZ$model3.e ## weights: Hamilton_TAZ.w ## ## Moran I statistic standard deviate = 24.616, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.835679736 -0.003378378 0.001161885 As seen above, the model with the full set of relevant variables resolves this problem. 29.6 Remedial action When spatial autocorrelation is detected in the residuals, further work is warranted. The preceding examples illustrate two possible solutions to the issue of residual pattern: Modifications of the model to approximate the true functional form of the process; and Inclusion of relevant variables. Ideally, we would try to ensure that the model is properly specified. In practice, however, it is not always evident what the functional form of the model should be. The search for an appropriate functional form can be guided by theoretical considerations, empirical findings, and experimentation. With respect to inclusion of relevant variables, it is not always possible to find all the information we desire. This could be because of limited resources, or because some aspects of the process are not known and therefore we do not even know what additional information should be collected. In these cases, it is a fact that residual spatial autocorrelation is problematic. Fortunately, a number of approaches have been proposed in the literature that can be used for remedial action. In the following sections we will review some of them. 29.7 Flexible functional forms and models with spatially-varying coefficients Some models use variable transformations to create more flexible functions, while others use adaptive estimation strategies. 29.7.1 Trend surface analysis Trend surface analysis is a simple way to generate relatively flexible surfaces. This approach consists of using the coordinates as covariates, and transforming them into polynomials of different orders. Seen this way, linear regression is the analog of a trend surface of first degree: \\[ z = f(x,y) = \\beta_0 + \\beta_1x + \\beta_2y \\] where \\(x\\) and \\(y\\) are the coordinates. A figure illustratates how the function above creates a regression plane. First, create a grid of coordinates for plotting: df &lt;- expand.grid(x = seq(from = -2, to = 2, by = 0.2), y = seq(from = -2, to = 2, by = 0.2)) Next, select some values for the coefficients (feel free to experiment with these values): b0 &lt;- 0.5 #0.5 b1 &lt;- 1 #1 b2 &lt;- 2 #2 z1 &lt;- b0 + b1 * df$x + b2 * df$y z1 &lt;- matrix(z1, nrow = 21, ncol = 21) The plot is as follows: plot_ly(z = ~z1) %&gt;% add_surface() %&gt;% layout(scene = list(xaxis = list(ticktext = c(&quot;-2&quot;, &quot;0&quot;, &quot;2&quot;), tickvals = c(0, 10, 20)), yaxis = list(ticktext = c(&quot;-2&quot;, &quot;0&quot;, &quot;2&quot;), tickvals = c(0, 10, 20)) ) ) A trend surface of second degree, or quadratic, would be as follows. Notice how it includes all possible quadratic terms, including the product \\(xy\\): \\[ z = f(x,y) = \\beta_0 + \\beta_1x^2 + \\beta_2x + \\beta_3xy + \\beta_4y + \\beta_5y^2 \\] Use the same grid as above to create now a regression surface. Select some coefficients: b0 &lt;- 0.5 #0.5 b1 &lt;- 2 #2 b2 &lt;- 1 #1 b3 &lt;- 1 #1 b4 &lt;- 1.5 #1.5 b5 &lt;- 0.5 #2.5 z2 &lt;- b0 + b1 * df$x^2 + b2 * df$x + b3 * df$x * df$y + b4 * df$y + b5 * df$y^2 z2 &lt;- matrix(z2, nrow = 21, ncol = 21) And the plot is as follows: plot_ly(z = ~z2) %&gt;% add_surface() %&gt;% layout(scene = list(xaxis = list(ticktext = c(&quot;-2&quot;, &quot;0&quot;, &quot;2&quot;), tickvals = c(0, 10, 20)), yaxis = list(ticktext = c(&quot;-2&quot;, &quot;0&quot;, &quot;2&quot;), tickvals = c(0, 10, 20)) ) ) Higher order polynomials (i.e., cubic, quartic, etc.) are possible in principle. Something to keep in mind is that the higher the order of the polynomial, the more flexible the surface, which may lead to the following issues: Multicollinearity. Powers of variables tend to be highly correlated with each other. See the following table of correlations for the x coordinate in the example: x x^2 x^3 x^4 x 1.00 0.00 0.92 0.00 x^2 0.00 1.00 0.00 0.96 x^3 0.92 0.00 1.00 0.00 x^4 0.00 0.96 0.00 1.00 When two variables are highly collinear, the model has difficulties discriminating their relative contribution to the model. This is manifested by inflated standard errors that may depress the significance of the coefficients, and occasionally by sign reversals. Overfitting. Overfitting is another possible consequence of using a trend surface that is too flexible. This happens when a model fits too well the observations used for callibration, but because of this it may fail to fit well new information. To illustrate overfitting consider a simple example. Below we simulate a simple linear model with \\(y_i = x_i + \\epsilon_i\\) (the random terms are drawn from the uniform distribution). We also simulate new data using the exact same process: # Dataset for estimation df.of1 &lt;- data.frame(x = seq(from = 1, to = 10, by = 1)) df.of1 &lt;- mutate(df.of1, y = x + runif(10, -1, 1)) # New data new_data &lt;- data.frame(x = seq(from = 1, to = 10, by = 0.5)) df.of2 &lt;- mutate(new_data, y = x + runif(nrow(new_data), -1, 1)) This is the scatterplot of the observations in the estimation dataset: p &lt;- ggplot(data = df.of1, aes(x = x, y = y)) p + geom_point(size = 3) A model with a first order trend (essentially linear regression), does not fit the observations perfectly, but when confronted with new data (plotted as red squares), it predicts them with reasonable accuracy: mod.of1 &lt;- lm(formula = y ~ x, data = df.of1) pred1 &lt;- predict(mod.of1, newdata = new_data) #mod.of1$fitted.values p + geom_abline(slope = mod.of1$coefficients[2], intercept = mod.of1$coefficients[1], color = &quot;blue&quot;, size = 1) + geom_point(data = df.of2, aes(x = x, y = y), shape = 0, color = &quot;red&quot;) + geom_segment(data = df.of2, aes(xend = x, yend = pred1)) + geom_point(size = 3) + xlim(c(1, 10)) Compare to a polynomial of very high degree (nine in this case). The model is much more flexible, to the extent that it perfectly matches the observations in the estimation dataset. However, this flexibility has a downside. When the model is confronted with new information, its performance is less satisfactory. mod.of2 &lt;- lm(formula = y ~ poly(x, degree = 9, raw = TRUE), data = df.of1) poly.fun &lt;- predict(mod.of2, data.frame(x = seq(1, 10, 0.1))) pred2 &lt;- predict(mod.of2, newdata = new_data) #mod.of1$fitted.values p + #stat_function(fun = fun.pol, geom_line(data = data.frame(x = seq(1, 10, 0.1), y = poly.fun), aes(x = x, y = y), color = &quot;blue&quot;, size = 1) + geom_point(data = df.of2, aes(x = x, y = y), shape = 0, color = &quot;red&quot;) + geom_segment(data = df.of2, aes(xend = x, yend = pred2)) + geom_point(size = 3) + xlim(c(1, 10)) We can compute the root mean square (RMS), for each of the two models. The RMS is a measure of error calculated as the square root of the mean of the squared differences between two values (in this case the prediction of the model and the new information). This statistic is a measure of the typical deviation between two sets of values. Given new information, the RMS would tell us the expected size of the error when making a prediction using a given model. The RMS for model 1 is: sqrt(mean((df.of2$y - pred1)^2)) ## [1] 0.525595 And for model 2: sqrt(mean((df.of2$y - pred2)^2)) ## [1] 1.681143 You will notice how model 2, despite fitting the estimation data better than model 1, typically produces larger errors when new information becomes available. Edge effects. Another consequence of overfitting, is that the resulting functions tend to display extreme behavior when taken outside of their estimation range, where the largest polynomial terms tend to dominate. The plot below is the same high degree polynomial estimated above, just plotted in a slightly larger range of plus/minus one unit: poly.fun &lt;- predict(mod.of2, data.frame(x = seq(0, 11, 0.1))) p + geom_line(data = data.frame(x = seq(0, 11, 0.1), y = poly.fun), aes(x = x, y = y), color = &quot;blue&quot;, size = 1) + geom_point(data = df.of2, aes(x = x, y = y), shape = 0, color = &quot;red&quot;) + geom_segment(data = df.of2, aes(xend = x, yend = pred2)) + geom_point(size = 3) 29.7.2 Models with spatially varying coefficients Another way to generate flexible functional forms is by means of models with spatially varying coefficients. Two approaches are reviewed here. 29.7.2.1 Expansion method The expansion method (Casetti, 1972) is an approach to generate models with contextual effects. It follows a philosophy of specifying first a substantive model with variables of interest, and then an expanded model with contextual variables. In geographical analysis, typically the contextual variables are trend surfaces estimated using the coordinates of the observations. To illustrate this, suppose that there is the following initial model of proportion of donors in a population, with two variables of substantive interest (say, income and education): \\[ d_i = \\beta_i(x_i,y_i) + \\beta_1(x_i,y_i)I_i + \\beta_3(x_i,y_i)Ed_i + \\epsilon_i \\] Note how the coefficients are now a function of the coordinates at \\(i\\). Unlike previous models that had global coefficients, the coefficients in this model are allowed to adapt by location. Unfortunately, it is not possible to estimate one coefficient per location. In this case, there are \\(n\\times k\\) coefficients, which exceeds the size of the sample (\\(n\\)). It is not possible to retrieve more information from the sample than \\(n\\) parameters (this is called the incidental parameter problem.) A possible solution is to specify a function for the coefficients, for instance, by specifying a trend surface for them: \\[ \\begin{array}{l} \\beta_0(x_i, y_i) = \\beta_{01} +\\beta_{02}x_i + \\beta_{03}y_i\\\\ \\beta_1(x_i, y_i) = \\beta_{11} +\\beta_{12}x_i + \\beta_{13}y_i\\\\ \\beta_2(x_i, y_i) = \\beta_{21} +\\beta_{22}x_i + \\beta_{23}y_i \\end{array} \\] By specifying the coefficients as a function of the coordinates, we allow them to vary by location. Next, if we substitute these coefficients in the intial model, we arrive at a final expanded model: \\[ d_i = \\beta_{01} +\\beta_{02}x_i + \\beta_{03}y_i + \\beta_{11}I_i +\\beta_{12}x_iI_i + \\beta_{13}y_iI_i + \\beta_{21}Ed_i +\\beta_{22}x_iEd_i + \\beta_{23}y_iEd_i + \\epsilon_i \\] This model has now nine coefficients, instead of \\(n\\times 3\\), and can be estimated as usual. It is important to note that since models generated based on the expansion method are based on the use of trend surfaces, similar caveats apply with respect to multicollinearity and overfitting. 29.7.2.2 Geographically weighted regression (GWR) A different strategy to estimate models with spatially-varying coefficients is a semi-parametric approach, called geographically weighted regression (see Brunsdon et al., 1996). Instead of selecting a functional form for the coefficients as the expansion method does, the functions are left unspecified. The spatial variation of the coefficients results from an estimation strategy that takes subsamples of the data in a systematic way. If you recall kernel density analysis, a kernel was a way of weighting observations based on their distance from a focal point. Geographically weighted regression applies a similar concept, with a moving window that visits a focal point and estimates a weighted least squares model at that location. The results of the regression are conventionally applied to the focal point, in such a way that not only the coefficients are localized, but also every other regression diagnostic (e.g., the coefficient of determination, the standard deviation, etc.) A key aspect of implementing this model is the selection of the kernel bandwidth, that is, the size of the window. If the window is too large, the local models tend towards the global model (estimated using the whole sample). If the window is too small, the model tends to overfit, since in the limit each window will contain only one, or a very small number of observations. The kernel bandwidth can be selected if we define some loss function to minimize. A conventional approach (but not the only one), is to minimize a cross-validation score of the following form: \\[ CV (\\delta) = \\sum_{i=1}^n{\\big(y_i - \\hat{y}_{\\neq i}(\\delta)\\big)^2} \\] In this notation, \\(\\delta\\) is the bandwidth, and \\(\\hat{y}_{\\neq i}(\\delta)\\) is the value of \\(y\\) predicted by a model with a bandwidth of \\(\\delta\\) after excluding the observation at \\(i\\). This is called a leave-one-out cross-validation procedure, used to prevent the estimation from shrinking the bandwidth to zero. GWR is implemented in the package spgwr. To estimate models using this approach, the function sel.GWR, which takes as inputs a formula specifying the dependent and independent variables, a SpatialPolygonsDataFrame (or a SpatialPointsDataFrame), and the kernel function (in the example below a Gaussian kernel): delta &lt;- gwr.sel(formula = z ~ x + y, data = Hamilton_TAZ, gweight = gwr.Gauss) ## Bandwidth: 25.59084 CV score: 399.3492 ## Bandwidth: 41.36552 CV score: 418.0564 ## Bandwidth: 15.84156 CV score: 363.7496 ## Bandwidth: 9.816173 CV score: 323.2672 ## Bandwidth: 6.092278 CV score: 300.7067 ## Bandwidth: 3.790784 CV score: 306.2781 ## Bandwidth: 5.801654 CV score: 299.5063 ## Bandwidth: 5.313051 CV score: 298.1325 ## Bandwidth: 4.731597 CV score: 298.2069 ## Bandwidth: 5.045611 CV score: 297.8656 ## Bandwidth: 5.040167 CV score: 297.8649 ## Bandwidth: 5.020481 CV score: 297.8638 ## Bandwidth: 5.022892 CV score: 297.8638 ## Bandwidth: 5.022961 CV score: 297.8638 ## Bandwidth: 5.023002 CV score: 297.8638 ## Bandwidth: 5.022961 CV score: 297.8638 The function gwr estimates the suite of local models given a bandwidth: model.gwr &lt;- gwr(formula = z ~ x + y, bandwidth = delta, data = Hamilton_TAZ, gweight = gwr.Gauss) model.gwr ## Call: ## gwr(formula = z ~ x + y, data = Hamilton_TAZ, bandwidth = delta, ## gweight = gwr.Gauss) ## Kernel function: gwr.Gauss ## Fixed bandwidth: 5.022961 ## Summary of GWR coefficient estimates at data points: ## Min. 1st Qu. Median 3rd Qu. Max. Global ## X.Intercept. -19.8637 -6.3578 -2.6342 -1.3318 1.3375 -4.248 ## x 7.4674 17.7597 19.7687 25.1595 38.9585 21.948 ## y 21.7465 33.2582 38.5066 48.8746 96.7801 46.299 The results are given for each location where a local regression was estimated. Lets append to our tidy dataframe for plotting: Hamilton_TAZ.t &lt;- left_join(Hamilton_TAZ.t, data.frame(GTA06 = Hamilton_TAZ$GTA06, model.gwr$SDF@data)) ## Joining, by = &quot;GTA06&quot; ## Warning: Column `GTA06` joining character vector and factor, coercing into ## character vector Hamilton_TAZ.t &lt;- rename(Hamilton_TAZ.t, beta0 = X.Intercept., beta1 = x, beta2 = y) The results can be mapped as shown below (try mapping beta1, beta2, localR2, or the residuals gwr.e): ggplot(data = Hamilton_TAZ.t, aes(x = long, y = lat, group = group, fill = beta0)) + geom_polygon(color = &quot;white&quot;) + scale_fill_distiller(palette = &quot;YlOrRd&quot;, trans = &quot;reverse&quot;) + coord_equal() You can verify that the residuals are not spatially autocorrelated: moran.test(model.gwr$SDF$gwr.e, Hamilton_TAZ.w) ## ## Moran I test under randomisation ## ## data: model.gwr$SDF$gwr.e ## weights: Hamilton_TAZ.w ## ## Moran I statistic standard deviate = -0.033896, p-value = 0.5135 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## -0.004534922 -0.003378378 0.001164212 Some caveats with respect to GWR. Since estimation requires the selection of a kernel bandwidth, and a kernel bandwidth requires the estimation of many times leave-one-out regressions, GWR can be computationally quite demanding, especially for large datasets. GWR has become a very popular method, however, there is conflicting evidence regarding its ability to retrieve a known spatial process, and interpretation of the spatially-varying coefficients must be conducted with a grain of salt, although this seems to be less of a concern with larger samples - but at the moment it is not known how large a sample is safe (and larger samples also become computationally more demanding). As well, the estimation method is known to be sensitive to unusual observations. At the moment, I recommend that GWR be used for prediction only, and in this respect it seems to perform as well, or even better than alternatives approaches. 29.8 Spatial Error Model (SEM) A model that can be used to take direct remedial action with respect to residual spatial autocorrelation is the spatial error model. This model is specified as follows: \\[ y_i = \\beta_0 + \\sum_{j=1}^k{\\beta_kx_{ij}} + \\epsilon_i \\] However, it is no longer assumed that the residuals \\(\\epsilon\\) are independent, but instead display map pattern, in the shape of a moving average: \\[ \\epsilon_i = \\lambda\\sum_{i=1}^n{w_{ij}^{st}\\epsilon_i} + \\mu_i \\] A second set of residuals \\(\\mu\\) are assumed to be independent. It is possible to show that this model is no longer linear in the coefficients (but this would require a little bit of matrix algebra). For this reason, ordinary least squares is no longer an appropriate estimation algorithm, and models of this kind are instead estimated based on maximum likelihood. Spatial error models are implemented in the package spdep. As a remedial model, it can account for a model with a misspecified functional form. We know that the underlying process is not linear, but we specify a linear relationship between the covariates in the form of \\(z = \\beta_0 + \\beta_1x + \\beta_2y\\): model.sem1 &lt;- errorsarlm(formula = z ~ x + y, data = Hamilton_TAZ@data, listw = Hamilton_TAZ.w) summary(model.sem1) ## ## Call: ## errorsarlm(formula = z ~ x + y, data = Hamilton_TAZ@data, listw = Hamilton_TAZ.w) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.835717 -0.844457 0.028902 0.778175 2.571815 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.3968 0.5859 -7.5044 6.173e-14 ## x 21.9446 1.2605 17.4092 &lt; 2.2e-16 ## y 47.2179 1.8584 25.4077 &lt; 2.2e-16 ## ## Lambda: 0.55022, LR test value: 59.092, p-value: 1.5099e-14 ## Asymptotic standard error: 0.066516 ## z-value: 8.2721, p-value: 2.2204e-16 ## Wald statistic: 68.427, p-value: &lt; 2.22e-16 ## ## Log likelihood: -443.6038 for error model ## ML residual variance (sigma squared): 1.0906, (sigma: 1.0443) ## Number of observations: 297 ## Number of parameters estimated: 5 ## AIC: 897.21, (AIC for lm: 954.3) The coefficient \\(\\lambda\\) is positive (indicative of positive autocorrelation) and high, since about 50% of the moving average of the residuals \\(\\epsilon\\) in the neighborhood of \\(i\\) contribute to the value of \\(\\epsilon_i\\). You can verify that the residuals are spatially uncorrelated (note that the alternative is “less” because of the negative sign of Moran’s I coefficient): moran.test(model.sem1$residuals, Hamilton_TAZ.w, alternative = &quot;less&quot;) ## ## Moran I test under randomisation ## ## data: model.sem1$residuals ## weights: Hamilton_TAZ.w ## ## Moran I statistic standard deviate = -0.82584, p-value = 0.2044 ## alternative hypothesis: less ## sample estimates: ## Moran I statistic Expectation Variance ## -0.031552712 -0.003378378 0.001163896 Now consider the case of a missing covariate: model.sem2 &lt;- errorsarlm(formula = log(z) ~ x, data = Hamilton_TAZ@data, listw = Hamilton_TAZ.w) summary(model.sem2) ## ## Call: ## errorsarlm(formula = log(z) ~ x, data = Hamilton_TAZ@data, listw = Hamilton_TAZ.w) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4014843 -0.0670803 0.0079365 0.0791249 0.4647178 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.67842 0.18291 9.1763 &lt; 2.2e-16 ## x 1.92656 0.48773 3.9500 7.814e-05 ## ## Lambda: 0.91625, LR test value: 469.65, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.02273 ## z-value: 40.31, p-value: &lt; 2.22e-16 ## Wald statistic: 1624.9, p-value: &lt; 2.22e-16 ## ## Log likelihood: 177.2607 for error model ## ML residual variance (sigma squared): 0.013849, (sigma: 0.11768) ## Number of observations: 297 ## Number of parameters estimated: 4 ## AIC: -346.52, (AIC for lm: 121.13) In this case, the residual pattern is particularly strong, with more than 90% of the moving average contributing to Alas, in this case, the remedial action falls short of cleaning the residuals, and we can see that they still remain spatially correlated: moran.test(model.sem2$residuals, Hamilton_TAZ.w, alternative = &quot;less&quot;) ## ## Moran I test under randomisation ## ## data: model.sem2$residuals ## weights: Hamilton_TAZ.w ## ## Moran I statistic standard deviate = -3.3267, p-value = 0.0004395 ## alternative hypothesis: less ## sample estimates: ## Moran I statistic Expectation Variance ## -0.116544541 -0.003378378 0.001157221 This would suggest the need for alternative action (such as the search for additional covariates). Ideally, a model should be well-specified, and remedial action should be undertaken only when other alternatives have been exhausted. This concludes Practice 14. "],
["area-data-vi-activity.html", "30 Area Data VI-Activity 30.1 Practice questions 30.2 Learning objectives 30.3 Suggested reading 30.4 Preliminaries 30.5 Activity", " 30 Area Data VI-Activity 30.1 Practice questions Answer the following questions: Describe and discuss the possible sources of autocorrelation in the residuals of a model. List possible corrective/remedial actions when residual autocorrelation is detected. Under which situations is a Spatial Error Model an adequate modeling strategy? 30.2 Learning objectives In this activity, you will: Explore a dataset with area data using visualization as appropriate. Discuss a process that might explain any pattern observed from the data. Conduct a modeling exercise using appropriate techniques. Justify your modeling decisions. 30.3 Suggested reading O’Sullivan D and Unwin D (2010) Geographic Information Analysis, 2nd Edition, Chapter 5. John Wiley &amp; Sons: New Jersey. 30.4 Preliminaries For this activity you will need the following: This R markdown notebook. A dataset of your choice. It is good practice to clear the working space to make sure that you do not have extraneous items there when you begin your work. The command in R to clear the workspace is rm (for “remove”), followed by a list of items to be removed. To clear the workspace from all objects, do the following: rm(list = ls()) Note that ls() lists all objects currently on the worspace. Load the libraries you will use in this activity (load other packages as appropriate). library(tidyverse) library(spatstat) library(spdep) Choose one of the following datasets. 30.4.1 New York leukemia data load(&quot;New York Leukemia.RData&quot;) A SpatialPolygonsDataFrame that contains the following variables: AREANAME name of census tract AREAKEY unique FIPS code for each tract POP8 population size (1980 U.S. Census) TRACTCAS number of cases of leukemia (1978-1982) PROPCAS proportion of cases per tract PCTOWNHOME percentage of people in each tract owning their own home PCTAGE65P percentage of people in each tract aged 65 or more Z transformed proportions AVGIDIST average distance between centroid and TCE sites PEXPOSURE “exposure potential”: inverse distance between each census tract centroid and the nearest TCE site, IDIST, transformed via log(100*IDIST) 30.4.2 Pennsylvania lung cancer load(&quot;Penn Lung Cancer.RData&quot;) A SpatialPolygonsDataFrame that contains the following variables: county: Name of the county cases: Number of cases of lung cancer population: Population by county rate: Lung cancer rate by county smoking: Smoking rate by county cancer_ rate: Lung cancer rate by county (%) smoking_rate: Smoking rate by county (%) 30.5 Activity Partner with a fellow student to analyze the chosen dataset. Visualize/explore the dataset using appropriate tools. Analyze your dataset by means of regression modeling. Which should be the dependent variable in your dataset? Why? Discuss the results of your analysis, including possible limitations, and possible ways to improve it (e.g., what additional variables would you like to use?) "]
]
